[
  {
    "objectID": "test_viz.html",
    "href": "test_viz.html",
    "title": "",
    "section": "",
    "text": "# prerequisites\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.manifold import TSNE\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n# Define a list of words to visualize\nwords = [\"python\", \"javascript\", \"c++\", \"reptile\", \"snake\"]\n\n# Get embeddings for the words\nresponse = client.embeddings.create(\n    input=words,\n    model=MODEL\n)\n\nembeddings = [emb.embedding for emb in response.data]\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n    n_components=2, \n    random_state=42,\n    perplexity=4 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    plt.scatter(x, y, marker='o', color='red')\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\n---------------------------------------------------------------------------\nAuthenticationError                       Traceback (most recent call last)\nCell In[1], line 23\n     20 words = [\"python\", \"javascript\", \"c++\", \"reptile\", \"snake\"]\n     22 # Get embeddings for the words\n---&gt; 23 response = client.embeddings.create(\n     24     input=words,\n     25     model=MODEL\n     26 )\n     28 embeddings = [emb.embedding for emb in response.data]\n     30 # Apply t-SNE dimensionality reduction\n\nFile ~/Documents/Code/llm_script/.script_venv/lib/python3.12/site-packages/openai/resources/embeddings.py:125, in Embeddings.create(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\n    119         embedding.embedding = np.frombuffer(  # type: ignore[no-untyped-call]\n    120             base64.b64decode(data), dtype=\"float32\"\n    121         ).tolist()\n    123     return obj\n--&gt; 125 return self._post(\n    126     \"/embeddings\",\n    127     body=maybe_transform(params, embedding_create_params.EmbeddingCreateParams),\n    128     options=make_request_options(\n    129         extra_headers=extra_headers,\n    130         extra_query=extra_query,\n    131         extra_body=extra_body,\n    132         timeout=timeout,\n    133         post_parser=parser,\n    134     ),\n    135     cast_to=CreateEmbeddingResponse,\n    136 )\n\nFile ~/Documents/Code/llm_script/.script_venv/lib/python3.12/site-packages/openai/_base_client.py:1260, in SyncAPIClient.post(self, path, cast_to, body, options, files, stream, stream_cls)\n   1246 def post(\n   1247     self,\n   1248     path: str,\n   (...)\n   1255     stream_cls: type[_StreamT] | None = None,\n   1256 ) -&gt; ResponseT | _StreamT:\n   1257     opts = FinalRequestOptions.construct(\n   1258         method=\"post\", url=path, json_data=body, files=to_httpx_files(files), **options\n   1259     )\n-&gt; 1260     return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n\nFile ~/Documents/Code/llm_script/.script_venv/lib/python3.12/site-packages/openai/_base_client.py:937, in SyncAPIClient.request(self, cast_to, options, remaining_retries, stream, stream_cls)\n    928 def request(\n    929     self,\n    930     cast_to: Type[ResponseT],\n   (...)\n    935     stream_cls: type[_StreamT] | None = None,\n    936 ) -&gt; ResponseT | _StreamT:\n--&gt; 937     return self._request(\n    938         cast_to=cast_to,\n    939         options=options,\n    940         stream=stream,\n    941         stream_cls=stream_cls,\n    942         remaining_retries=remaining_retries,\n    943     )\n\nFile ~/Documents/Code/llm_script/.script_venv/lib/python3.12/site-packages/openai/_base_client.py:1041, in SyncAPIClient._request(self, cast_to, options, remaining_retries, stream, stream_cls)\n   1038         err.response.read()\n   1040     log.debug(\"Re-raising status error\")\n-&gt; 1041     raise self._make_status_error_from_response(err.response) from None\n   1043 return self._process_response(\n   1044     cast_to=cast_to,\n   1045     options=options,\n   (...)\n   1049     retries_taken=options.get_max_retries(self.max_retries) - retries,\n   1050 )\n\nAuthenticationError: Error code: 401 - {'statusCode': 401, 'message': 'Unauthorized. Access token is missing, invalid, audience is incorrect (https://cognitiveservices.azure.com), or have expired.'}\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "nlp/exercises/ex_word_matching.html",
    "href": "nlp/exercises/ex_word_matching.html",
    "title": "Exercise: Word matching",
    "section": "",
    "text": "Task: For each element of the following list of keywords, determine whether it is contained in the text.\nInstructions:\n\nTransform the text to lower case and use a tokenizer to split the text into word tokens.\nFirst, use a simple comparison of strings to check whether the keywords match any token. When does this approach fail?\nLemmatize the tokens from your text in order to handle some more matching cases. When does this approach still fail? Hint: Use the different options for pos in order to handle different types of words such as nouns, verbs etc.\n\n\ntext = \"The company's latest quarterly earnings reports exceeded analysts' expectations, driving up the stock price. However, concerns about future growth prospects weighed on investor sentiment. The CEO announced plans to diversify the company's product portfolio and expand into new markets, aiming to sustain long-term profitability. The marketing team launched a new advertising campaign to promote the company's flagship product, targeting key demographics. Despite challenges in the competitive landscape, the company remains committed to innovation and customer satisfaction.\"\n\n\nkeywords = [\n    \"Announce\", \n    \"Aim\",\n    \"Earnings\",\n    \"Quarter\",\n    \"Report\",\n    \"Investor\",\n    \"Analysis\",\n    \"Market\",\n    \"Diversity\",\n    \"Product portfolio\",\n    \"Advertisment\",\n    \"Stock\",\n    \"Landscpe\" # yes, this is here on purpose\n]\n\n\n\nShow solution\n\n\nfrom pprint import pprint\nfrom nltk.tokenize import wordpunct_tokenize\n\ntext_token = wordpunct_tokenize(text=text.lower())\ndetected_words = [\n    (keyword, keyword.lower() in text_token) for keyword in keywords\n]\npprint(detected_words)\nprint(f\"\\nDetected {sum([x[1] for x in detected_words])}/{len(keywords)} words.\")\n\n[('Announce', False),\n ('Aim', False),\n ('Earnings', True),\n ('Quarter', False),\n ('Report', False),\n ('Investor', True),\n ('Analysis', False),\n ('Market', False),\n ('Diversity', False),\n ('Product portfolio', False),\n ('Advertisment', False),\n ('Stock', True),\n ('Landscpe', False)]\n\nDetected 3/13 words.\n\n\n\nfrom nltk.stem import WordNetLemmatizer\n\nwnl = WordNetLemmatizer()\n\nlemmatized_text_token = [\n    wnl.lemmatize(w) for w in text_token\n]\ndetected_words = [\n    (keyword, keyword.lower() in lemmatized_text_token) for keyword in keywords\n]\npprint(detected_words)\nprint(f\"\\nDetected {sum([x[1] for x in detected_words])}/{len(keywords)} words.\")\n\n[('Announce', False),\n ('Aim', False),\n ('Earnings', True),\n ('Quarter', False),\n ('Report', True),\n ('Investor', True),\n ('Analysis', False),\n ('Market', True),\n ('Diversity', False),\n ('Product portfolio', False),\n ('Advertisment', False),\n ('Stock', True),\n ('Landscpe', False)]\n\nDetected 5/13 words.\n\n\n\nfully_lemmatized_text_token = []\n\nfor token in text_token:\n    lemmatized_token = token\n    for pos in [\"n\", \"v\", \"a\"]:\n        lemmatized_token = wnl.lemmatize(token, pos=pos)\n        \n        fully_lemmatized_text_token.append(lemmatized_token)\n\ndetected_words = [\n    (keyword, keyword.lower() in fully_lemmatized_text_token) for keyword in keywords\n]\npprint(detected_words)    \nprint(f\"\\nDetected {sum([x[1] for x in detected_words])}/{len(keywords)} words.\")  \n        \n\n[('Announce', True),\n ('Aim', True),\n ('Earnings', True),\n ('Quarter', False),\n ('Report', True),\n ('Investor', True),\n ('Analysis', False),\n ('Market', True),\n ('Diversity', False),\n ('Product portfolio', False),\n ('Advertisment', False),\n ('Stock', True),\n ('Landscpe', False)]\n\nDetected 7/13 words.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Exercise: Word matching"
    ]
  },
  {
    "objectID": "nlp/exercises/ex_tokenization.html",
    "href": "nlp/exercises/ex_tokenization.html",
    "title": "Exercise: Sentence tokenization",
    "section": "",
    "text": "Task: Write a sentence tokenizer that takes the given paragraph and tokenizes it into sentences. Then, count the number of sentences and display the result.\nInstructions:\n\nStart with just a simple punctuation (.) as the delimiter for sentences.\nCheck out the regex library re and the function re.split to include also other delimiters. Try out the following regex r'[.:;!?]\\s*'.\n\n\nparagraph = \"The distant planet, its surface shrouded in mystery and intrigue! With its swirling clouds and alien landscapes, the planet: a tantalizing enigma to explorers and scientists alike? Oh, the wonders it conceals: ancient ruins and extraterrestrial life forms, waiting to be discovered! As the spacecraft descended through the atmosphere, anticipation filled the hearts of the crew. Little did they know, their journey was about to unveil secrets beyond their wildest imagination.\"\n\n\n\nShow code\n\n\nfrom typing import List\n\ndef tokenize_sentences_at_dot(paragraph: str) -&gt; List[str]:\n    sentence_tokens = paragraph.split(\".\")\n    sentence_tokens = [s.strip() for s in sentence_tokens if s.strip() != \"\"] # remove white space after .\n    return sentence_tokens\n\n\ntokenized_sentence = tokenize_sentences_at_dot(paragraph=paragraph)\nprint(f\"The paragraph contains {len(tokenized_sentence)} sentences.\")\n\nThe paragraph contains 2 sentences.\n\n\n\nimport re \n\ndef tokenize_sentences_at_punctuation(paragraph: str) -&gt; List[str]:\n    sentence_tokens = re.split(r'[.:;!?]\\s*', paragraph)\n    sentence_tokens = [s.strip() for s in sentence_tokens if s.strip() != \"\"] # remove white space after .\n    \n    return sentence_tokens\n\n\ntokenized_sentence = tokenize_sentences_at_punctuation(paragraph=paragraph)\nprint(f\"The paragraph contains {len(tokenized_sentence)} sentences.\")\n\nThe paragraph contains 7 sentences.\n\n\n\nfor sentence in tokenized_sentence:\n    print(sentence)\n\nThe distant planet, its surface shrouded in mystery and intrigue\nWith its swirling clouds and alien landscapes, the planet\na tantalizing enigma to explorers and scientists alike\nOh, the wonders it conceals\nancient ruins and extraterrestrial life forms, waiting to be discovered\nAs the spacecraft descended through the atmosphere, anticipation filled the hearts of the crew\nLittle did they know, their journey was about to unveil secrets beyond their wildest imagination\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Exercise: Sentence tokenization"
    ]
  },
  {
    "objectID": "nlp/statistical_text_analysis.html",
    "href": "nlp/statistical_text_analysis.html",
    "title": "Statistical text analysis",
    "section": "",
    "text": "So far we have mainly looked at the analysis of single words/token or n-grams. But what about the analysis of a full text? There are many approaches to this but a good way to get into the topic is a simple statistical analysis of a text. For starters, let’s simply count the number of appearances of each word in a text, also known as term frequency.\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\nfrom collections import Counter\nfrom typing import List\n\nfrom nltk.corpus import stopwords\n# python -m nltk.downloader stopwords -&gt; run this in your console once to get the stopwords\n\n\n# load a text from file\ntext = \"\"\nwith open(\"../assets/chapter1.txt\", \"r\") as file:  \n    for line in file:\n        text += line.strip()\n\n\ndef preprocess_text(text: str) -&gt; List[str]:\n    # tokenize text\n    tokens = wordpunct_tokenize(text.lower())\n\n    # remove punctuation\n    tokens = [t for t in tokens if t not in punctuation]\n\n    # remove stopwords\n    stop_words = stopwords.words(\"english\")\n    tokens = [t for t in tokens if t not in stop_words]\n\n    return tokens\n\n# count the most frequent words\ntokens = preprocess_text(text=text)\n\nfor t in Counter(tokens).most_common(15):\n    print(f\"{t[0]}: {t[1]}\")\n\none: 35\nwinston: 32\nface: 28\neven: 24\n--: 24\nbig: 22\ncould: 19\nparty: 18\nwould: 18\nmoment: 18\nlike: 17\nbrother: 15\ngoldstein: 15\ntelescreen: 14\nseemed: 14\nJust from the most frequent words, can you guess the text?\nIn many cases, just the simple number of appearances of a token in a text can determine its importance. The concept of counting the term frequency across multiple documents in order to create a fixed vocabulary is also known as bag of words.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Statistical text analysis"
    ]
  },
  {
    "objectID": "nlp/statistical_text_analysis.html#bag-of-words",
    "href": "nlp/statistical_text_analysis.html#bag-of-words",
    "title": "Statistical text analysis",
    "section": "Bag of Words",
    "text": "Bag of Words\nIf we do the same with multiple texts, we can build up a vocabulary of words and compare different texts to each other based on the appearance of terms.\n\nfrom collections import Counter\n\n\ndef create_bag_of_words(texts):\n    # Count the frequency of each word in the corpus\n    word_counts = Counter()\n    \n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Update word counts\n        word_counts.update(words)\n    \n    # Create vocabulary by sorting the words based on their frequency\n    vocabulary = [word for word, _ in sorted(word_counts.items())]\n    \n    # Create BoW vectors for each document\n    bow_vectors = []\n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Create a Counter object to count word frequencies\n        bow_vector = Counter(words)\n        \n        # Fill in missing words with zero counts\n        for word in vocabulary:\n            if word not in bow_vector:\n                bow_vector[word] = 0\n\n        # Sort the BoW vector based on the vocabulary order\n        sorted_bow_vector = [bow_vector[word] for word in vocabulary]\n        \n        # Append the BoW vector to the list\n        bow_vectors.append(sorted_bow_vector)\n    \n    return vocabulary, bow_vectors\n\n# Example texts\ntexts = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\",\n]\n\n# Create Bag of Words\nvocabulary, bow_vectors = create_bag_of_words(texts)\n\n# Print vocabulary\nprint(\"Vocabulary:\")\nprint(vocabulary)\n\n# Print BoW vectors\nprint(\"\\nBag of Words Vectors:\")\nfor i, bow_vector in enumerate(bow_vectors):\n    print(f\"Document {i + 1}: {bow_vector}\")\n\nVocabulary:\n['document', 'first', 'one', 'second', 'third']\n\nBag of Words Vectors:\nDocument 1: [1, 1, 0, 0, 0]\nDocument 2: [2, 0, 0, 1, 0]\nDocument 3: [0, 0, 1, 0, 1]\nDocument 4: [1, 1, 0, 0, 0]\n\n\nBag of words actually gives us some vector representation of our texts with respect to the given vocabulary. We can even calculate with these vectors and try to determine a similarity between the texts.\n\nimport numpy as np\n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -&gt; float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\n\nquery = bow_vectors[3]\n\nsimilarities = []\nfor i, bv in enumerate(bow_vectors):\n\n    similarity = cosine_similarity(\n            vec1=query, \n            vec2=bv\n        )\n\n    similarities.append(\n        (texts[i], round(similarity, 2))\n    )\n\nsimilarities\n\n[('This is the first document.', 1.0),\n ('This document is the second document.', 0.63),\n ('And this is the third one.', 0.0),\n ('Is this the first document?', 1.0)]",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Statistical text analysis"
    ]
  },
  {
    "objectID": "nlp/statistical_text_analysis.html#limitations-of-term-frequency-and-bag-of-words",
    "href": "nlp/statistical_text_analysis.html#limitations-of-term-frequency-and-bag-of-words",
    "title": "Statistical text analysis",
    "section": "Limitations of Term Frequency and Bag of Words",
    "text": "Limitations of Term Frequency and Bag of Words\nPure statistical text analysis methods like Term Frequency (also its extension Term Frequency-Inverse Document Frequency) or Bag of Words are usually a convenient starting point for text analysis. However, while they offer useful insights into textual data, they are not without their limitations.\nOne significant drawback is the sheer size of the vocabulary they handle. As the corpus grows, so does the vocabulary, which can become overwhelmingly large, leading to computational inefficiencies and increased memory requirements.\nMoreover, the mentioned methods struggle with out-of-vocabulary words. Since the vocabulary is fixed after the training has finished (the bag of words has been created with plenty of documents), words not present in the vocabulary are often either ignored, leading to information loss, or arbitrarily handled, potentially skewing the analysis results. This limitation becomes particularly pronounced in domains with specialized jargon or evolving lexicons.\nAnother critical limitation is the lack of context in these approaches. By treating each word independently and ignoring their sequential and syntactical relationships, TF and Bag of Words fail to capture the nuanced meanings embedded in language. This deficiency hampers their ability to comprehend subtleties such as sarcasm, irony, or metaphors, limiting their applicability in tasks requiring deeper semantic understanding.\nLast but not least, these methods lack structural awareness. They disregard the hierarchical and syntactic structures inherent in language, missing out on essential cues provided by sentence and paragraph boundaries. Thus they often struggle to differentiate between sentences with similar word distributions but differing in meaning or intent.\nAs a consequence, more sophisticated methods are required to really start understanding human language in a more sophisticated way. But before we get to such methods, let’s try one more thing.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Statistical text analysis"
    ]
  },
  {
    "objectID": "nlp/statistical_text_analysis.html#clustering-of-bag-of-word-vectors",
    "href": "nlp/statistical_text_analysis.html#clustering-of-bag-of-word-vectors",
    "title": "Statistical text analysis",
    "section": "Clustering of Bag of Word vectors",
    "text": "Clustering of Bag of Word vectors\nAs we’ve seen above, the idea of BoW already gives us a rather simple possibility to compare texts to each other. Whenever we can compare different entities to each other (here: the texts), a pretty straight-forward extension is to try and find clusters, that is, groups of similar texts.\nClustering using bag-of-word vectors is a common technique in text analysis for grouping similar documents together based on the similarity of their word distributions. As seen above, each document is represented as a high-dimensional vector, with each dimension corresponding to a unique word in the vocabulary and its value reflecting the frequency of that word in the document. By treating documents as points in a high-dimensional space, clustering algorithms such as K-means or hierarchical clustering can be applied to partition the documents into coherent groups. The similarity between documents is typically measured using distance metrics such as cosine similarity or Euclidean distance, which quantify the degree of overlap between their word distributions.\nOne advantage of using bag-of-word vectors for clustering is its simplicity and scalability. Since the vectors only capture the frequency of words without considering their order or context, the computational complexity remains manageable even for large datasets with extensive vocabularies. However, clustering based on bag-of-word vectors also has its limitations. One major drawback is the reliance on word frequency alone, which may overlook important semantic similarities between documents. Additionally, the curse of dimensionality can become a challenge as the size of the vocabulary increases, leading to decreased clustering performance and increased computational overhead. Despite these limitations, clustering using bag-of-word vectors serves as a foundational approach in text analysis, providing valuable insights into document similarity and aiding tasks such as document organization, topic modeling, and information retrieval.\nLet’s finish up with a small code example.\n\nfrom sklearn.cluster import KMeans\n\n# do K-means clustering\nn_clusters = 2  # specify the number of clusters\nkmeans = KMeans(n_clusters=n_clusters, n_init=\"auto\")\ncluster_labels = kmeans.fit_predict(bow_vectors)\n\nprint(\"Cluster labels:\\n\")\nfor i, label in enumerate(cluster_labels):\n    print(f\"Document {i + 1} belongs to Cluster {label + 1}\")\n\nCluster labels:\n\nDocument 1 belongs to Cluster 1\nDocument 2 belongs to Cluster 1\nDocument 3 belongs to Cluster 2\nDocument 4 belongs to Cluster 1\n\n\nLet’s maybe do this with some “texts” that are more likely to actually create some clusters. And, of course, there are packages that can do the Bag of Words for us. Here we use scikit-learn.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Example texts representing different topics\ntexts = [\n    \"apple orange banana\",\n    \"apple orange mango\",\n    \"banana apple kiwi\",\n    \"strawberry raspberry blueberry\",\n    \"strawberry raspberry blackberry\"\n]\n\n# create Bag of Words using CountVectorizer\nvectorizer = CountVectorizer()\nbow_matrix = vectorizer.fit_transform(texts)\n\nprint(\"Bag of Word vectors:\")\nprint(bow_matrix.toarray())\n\n# perform K-means clustering\nnum_clusters = 2\nkmeans = KMeans(n_clusters=num_clusters, n_init=\"auto\")\ncluster_labels = kmeans.fit_predict(bow_matrix)\n\nprint(\"\\nCluster labels:\")\nfor i, label in enumerate(cluster_labels):\n    print(f\"Document {i + 1} belongs to Cluster {label + 1}\")\n\nBag of Word vectors:\n[[1 1 0 0 0 0 1 0 0]\n [1 0 0 0 0 1 1 0 0]\n [1 1 0 0 1 0 0 0 0]\n [0 0 0 1 0 0 0 1 1]\n [0 0 1 0 0 0 0 1 1]]\n\nCluster labels:\nDocument 1 belongs to Cluster 2\nDocument 2 belongs to Cluster 2\nDocument 3 belongs to Cluster 2\nDocument 4 belongs to Cluster 1\nDocument 5 belongs to Cluster 1\n\n\n\n# Get the vocabulary\nvocabulary = vectorizer.get_feature_names_out()\n\n# print the vocabulary\nprint(\"Vocabulary:\")\nprint(vocabulary)\n\n# print the Bag of Words matrix with corresponding words\nprint(\"\\nBag of Words matrix with corresponding words:\")\nbow_matrix_array = bow_matrix.toarray()\nfor i, document_vector in enumerate(bow_matrix_array):\n    words_in_document = [(word, frequency) for word, frequency in zip(vocabulary, document_vector) if frequency &gt; 0]\n    print(f\"Document {i + 1}: {words_in_document}\")\n\nVocabulary:\n['apple' 'banana' 'blackberry' 'blueberry' 'kiwi' 'mango' 'orange'\n 'raspberry' 'strawberry']\n\nBag of Words matrix with corresponding words:\nDocument 1: [('apple', 1), ('banana', 1), ('orange', 1)]\nDocument 2: [('apple', 1), ('mango', 1), ('orange', 1)]\nDocument 3: [('apple', 1), ('banana', 1), ('kiwi', 1)]\nDocument 4: [('blueberry', 1), ('raspberry', 1), ('strawberry', 1)]\nDocument 5: [('blackberry', 1), ('raspberry', 1), ('strawberry', 1)]",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Statistical text analysis"
    ]
  },
  {
    "objectID": "nlp/overview.html",
    "href": "nlp/overview.html",
    "title": "Overview of NLP",
    "section": "",
    "text": "In order to understand and appreciate very advanced topics such as Large Language Models, it is often helpful to get a quick overview of the history and how things developed. So let’s get started with a few basics.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Overview of NLP"
    ]
  },
  {
    "objectID": "nlp/overview.html#a-short-history-of-natural-language-processing",
    "href": "nlp/overview.html#a-short-history-of-natural-language-processing",
    "title": "Overview of NLP",
    "section": "A short history of Natural Language Processing",
    "text": "A short history of Natural Language Processing\nThe field of Natural Language Processing (NLP) has undergone a remarkable evolution, spanning decades and driven by the convergence of computer science, artificial intelligence, and linguistics. From its nascent stages to its current state, NLP has witnessed transformative shifts, propelled by groundbreaking research and technological advancements. Today, it stands as a testament to humanity’s quest to bridge the gap between human language and machine comprehension. The journey through NLP’s history offers profound insights into its trajectory and the challenges encountered along the way.\n\nEarly Days: Rule-Based Approaches (1960s-1980s)\nIn its infancy, NLP relied heavily on rule-based approaches, where researchers painstakingly crafted sets of linguistic rules to analyze and manipulate text. This period, spanning from the 1960s to the 1980s, saw significant efforts in tasks such as part-of-speech tagging, named entity recognition, and machine translation. However, rule-based systems struggled to cope with the inherent ambiguity and complexity of natural language. Different languages presented unique challenges, necessitating the development of language-specific rulesets. Despite their limitations, rule-based approaches laid the groundwork for future advancements in NLP.\n\n\nRise of Statistical Methods (1990s-2000s)\nThe 1990s marked a pivotal shift in NLP with the emergence of statistical methods as a viable alternative to rule-based approaches. Researchers began harnessing the power of statistics and probabilistic models to analyze large corpora of text. Techniques like Hidden Markov Models and Conditional Random Fields gained prominence, offering improved performance in tasks such as text classification, sentiment analysis, and information extraction. Statistical methods represented a departure from rigid rule-based systems, allowing for greater flexibility and adaptability. However, they still grappled with the nuances and intricacies of human language, particularly in handling ambiguity and context.\n\n\nMachine Learning Revolution (2010s)\nThe advent of the 2010s witnessed a revolution in NLP fueled by the rise of machine learning, particularly deep learning. With the availability of vast amounts of annotated data and unprecedented computational power, researchers explored neural network architectures tailored for NLP tasks. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) gained traction, demonstrating impressive capabilities in tasks such as sentiment analysis, text classification, and sequence generation. These models represented a significant leap forward in NLP, enabling more nuanced and context-aware language processing.\n\n\nLarge Language Models: Transformers (2010s-Present)\nThe latter half of the 2010s heralded the rise of large language models, epitomized by the revolutionary Transformer architecture. Powered by self-attention mechanisms, Transformers excel at capturing long-range dependencies in text and generating coherent and contextually relevant responses. Pre-trained on massive text corpora, models like GPT (Generative Pre-trained Transformer) have achieved unprecedented performance across a wide range of NLP tasks, including machine translation, question-answering, and language understanding. Their ability to leverage vast amounts of data and learn intricate patterns has propelled NLP to new heights of sophistication.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Overview of NLP"
    ]
  },
  {
    "objectID": "nlp/overview.html#challenges-in-nlp",
    "href": "nlp/overview.html#challenges-in-nlp",
    "title": "Overview of NLP",
    "section": "Challenges in NLP",
    "text": "Challenges in NLP\nDespite the remarkable progress, NLP grapples with a myriad of challenges that continue to shape its trajectory:\n\nAmbiguity of Language: The inherent ambiguity of natural language poses significant challenges in accurately interpreting meaning, especially in tasks like sentiment analysis and named entity recognition.\nDifferent Languages: NLP systems often struggle with languages other than English, facing variations in syntax, semantics, and cultural nuances, requiring tailored approaches for each language.\nBias: NLP models can perpetuate biases present in the training data, leading to unfair or discriminatory outcomes, particularly in tasks like text classification and machine translation.\nImportance of Context: Understanding context is paramount for NLP tasks, as the meaning of words and phrases can vary drastically depending on the surrounding context.\nWorld Knowledge: NLP systems lack comprehensive world knowledge, hindering their ability to understand references, idioms, and cultural nuances embedded in text.\nCommon Sense Reasoning: Despite advancements, NLP models still struggle with common sense reasoning, often producing nonsensical or irrelevant responses in complex scenarios.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Overview of NLP"
    ]
  },
  {
    "objectID": "nlp/overview.html#classic-nlp-tasksapplications",
    "href": "nlp/overview.html#classic-nlp-tasksapplications",
    "title": "Overview of NLP",
    "section": "Classic NLP tasks/applications",
    "text": "Classic NLP tasks/applications\n\nPart-of-Speech Tagging\nPart-of-speech tagging involves labeling each word in a sentence with its corresponding grammatical category, such as noun, verb, adjective, or adverb. For example, in the sentence “The cat is sleeping,” part-of-speech tagging would identify “cat” as a noun and “sleeping” as a verb. This task is crucial for many NLP applications, including language understanding, information retrieval, and machine translation. Accurate part-of-speech tagging lays the foundation for deeper linguistic analysis and improves the performance of downstream tasks.\n\n\nCode example\n\n\nimport spacy\n\n# Load the English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example text\ntext = \"The sun sets behind the mountains, casting a golden glow across the sky.\"\n\n# Process the text with spaCy\ndoc = nlp(text)\n\n# Find the maximum length of token text and POS tag\nmax_token_length = max(len(token.text) for token in doc)\nmax_pos_length = max(len(token.pos_) for token in doc)\n\n# Print each token along with its part-of-speech tag\nfor token in doc:\n    print(f\"Token: {token.text.ljust(max_token_length)} | POS Tag: {token.pos_.ljust(max_pos_length)}\")\n\nToken: The       | POS Tag: DET  \nToken: sun       | POS Tag: NOUN \nToken: sets      | POS Tag: VERB \nToken: behind    | POS Tag: ADP  \nToken: the       | POS Tag: DET  \nToken: mountains | POS Tag: NOUN \nToken: ,         | POS Tag: PUNCT\nToken: casting   | POS Tag: VERB \nToken: a         | POS Tag: DET  \nToken: golden    | POS Tag: ADJ  \nToken: glow      | POS Tag: NOUN \nToken: across    | POS Tag: ADP  \nToken: the       | POS Tag: DET  \nToken: sky       | POS Tag: NOUN \nToken: .         | POS Tag: PUNCT\n\n\n\n\n\nNamed Entity Recognition\nNamed Entity Recognition (NER) involves identifying and classifying named entities in text, such as people, organizations, locations, dates, and more. For instance, in the sentence “Apple is headquartered in Cupertino,” NER would identify “Apple” as an organization and “Cupertino” as a location. NER is essential for various applications, including information retrieval, document summarization, and question-answering systems. Accurate NER enables machines to extract meaningful information from unstructured text data.\n\n\nCode example\n\n\nimport spacy\n\n# Load the English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example text\ntext = \"Apple is considering buying a U.K. based startup called LanguageHero located in London for $1 billion.\"\n\n# Process the text with spaCy\ndoc = nlp(text)\n\n# Print each token along with its Named Entity label\nfor ent in doc.ents:\n    print(f\"Entity: {ent.text.ljust(20)} | Label: {ent.label_}\")\n\nEntity: Apple                | Label: ORG\nEntity: U.K.                 | Label: GPE\nEntity: LanguageHero         | Label: PRODUCT\nEntity: London               | Label: GPE\nEntity: $1 billion           | Label: MONEY\n\n\n\n\n\nMachine Translation\nMachine Translation (MT) aims to automatically translate text from one language to another, facilitating communication across language barriers. For example, translating a sentence from English to Spanish or vice versa. MT systems utilize sophisticated algorithms and linguistic models to generate accurate translations while preserving the original meaning and nuances of the text. MT has numerous practical applications, including cross-border communication, localization of software and content, and global commerce.\n\n\nSentiment Analysis\nSentiment Analysis involves analyzing text data to determine the sentiment or opinion expressed within it, such as positive, negative, or neutral. For instance, analyzing product reviews to gauge customer satisfaction or monitoring social media sentiment towards a brand. Sentiment Analysis employs machine learning algorithms to classify text based on sentiment, enabling businesses to understand customer feedback, track public opinion, and make data-driven decisions.\n\n\nCode example\n\n\n# python -m textblob.download_corpora\n\nfrom textblob import TextBlob\n\n# Example text\ntext = \"I love TextBlob! It's an amazing library for natural language processing.\"\n\n# Perform sentiment analysis with TextBlob\nblob = TextBlob(text)\nsentiment_score = blob.sentiment.polarity\n\n# Determine sentiment label based on sentiment score\nif sentiment_score &gt; 0:\n    sentiment_label = \"Positive\"\nelif sentiment_score &lt; 0:\n    sentiment_label = \"Negative\"\nelse:\n    sentiment_label = \"Neutral\"\n\n# Print sentiment analysis results\nprint(f\"Text: {text}\")\nprint(f\"Sentiment Score: {sentiment_score:.2f}\")\nprint(f\"Sentiment Label: {sentiment_label}\")\n\nText: I love TextBlob! It's an amazing library for natural language processing.\nSentiment Score: 0.44\nSentiment Label: Positive\n\n\n\n\n\nText Classification\nText Classification is the task of automatically categorizing text documents into predefined categories or classes. For example, classifying news articles into topics like politics, sports, or entertainment. Text Classification is widely used in various domains, including email spam detection, sentiment analysis, and content categorization. It enables organizations to organize and process large volumes of textual data efficiently, leading to improved decision-making and information retrieval.\n\n\nCode example\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder\n\n# Example labeled dataset\ntexts = [\n    \"I love this product!\",\n    \"This product is terrible.\",\n    \"Great service, highly recommended.\",\n    \"I had a bad experience with this company.\",\n]\nlabels = [\n    \"Positive\",\n    \"Negative\",\n    \"Positive\",\n    \"Negative\",\n]\n\n# Create a TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n\n# Encode labels as integers\nlabel_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(labels)\n\n# Create a pipeline with TF-IDF vectorizer and SVM classifier\nclassifier = make_pipeline(vectorizer, SVC(kernel='linear'))\n\n# Train the classifier\nclassifier.fit(texts, encoded_labels)\n\n# Example test text\ntest_text = \"I love what this product can do.\"\n\n# Predict the label for the test text\npredicted_label = classifier.predict([test_text])[0]\n\n# Decode the predicted label back to original label\npredicted_label_text = label_encoder.inverse_transform([predicted_label])[0]\n\n# Print the predicted label\nprint(f\"Text: {test_text}\")\nprint(f\"Predicted Label: {predicted_label_text}\")\n\nText: I love what this product can do.\nPredicted Label: Positive\n\n\n\n\n\nInformation Extraction\nInformation Extraction involves automatically extracting structured information from unstructured text data, such as documents, articles, or web pages. This includes identifying entities, relationships, and events mentioned in the text. For example, extracting names of people mentioned in news articles or detecting company acquisitions from financial reports. Information Extraction plays a crucial role in tasks like knowledge base construction, data integration, and business intelligence.\n\n\nQuestion-Answering\nQuestion-Answering (QA) systems aim to automatically generate accurate answers to user queries posed in natural language. These systems comprehend the meaning of questions and retrieve relevant information from a knowledge base or text corpus to provide precise responses. For example, answering factual questions like “Who is the president of the United States?” or “What is the capital of France?”. QA systems are essential for information retrieval, virtual assistants, and educational applications, enabling users to access information quickly and efficiently.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Overview of NLP"
    ]
  },
  {
    "objectID": "ethics/data_privacy.html",
    "href": "ethics/data_privacy.html",
    "title": "Data Privacy",
    "section": "",
    "text": "In the rapidly evolving landscape of artificial intelligence and natural language processing, using the power of language models comes with a set of critical considerations regarding data security. Whenever we are starting to use these technologies, understanding the importance of safeguarding data integrity, confidentiality, and privacy is paramount to ensure responsible and ethical use.\nData security concerns in language model applications\n\nPrivacy risks: Language models often require access to vast amounts of textual data for training, which may include sensitive or personally identifiable information (PII). Improper handling of this data can pose significant privacy risks, especially in applications involving user-generated content or personal communications.\nData breaches: The storage and transmission of large datasets used to train language models can be susceptible to data breaches, unauthorized access, or cyberattacks. A breach of sensitive training data can lead to the exposure of confidential information, intellectual property theft, or reputational damage.\nAdversarial attacks: Language models are vulnerable to adversarial attacks, where malicious actors manipulate input data to deceive or exploit the model’s vulnerabilities. Adversarial examples crafted to evade detection or trigger undesirable behavior can compromise the integrity and reliability of language model outputs.\nEthical considerations: Language models trained on biased or unethical datasets may inadvertently perpetuate harmful stereotypes, discriminatory language, or misinformation, raising ethical concerns about the responsible use of AI technology and its potential impact on society. See also here.\n\nMitigating data security risks\n\nData minimization: Adopting data minimization practices by limiting the collection, storage, and retention of sensitive or unnecessary data can mitigate privacy risks and reduce the attack surface for potential breaches.\nEncryption and secure transmission: Implementing robust encryption protocols and secure transmission mechanisms for handling data during training, inference, and storage can safeguard against unauthorized access and data interception.\nAnonymization and differential privacy: Employing anonymization techniques and differential privacy mechanisms to anonymize or obfuscate sensitive information in datasets can protect individual privacy while preserving the utility of the data for training language models.\nThreat modeling and risk assessment: Conducting comprehensive threat modeling and risk assessments to identify potential security vulnerabilities, anticipate adversarial scenarios, and develop proactive strategies for mitigating data security risks.\n\nEthical considerations and transparency\n\nTransparency and accountability: Promoting transparency and accountability in the development and deployment of language models by adhering to ethical guidelines, disclosing data sources and training methodologies, and enabling independent scrutiny and oversight.\nInformed consent and user rights: Prioritizing informed consent, user autonomy, and data subject rights by providing clear and accessible information about data usage, consent options, and mechanisms for data access, correction, or deletion.\nResponsible AI governance: Establishing robust governance frameworks, ethical guidelines, and regulatory mechanisms to ensure responsible and ethical use of language models, mitigate potential harms, and uphold principles of fairness, accountability, and transparency.\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Ethical Considerations",
      "Data Privacy"
    ]
  },
  {
    "objectID": "slides/nlp/tokenization.html#tokenization",
    "href": "slides/nlp/tokenization.html#tokenization",
    "title": "Tokenization",
    "section": "Tokenization",
    "text": "Tokenization\n\nsentence = \"I love reading science fiction books or books about science.\"\n\n \n\n\n\n\n\n\n\n\nDefinition\n\n\nTokenization is the process of breaking down a text into smaller units called tokens.\n\n\n\n\n\n \n\n\ntokenized_sentence = sentence.split(\" \")\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science.']"
  },
  {
    "objectID": "slides/nlp/tokenization.html#counting-token",
    "href": "slides/nlp/tokenization.html#counting-token",
    "title": "Tokenization",
    "section": "Counting token",
    "text": "Counting token\n\nfrom collections import Counter\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(3))\n\n[('books', 2), ('I', 1), ('love', 1)]\n\n\n \n\n\ntokenized_sentence = sentence.replace(\".\", \" \").split(\" \")\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n\n[('science', 2), ('books', 2)]"
  },
  {
    "objectID": "slides/nlp/tokenization.html#nltk-tokenization",
    "href": "slides/nlp/tokenization.html#nltk-tokenization",
    "title": "Tokenization",
    "section": "NLTK tokenization",
    "text": "NLTK tokenization\n\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\n\ntokenized_sentence = wordpunct_tokenize(sentence)\ntokenized_sentence = [t for t in tokenized_sentence if t not in punctuation]\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science']"
  },
  {
    "objectID": "slides/nlp/tokenization.html#lemmatization",
    "href": "slides/nlp/tokenization.html#lemmatization",
    "title": "Tokenization",
    "section": "Lemmatization",
    "text": "Lemmatization\n\nReduce words to their base or canonical form\nRepresents the dictionary form of a word (lemma)\nStandardizes words for better text analysis accuracy\nExample: meeting –&gt; meet (verb)"
  },
  {
    "objectID": "slides/nlp/tokenization.html#lemmatization-with-wordnet-nouns",
    "href": "slides/nlp/tokenization.html#lemmatization-with-wordnet-nouns",
    "title": "Tokenization",
    "section": "Lemmatization with WordNet: Nouns",
    "text": "Lemmatization with WordNet: Nouns\n\nfrom nltk.stem import WordNetLemmatizer\n\nsentence = \"The three brothers went over three big bridges\"\n\nwnl = WordNetLemmatizer()\n\nlemmatized_sentence_token = [\n    wnl.lemmatize(w, pos=\"n\") for w in sentence.split(\" \")\n]\n\nprint(lemmatized_sentence_token)\n\n\n\n['The', 'three', 'brother', 'went', 'over', 'three', 'big', 'bridge']"
  },
  {
    "objectID": "slides/nlp/tokenization.html#lemmatization-with-wordnet-verbs",
    "href": "slides/nlp/tokenization.html#lemmatization-with-wordnet-verbs",
    "title": "Tokenization",
    "section": "Lemmatization with WordNet: Verbs",
    "text": "Lemmatization with WordNet: Verbs\n\nlemmatized_sentence_token = [\n    wnl.lemmatize(w, pos=\"v\") for w in sentence.split(\" \")\n]\n\nprint(lemmatized_sentence_token)\n\n\n\n['The', 'three', 'brothers', 'go', 'over', 'three', 'big', 'bridge']"
  },
  {
    "objectID": "slides/nlp/tokenization.html#lemmatization-with-wordnet-and-pos-tagging",
    "href": "slides/nlp/tokenization.html#lemmatization-with-wordnet-and-pos-tagging",
    "title": "Tokenization",
    "section": "Lemmatization with WordNet and POS-tagging",
    "text": "Lemmatization with WordNet and POS-tagging\n\npos_dict = {\n  \"brothers\": \"n\", \n  \"went\": \"v\",\n  \"big\": \"a\",\n  \"bridges\": \"n\"\n}\n\nlemmatized_sentence_token = []\nfor token in sentence.split(\" \"):\n    if token in pos_dict:\n        lemma = wnl.lemmatize(token, pos=pos_dict[token])\n    else: \n        lemma = token # leave as it is\n\n    lemmatized_sentence_token.append(lemma)\n\nprint(lemmatized_sentence_token)\n\n\n\n['The', 'three', 'brother', 'go', 'over', 'three', 'big', 'bridge']"
  },
  {
    "objectID": "slides/nlp/tokenization.html#bit-pair-encoding-why",
    "href": "slides/nlp/tokenization.html#bit-pair-encoding-why",
    "title": "Tokenization",
    "section": "Bit Pair Encoding: Why?",
    "text": "Bit Pair Encoding: Why?\n\nTokenization: Breaking text into smaller chunks (tokens)\nTraditional vocabularies: Fixed-size, memory-intensive\nBit pair encoding: Compression technique for large vocabularies"
  },
  {
    "objectID": "slides/nlp/tokenization.html#bit-pair-encoding-how",
    "href": "slides/nlp/tokenization.html#bit-pair-encoding-how",
    "title": "Tokenization",
    "section": "Bit Pair Encoding: How?",
    "text": "Bit Pair Encoding: How?\n\nPair Identification: Identifies frequent pairs of characters\nReplacement with Single Token: Replaces pairs with single token\nIterative Process: Continues until stopping criterion met\nVocabulary Construction: Construct vocabulary with single tokens\nEncoding and Decoding: Text encoded and decoded using constructed vocabulary"
  },
  {
    "objectID": "slides/nlp/tokenization.html#bit-pair-encoding-pros-and-cons",
    "href": "slides/nlp/tokenization.html#bit-pair-encoding-pros-and-cons",
    "title": "Tokenization",
    "section": "Bit Pair Encoding: Pros and Cons",
    "text": "Bit Pair Encoding: Pros and Cons\n\nEfficient Memory Usage\nRetains Information\nFlexibility\nComputational Overhead\nLoss of Granularity\n\n\n\nReduces vocabulary size, efficient memory usage\nCaptures frequent character pairs, retains linguistic information\nAdaptable to different tokenization strategies and corpus characteristics\nIterative nature can be computationally intensive\nMay lead to loss of granularity, especially for rare words\nEffectiveness depends on tokenization strategy and corpus characteristics\n\n\n\n\n\n\nSeminar: LLM, WiSe 2024/25"
  },
  {
    "objectID": "slides/about/projects.html#question-answering-chatbot",
    "href": "slides/about/projects.html#question-answering-chatbot",
    "title": "Projects: Large Language Models",
    "section": "Question-Answering Chatbot",
    "text": "Question-Answering Chatbot\nBuild a chatbot that can answer questions posed by users on a specific topic provided in form of documents. Users input their questions, the chatbot retrieves relevant information from a pre-defined set of documents, and uses the information to answer the question."
  },
  {
    "objectID": "slides/about/projects.html#document-tagging-classification",
    "href": "slides/about/projects.html#document-tagging-classification",
    "title": "Projects: Large Language Models",
    "section": "Document tagging / classification",
    "text": "Document tagging / classification\nUse GPT and its tools (e.g., function calls) and/or embeddings to classify documents or assign tags to them. Example: Sort bug reports or complaints into categories depending on the problem."
  },
  {
    "objectID": "slides/about/projects.html#clustering-of-text-based-entities",
    "href": "slides/about/projects.html#clustering-of-text-based-entities",
    "title": "Projects: Large Language Models",
    "section": "Clustering of text-based entities",
    "text": "Clustering of text-based entities\nCreate a small tool that can cluster text-based entities based on embeddings, for example, groups of texts or keywords. Example: Structure a folder of text files based on their content."
  },
  {
    "objectID": "slides/about/projects.html#text-based-rpg-game",
    "href": "slides/about/projects.html#text-based-rpg-game",
    "title": "Projects: Large Language Models",
    "section": "Text-based RPG Game",
    "text": "Text-based RPG Game\nDevelop a text-based role-playing game where players interact with characters and navigate through a story generated by GPT. Players make choices that influence the direction of the narrative."
  },
  {
    "objectID": "slides/about/projects.html#sentiment-analysis-tool",
    "href": "slides/about/projects.html#sentiment-analysis-tool",
    "title": "Projects: Large Language Models",
    "section": "Sentiment Analysis Tool",
    "text": "Sentiment Analysis Tool\nBuild an app that analyzes the sentiment of text inputs (e.g., social media posts, customer reviews) using GPT. Users can input text, and the app provides insights into the overall sentiment expressed in the text."
  },
  {
    "objectID": "slides/about/projects.html#text-summarization-tool",
    "href": "slides/about/projects.html#text-summarization-tool",
    "title": "Projects: Large Language Models",
    "section": "Text Summarization Tool",
    "text": "Text Summarization Tool\nCreate an application that summarizes long blocks of text into shorter, concise summaries. Users can input articles, essays, or documents, and the tool generates a summarized version."
  },
  {
    "objectID": "slides/about/projects.html#language-translation-tool",
    "href": "slides/about/projects.html#language-translation-tool",
    "title": "Projects: Large Language Models",
    "section": "Language Translation Tool",
    "text": "Language Translation Tool\nBuild a simple translation app that utilizes GPT to translate text between different languages. Users can input text in one language, and the app outputs the translated text in the desired language. Has to include some nice tweaks."
  },
  {
    "objectID": "slides/about/projects.html#personalized-recipe-generator",
    "href": "slides/about/projects.html#personalized-recipe-generator",
    "title": "Projects: Large Language Models",
    "section": "Personalized Recipe Generator",
    "text": "Personalized Recipe Generator\nDevelop an app that generates personalized recipes based on user preferences and dietary restrictions. Users input their preferred ingredients and dietary needs, and the app generates custom recipes using GPT."
  },
  {
    "objectID": "slides/about/projects.html#lyrics-generator",
    "href": "slides/about/projects.html#lyrics-generator",
    "title": "Projects: Large Language Models",
    "section": "Lyrics Generator",
    "text": "Lyrics Generator\nCreate a lyrics generation tool that generates lyrics based on user input such as themes, music style, emotions, or keywords. Users can explore different poetic styles and themes generated by GPT."
  },
  {
    "objectID": "slides/about/projects.html#tools",
    "href": "slides/about/projects.html#tools",
    "title": "Projects: Large Language Models",
    "section": "Tools",
    "text": "Tools\n\nYou can use everything in the Jupyterlab (put pip list in a terminal to see all Python packages)\nIf there are specific packages you need, we can organize them\nYou can simply build your application in a Jupyter notebook!\nOr: Use Dash!"
  },
  {
    "objectID": "slides/about/projects.html#dash",
    "href": "slides/about/projects.html#dash",
    "title": "Projects: Large Language Models",
    "section": "Dash",
    "text": "Dash\nPut the following files into your home in the Jupyterlab:\nmy_layout.py\n\nfrom dash import html\nfrom dash import dcc\n\n\nlayout = html.Div([\n    html.H1(\"Yeay, my app!\"),\n    html.Div([\n        html.Label(\"Enter your text:\"),\n        dcc.Input(id='input-text', type='text', value=''),\n        html.Button('Submit', id='submit-btn', n_clicks=0),\n    ]),\n    html.Div(id='output-container-button')\n])"
  },
  {
    "objectID": "slides/python_intro/string_operations.html#strings-in-nlp",
    "href": "slides/python_intro/string_operations.html#strings-in-nlp",
    "title": "String Operations in Python",
    "section": "Strings in NLP",
    "text": "Strings in NLP\n\n\nWhat Are Strings?\n\nStrings represent text data in Python.\nUsed to store sequences of characters (letters, digits, punctuation).\nCentral to NLP tasks where text manipulation is required.\n\n\nsentence = \"Natural Language Processing\"\nprint(sentence)\n\nNatural Language Processing\n\n\n\n\nNLP and Strings\n\nWhy strings in NLP?\n\nTo tokenize, clean, and analyze text data.\nEach word in a sentence is processed as a string.\n\n\n\ntoken = \"word\"\nprint(len(token))  # Outputs: 4\n\n4\n\n\n\nLength of strings helps in tokenization."
  },
  {
    "objectID": "slides/python_intro/string_operations.html#concatenating-strings",
    "href": "slides/python_intro/string_operations.html#concatenating-strings",
    "title": "String Operations in Python",
    "section": "Concatenating Strings",
    "text": "Concatenating Strings\n\n\nWhat is Concatenation?\n\nConcatenation joins two or more strings together.\nYou can use the + operator or join() method for concatenation.\n\n\ngreeting = \"Hello, \" + \"world!\"\nprint(greeting)  # Outputs: Hello, world!\n\nHello, world!\n\n\n\n\nNLP Example: Joining Words\n\nOften in NLP, you need to combine words (tokens) back into sentences.\n\n\nwords = [\"NLP\", \"is\", \"fun\"]\nsentence = \" \".join(words)\nprint(sentence)  # Outputs: NLP is fun\n\nNLP is fun"
  },
  {
    "objectID": "slides/python_intro/string_operations.html#accessing-and-slicing-strings",
    "href": "slides/python_intro/string_operations.html#accessing-and-slicing-strings",
    "title": "String Operations in Python",
    "section": "Accessing and Slicing Strings",
    "text": "Accessing and Slicing Strings\n\n\nAccessing Characters\n\nEach character in a string has an index.\nYou can access them using square brackets.\n\n\nword = \"token\"\nprint(word[0])  # Outputs: t\n\nt\n\n\n\n\nSlicing Strings\n\nSlicing extracts part of a string using [start:end].\n\n\nphrase = \"language model\"\nprint(phrase[0:8])  # Outputs: language\n\nlanguage\n\n\n\nUseful in NLP when extracting parts of text."
  },
  {
    "objectID": "slides/python_intro/string_operations.html#modifying-strings",
    "href": "slides/python_intro/string_operations.html#modifying-strings",
    "title": "String Operations in Python",
    "section": "Modifying Strings",
    "text": "Modifying Strings\n\n\nChanging Case\n\nStrings offer methods like upper() and lower() for case modification.\n\n\ntext = \"Natural Language Processing\"\nprint(text.lower())  # Outputs: natural language processing\n\nnatural language processing\n\n\n\n\nNLP Example: Normalizing Text\n\nText normalization often involves converting everything to lowercase to ensure uniformity.\n\n\nsentence = \"HELLO World!\"\nprint(sentence.lower())  # Outputs: hello world!\n\nhello world!\n\n\n\nThis is important for case-insensitive comparisons in NLP."
  },
  {
    "objectID": "slides/python_intro/string_operations.html#splitting-strings",
    "href": "slides/python_intro/string_operations.html#splitting-strings",
    "title": "String Operations in Python",
    "section": "Splitting Strings",
    "text": "Splitting Strings\n\n\nTokenization: Splitting Text\n\nThe split() method divides a string into a list of words.\n\n\nsentence = \"Tokenize this sentence.\"\ntokens = sentence.split(\" \")\nprint(tokens)  # Outputs: ['Tokenize', 'this', 'sentence.']\n\n['Tokenize', 'this', 'sentence.']\n\n\n\n\nTokenization in NLP\n\nTokenization is the process of breaking text into smaller units, often words or sentences.\nSplit text into tokens based on spaces or punctuation.\n\n\nsentence = \"Deep Learning and NLP\"\ntokens = sentence.split()\nprint(tokens)  # Outputs: ['Deep', 'Learning', 'and', 'NLP']\n\n['Deep', 'Learning', 'and', 'NLP']"
  },
  {
    "objectID": "slides/python_intro/string_operations.html#string-search",
    "href": "slides/python_intro/string_operations.html#string-search",
    "title": "String Operations in Python",
    "section": "String Search",
    "text": "String Search\n\n\nSearching in Strings\n\nUse in or find() to search for substrings.\n\n\ntext = \"machine learning is powerful\"\nprint(\"learning\" in text)  # Outputs: True\n\nTrue\n\n\n\n\nNLP: Searching for Keywords\n\nIn NLP, searching for specific words or phrases is common.\n\n\ntext = \"Neural networks are part of deep learning\"\nkeyword = \"deep\"\nprint(text.find(keyword))  # Outputs: 27 (index position)\n\n28\n\n\n\nUse this for keyword extraction in text analysis."
  },
  {
    "objectID": "slides/python_intro/string_operations.html#replacing-substrings",
    "href": "slides/python_intro/string_operations.html#replacing-substrings",
    "title": "String Operations in Python",
    "section": "Replacing Substrings",
    "text": "Replacing Substrings\n\n\nReplacing Text\n\nThe replace() method replaces parts of a string.\n\n\nsentence = \"I love machine learning\"\nsentence = sentence.replace(\"machine\", \"deep\")\nprint(sentence)  # Outputs: I love deep learning\n\nI love deep learning\n\n\n\n\nNLP Example: Text Replacement\n\nIn text preprocessing, you may need to replace or correct words.\n\n\ntext = \"Text analysis with NLP\"\nclean_text = text.replace(\"Text\", \"Document\")\nprint(clean_text)  # Outputs: Document analysis with NLP\n\nDocument analysis with NLP\n\n\n\nThis is helpful when cleaning or transforming data."
  },
  {
    "objectID": "slides/python_intro/string_operations.html#removing-whitespace-and-punctuation",
    "href": "slides/python_intro/string_operations.html#removing-whitespace-and-punctuation",
    "title": "String Operations in Python",
    "section": "Removing Whitespace and Punctuation",
    "text": "Removing Whitespace and Punctuation\n\n\nRemoving Extra Whitespace\n\nUse strip(), lstrip(), or rstrip() to remove unwanted spaces.\n\n\ntext = \"   clean me!   \"\nprint(text.strip())  # Outputs: \"clean me!\"\n\nclean me!\n\n\n\n\nRemoving Punctuation\n\nUse translate() to remove punctuation in strings for cleaning.\n\n\nimport string\nsentence = \"Hello, world!\"\ncleaned = sentence.translate(str.maketrans('', '', string.punctuation))\nprint(cleaned)  # Outputs: Hello world\n\nHello world\n\n\n\nThis is common in NLP preprocessing steps."
  },
  {
    "objectID": "slides/python_intro/string_operations.html#string-formatting",
    "href": "slides/python_intro/string_operations.html#string-formatting",
    "title": "String Operations in Python",
    "section": "String Formatting",
    "text": "String Formatting\n\n\nFormatting with f-strings\n\nUse f-strings for inserting variables into strings.\n\n\nname = \"NLP\"\nprint(f\"Welcome to {name} class!\")  # Outputs: Welcome to NLP class!\n\nWelcome to NLP class!\n\n\n\n\nNLP Use: Displaying Results\n\nUse string formatting to present results clearly.\n\n\nword = \"deep learning\"\nprint(f\"The word '{word}' has {len(word)} characters.\")\n\nThe word 'deep learning' has 13 characters.\n\n\n\nThis is useful for presenting text-based results from NLP models."
  },
  {
    "objectID": "slides/python_intro/string_operations.html#summary-of-string-operations",
    "href": "slides/python_intro/string_operations.html#summary-of-string-operations",
    "title": "String Operations in Python",
    "section": "Summary of String Operations",
    "text": "Summary of String Operations\n\nConcatenation: Joining strings together.\nAccessing and slicing: Extracting characters or parts of a string.\nModifying: Changing case or replacing parts of the string.\nSplitting: Tokenizing a sentence into words.\nSearching: Finding specific words in text.\nReplacing: Cleaning or transforming text.\nWhitespace/Punctuation removal: Important for cleaning data.\n\n\n\n\n\nSeminar: LLM, WiSe 2024/25"
  },
  {
    "objectID": "slides/python_intro/functions.html#functions-in-python",
    "href": "slides/python_intro/functions.html#functions-in-python",
    "title": "Functions in Python",
    "section": "Functions in Python",
    "text": "Functions in Python\n\n\nWhat Are Functions?\n\nFunctions are reusable blocks of code that perform specific tasks.\nThey help organize code into manageable parts.\nEssential for writing modular and maintainable code, especially in NLP.\n\n\ndef function_name():\n    # Code to be executed\n    pass\n\n\n\nWhy Use Functions in NLP?\n\nSimplify complex processes like data cleaning, preprocessing, and feature extraction.\nPromote code reusability and clarity, making it easier to manage large projects.\n\n\ndef preprocess_text(text):\n    return text.lower()  # Example function to convert text to lowercase"
  },
  {
    "objectID": "slides/python_intro/functions.html#defining-functions",
    "href": "slides/python_intro/functions.html#defining-functions",
    "title": "Functions in Python",
    "section": "Defining Functions",
    "text": "Defining Functions\n\n\nBasic Function Definition\n\nUse the def keyword to define a function.\nYou can pass arguments for customization.\n\n\ndef greet():\n    print(\"Hello, welcome to Python!\")\n\n\n\nExample: Calling a Function\n\nCall a function by using its name followed by parentheses.\n\n\ngreet()  # Output: Hello, welcome to Python!\n\nHello, welcome to Python!"
  },
  {
    "objectID": "slides/python_intro/functions.html#functions-with-arguments",
    "href": "slides/python_intro/functions.html#functions-with-arguments",
    "title": "Functions in Python",
    "section": "Functions with Arguments",
    "text": "Functions with Arguments\n\n\nFunctions with Arguments\n\nAccept data inputs to customize behavior.\n\n\ndef greet(name):\n    print(f\"Hello, {name}!\")\n\n\n\nExample: Custom Greetings\n\nCall the function with different arguments.\n\n\ngreet(\"Alice\")  # Output: Hello, Alice!\ngreet(\"Bob\")    # Output: Hello, Bob!\n\nHello, Alice!\nHello, Bob!"
  },
  {
    "objectID": "slides/python_intro/functions.html#returning-values-from-functions",
    "href": "slides/python_intro/functions.html#returning-values-from-functions",
    "title": "Functions in Python",
    "section": "Returning Values from Functions",
    "text": "Returning Values from Functions\n\n\nReturning Values\n\nUse return to send back a value from the function.\n\n\ndef add_numbers(a, b):\n    return a + b\n\n\n\nExample: Storing Returned Values\n\nStore the returned value for further use.\n\n\nsum_result = add_numbers(10, 20)\nprint(sum_result)  # Output: 30\n\n30"
  },
  {
    "objectID": "slides/python_intro/functions.html#default-arguments",
    "href": "slides/python_intro/functions.html#default-arguments",
    "title": "Functions in Python",
    "section": "Default Arguments",
    "text": "Default Arguments\n\n\nDefault Arguments\n\nAssign default values to function arguments.\n\n\ndef greet(name=\"there\"):\n    print(f\"Hello, {name}!\")\n\n\n\nExample: Calling with Default Argument\n\nIf no argument is provided, the default value is used.\n\n\ngreet()         # Output: Hello, there!\ngreet(\"Alice\")  # Output: Hello, Alice!\n\nHello, there!\nHello, Alice!"
  },
  {
    "objectID": "slides/python_intro/functions.html#lambda-functions",
    "href": "slides/python_intro/functions.html#lambda-functions",
    "title": "Functions in Python",
    "section": "Lambda Functions",
    "text": "Lambda Functions\n\n\nWhat Are Lambda Functions?\n\nSmall anonymous functions written in a single line.\nUseful for simple operations.\n\n\nsquare_lambda = lambda x: x ** 2\n\n\n\nExample: Using Lambda Functions\n\nCall the lambda function directly.\n\n\nprint(square_lambda(5))  # Output: 25\n\n25"
  },
  {
    "objectID": "slides/python_intro/functions.html#functions-and-lists",
    "href": "slides/python_intro/functions.html#functions-and-lists",
    "title": "Functions in Python",
    "section": "Functions and Lists",
    "text": "Functions and Lists\n\n\nCombining Functions with Lists\n\nUse functions to process items in a list efficiently.\n\n\ndef square(x):\n    return x ** 2\n\nnumbers = [1, 2, 3, 4]\nsquares = [square(num) for num in numbers]\n\n\n\nOutput the Results\n\nPrint the resulting list after processing.\n\n\nprint(squares)  # Output: [1, 4, 9, 16]\n\n[1, 4, 9, 16]"
  },
  {
    "objectID": "slides/python_intro/functions.html#best-practices-with-functions",
    "href": "slides/python_intro/functions.html#best-practices-with-functions",
    "title": "Functions in Python",
    "section": "Best Practices with Functions",
    "text": "Best Practices with Functions\n\n\nBest Practices\n\nUse descriptive names for clarity.\nKeep functions short and focused on a single task.\n\n\ndef calculate_area(radius):\n    \"\"\"Calculate the area of a circle.\"\"\"\n    return 3.14 * radius ** 2\n\n\n\nImportance of Documentation\n\nWrite docstrings and comments to explain complex functions.\n\n\ndef process_data(data):\n    \"\"\"Process the input data.\"\"\"\n    # Implementation goes here"
  },
  {
    "objectID": "slides/python_intro/functions.html#summary",
    "href": "slides/python_intro/functions.html#summary",
    "title": "Functions in Python",
    "section": "Summary",
    "text": "Summary\n\n\n\nFunctions are essential for modular programming in Python.\nThey improve code readability and maintainability, especially in NLP applications.\nCombine functions with lists to process collections of data efficiently.\n\n\n\n\n\n\n\n\nSeminar: LLM, WiSe 2024/25"
  },
  {
    "objectID": "slides/python_intro/conditional_statements.html#conditional-statements-in-python",
    "href": "slides/python_intro/conditional_statements.html#conditional-statements-in-python",
    "title": "Conditional Statements in Python",
    "section": "Conditional Statements in Python",
    "text": "Conditional Statements in Python\n\n\nWhat Are Conditional Statements?\n\nControl the flow of your program based on conditions.\nThe most common statement is the if statement, which executes code only when a condition is true.\nUseful for decision-making in various applications, including NLP tasks.\n\n\n\nImportance in NLP\n\nAdjust processing based on conditions (e.g., text case, keyword presence).\nEnable flexible and dynamic responses to input data.\n\n\nword = \"HELLO\"\nif word.isupper():\n    print(\"The word is uppercase!\")  # Output: The word is uppercase!\n\nThe word is uppercase!"
  },
  {
    "objectID": "slides/python_intro/conditional_statements.html#the-if-statement",
    "href": "slides/python_intro/conditional_statements.html#the-if-statement",
    "title": "Conditional Statements in Python",
    "section": "The if Statement",
    "text": "The if Statement\n\n\nBasic Syntax of if\n\nCheck a condition and execute a block of code if it evaluates to True.\n\n\nx = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")  # Output: x is greater than 5\n\nx is greater than 5\n\n\n\n\nExplanation\n\nThe condition x &gt; 5 is checked.\nSince x is 10, the print statement executes."
  },
  {
    "objectID": "slides/python_intro/conditional_statements.html#the-else-statement",
    "href": "slides/python_intro/conditional_statements.html#the-else-statement",
    "title": "Conditional Statements in Python",
    "section": "The else Statement",
    "text": "The else Statement\n\n\nUsing the else Statement\n\nDefines what happens when the if condition is False.\n\n\nx = 3\nif x &gt; 5:\n    print(\"x is greater than 5\")\nelse:\n    print(\"x is not greater than 5\")  # Output: x is not greater than 5\n\nx is not greater than 5\n\n\n\n\nExplanation\n\nSince x is 3, the else block executes, printing “x is not greater than 5”."
  },
  {
    "objectID": "slides/python_intro/conditional_statements.html#the-elif-statement",
    "href": "slides/python_intro/conditional_statements.html#the-elif-statement",
    "title": "Conditional Statements in Python",
    "section": "The elif Statement",
    "text": "The elif Statement\n\n\nMultiple Conditions with elif\n\nUse elif to check additional conditions if the previous ones are false.\n\n\nx = 7\nif x &gt; 10:\n    print(\"x is greater than 10\")\nelif x &gt; 5:\n    print(\"x is greater than 5 but less than or equal to 10\")  # Output: x is greater than 5 but less than or equal to 10\nelse:\n    print(\"x is less than or equal to 5\")\n\nx is greater than 5 but less than or equal to 10\n\n\n\n\nExplanation\n\nPython checks each condition sequentially; x &gt; 10 is false, but x &gt; 5 is true."
  },
  {
    "objectID": "slides/python_intro/conditional_statements.html#using-comparison-operators",
    "href": "slides/python_intro/conditional_statements.html#using-comparison-operators",
    "title": "Conditional Statements in Python",
    "section": "Using Comparison Operators",
    "text": "Using Comparison Operators\n\n\nCommon Comparison Operators\n\n==: Equal to\n!=: Not equal to\n&gt;: Greater than\n&lt;: Less than\n&gt;=: Greater than or equal to\n&lt;=: Less than or equal to\n\n\nage = 18\nif age == 18:\n    print(\"You are 18 years old\")  # Output: You are 18 years old\nif age != 20:\n    print(\"You are not 20 years old\")  # Output: You are not 20 years old\n\nYou are 18 years old\nYou are not 20 years old\n\n\n\n\nExplanation\n\nThe == operator checks if age is 18, while the != operator checks if it is not 20."
  },
  {
    "objectID": "slides/python_intro/conditional_statements.html#logical-operators",
    "href": "slides/python_intro/conditional_statements.html#logical-operators",
    "title": "Conditional Statements in Python",
    "section": "Logical Operators",
    "text": "Logical Operators\n\n\nCombining Conditions\n\nUse logical operators to evaluate multiple conditions.\n\n\nx = 7\n\nif x &gt; 5 and x &lt; 10:\n    print(\"x is between 5 and 10\")  # Output: x is between 5 and 10\n\nx is between 5 and 10\n\n\n\n\nOther Logical Operators\n\nor: True if at least one condition is true.\nnot: Inverts the truth value of the condition.\n\n\nif x &lt; 5 or x &gt; 6:\n    print(\"x is either less than 5 or greater than 6\")  # Output: x is either less than 5 or greater than 6\n\nif not x == 5:\n    print(\"x is not equal to 5\")  # Output: x is not equal to 5\n\nx is either less than 5 or greater than 6\nx is not equal to 5"
  },
  {
    "objectID": "slides/python_intro/conditional_statements.html#nested-if-statements",
    "href": "slides/python_intro/conditional_statements.html#nested-if-statements",
    "title": "Conditional Statements in Python",
    "section": "Nested if Statements",
    "text": "Nested if Statements\n\n\nWhat Are Nested if Statements?\n\nPlace one if statement inside another to check multiple conditions that depend on each other.\n\n\nx = 10\ny = 20\n\nif x &gt; 5:\n    if y &gt; 15:\n        print(\"x is greater than 5 and y is greater than 15\")  # Output: x is greater than 5 and y is greater than 15\n\nx is greater than 5 and y is greater than 15\n\n\n\n\nExplanation\n\nFirst check x &gt; 5. If true, then check y &gt; 15."
  },
  {
    "objectID": "slides/python_intro/conditional_statements.html#checking-membership-with-in-operator",
    "href": "slides/python_intro/conditional_statements.html#checking-membership-with-in-operator",
    "title": "Conditional Statements in Python",
    "section": "Checking Membership with in Operator",
    "text": "Checking Membership with in Operator\n\n\nUsing the in Operator\n\nCheck if a value exists in a collection (e.g., list, string, tuple).\n\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\nif \"banana\" in fruits:\n    print(\"Banana is in the list!\")  # Output: Banana is in the list!\n\nBanana is in the list!\n\n\n\n\nExample with Strings\n\nsentence = \"Hello world\"\nif \"world\" in sentence:\n    print(\"The word 'world' is in the sentence.\")  # Output: The word 'world' is in the sentence.\n\nThe word 'world' is in the sentence."
  },
  {
    "objectID": "slides/python_intro/conditional_statements.html#combining-conditions-in-practical-examples",
    "href": "slides/python_intro/conditional_statements.html#combining-conditions-in-practical-examples",
    "title": "Conditional Statements in Python",
    "section": "Combining Conditions in Practical Examples",
    "text": "Combining Conditions in Practical Examples\n\n\nExample: Password Validation\n\nCheck user input against certain conditions for validity.\n\n\npassword = \"Hello123\"\n\nif len(password) &gt;= 8 and any(char.isdigit() for char in password):\n    print(\"Password is valid\")  # Output: Password is valid\nelse:\n    print(\"Password must be at least 8 characters long and contain a number.\")\n\nPassword is valid\n\n\n\n\nExplanation\n\nChecks for length and if any character is a digit using char.isdigit()."
  },
  {
    "objectID": "slides/python_intro/conditional_statements.html#edge-cases-and-handling-errors",
    "href": "slides/python_intro/conditional_statements.html#edge-cases-and-handling-errors",
    "title": "Conditional Statements in Python",
    "section": "Edge Cases and Handling Errors",
    "text": "Edge Cases and Handling Errors\n\n\nImportance of Edge Case Handling\n\nThink about situations where input values might cause errors, such as empty strings or division by zero.\n\n\n# Checking for division by zero\nnumerator = 10\ndenominator = 0\n\nif denominator != 0:\n    result = numerator / denominator\n    print(result)\nelse:\n    print(\"Cannot divide by zero\")  # Output: Cannot divide by zero\n\nCannot divide by zero\n\n\n\n\nExplanation\n\nUse conditionals to prevent errors like division by zero by checking if the denominator is not zero.\n\n\n\n\n\n\nSeminar: LLM, WiSe 2024/25"
  },
  {
    "objectID": "slides/python_intro/classes.html#introduction-to-classes",
    "href": "slides/python_intro/classes.html#introduction-to-classes",
    "title": "Classes in Python",
    "section": "Introduction to Classes",
    "text": "Introduction to Classes\n\n\nWhat Are Classes?\n\nClasses are a core concept in object-oriented programming (OOP).\nThey allow the creation of custom data types that combine attributes (data) and methods (functionality).\n\nKey Benefits\n\nModel real-world entities.\nPromote code reusability.\nEnhance organization and modularity of programs.\n\n\n\nOverview\n\nA class serves as a blueprint for creating objects, encapsulating shared attributes and behaviors."
  },
  {
    "objectID": "slides/python_intro/classes.html#defining-a-class",
    "href": "slides/python_intro/classes.html#defining-a-class",
    "title": "Classes in Python",
    "section": "Defining a Class",
    "text": "Defining a Class\n\n\nBasic Syntax\n\nUse the class keyword followed by the class name (CamelCase convention).\n\n\nclass Text:\n    pass\n\n# Creating an instance of the Text class\nmy_text = Text()\nprint(my_text)  # Output: &lt;__main__.Text object at 0x...&gt;\n\n&lt;__main__.Text object at 0x108bd43e0&gt;\n\n\n\n\nExplanation\n\nDefined a simple class named Text and created an instance stored in my_text."
  },
  {
    "objectID": "slides/python_intro/classes.html#adding-attributes-and-methods",
    "href": "slides/python_intro/classes.html#adding-attributes-and-methods",
    "title": "Classes in Python",
    "section": "Adding Attributes and Methods",
    "text": "Adding Attributes and Methods\n\n\nAttributes and Methods\n\nAttributes are variables belonging to the class.\nMethods are functions defined within the class that manipulate attributes or perform actions.\n\nInitializing Attributes\n\nUse the __init__() method to initialize attributes during object creation.\n\n\n\nExample with Methods\n\nclass Text:\n    def __init__(self, content):\n        self.content = content  # Instance attribute\n    \n    def word_count(self):  # Method\n        return len(self.content.split())\n\n    def shout(self):  # Another method\n        result = self.content.upper()\n        result = result.replace(\".\", \"!\")\n        return result\n\nmy_text = Text(\"Hello, World! This is a test.\")\nprint(my_text.word_count())  # Output: 6\nprint(my_text.shout())  # Output: HELLO, WORLD! THIS IS A TEST!\n\n6\nHELLO, WORLD! THIS IS A TEST!"
  },
  {
    "objectID": "slides/python_intro/classes.html#accessing-attributes-and-methods",
    "href": "slides/python_intro/classes.html#accessing-attributes-and-methods",
    "title": "Classes in Python",
    "section": "Accessing Attributes and Methods",
    "text": "Accessing Attributes and Methods\n\n\nAccessing Attributes\n\nUse dot notation to access attributes and methods of a class instance.\n\n\nprint(my_text.content)  # Output: Hello, World! This is a test.\n\nHello, World! This is a test.\n\n\n\n\nExplanation\n\nAccessed the content attribute of my_text using dot notation."
  },
  {
    "objectID": "slides/python_intro/classes.html#class-vs.-instance-attributes",
    "href": "slides/python_intro/classes.html#class-vs.-instance-attributes",
    "title": "Classes in Python",
    "section": "Class vs. Instance Attributes",
    "text": "Class vs. Instance Attributes\n\n\nClass vs. Instance Attributes\n\nInstance attributes are defined in __init__() and belong to a specific instance.\nClass attributes are shared by all instances and defined directly in the class body.\n\nExample:\n\nclass Text:\n    language = \"English\"  # Class attribute\n\n    def __init__(self, content):\n        self.content = content  # Instance attribute\n\n\n\nAccessing Class Attributes\n\ntext1 = Text(\"Hello, World!\")\ntext2 = Text(\"Bonjour, le monde!\")\n\nprint(text1.language)  # Output: English\nprint(text2.language)  # Output: English\n\nEnglish\nEnglish"
  },
  {
    "objectID": "slides/python_intro/classes.html#changing-class-attributes",
    "href": "slides/python_intro/classes.html#changing-class-attributes",
    "title": "Classes in Python",
    "section": "Changing Class Attributes",
    "text": "Changing Class Attributes\n\n\nChanging Class Attributes\n\nModifying a class attribute affects all instances.\n\n\nText.language = \"French\"\nprint(text1.language)  # Output: French\n\nFrench\n\n\n\n\nExplanation\n\nChanging Text.language affects all instances of the class."
  },
  {
    "objectID": "slides/python_intro/classes.html#inheritance",
    "href": "slides/python_intro/classes.html#inheritance",
    "title": "Classes in Python",
    "section": "Inheritance",
    "text": "Inheritance\n\n\nWhat Is Inheritance?\n\nInheritance allows creating a new class that inherits attributes and methods from an existing class.\nPromotes code reuse and enables class hierarchies.\n\nExample:\n\nclass Text:\n    def __init__(self, content):\n        self.content = content\n\nclass FormattedText(Text):  # Derived class\n    def __init__(self, content, format_type):\n        super().__init__(content)  # Call the parent class's constructor\n\n\n\nExtending Functionality\n\n    def display(self):\n        return f\"[{self.format_type}] {self.content}\"\n\n# Creating instances\nmy_text = Text(\"Hello, World!\")\nformatted_text = FormattedText(\"Hello, World!\", \"Bold\")\n\nprint(my_text.word_count())  # Output: 2\nprint(formatted_text.display())  # Output: [Bold] Hello, World!"
  },
  {
    "objectID": "slides/python_intro/classes.html#key-takeaways",
    "href": "slides/python_intro/classes.html#key-takeaways",
    "title": "Classes in Python",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\n\n\nClasses encapsulate data and functionality.\nAttributes can be instance-specific or shared across instances.\nInheritance enables the creation of new classes based on existing ones, promoting reusability.\n\n\n\n\n\n\n\n\nSeminar: LLM, WiSe 2024/25"
  },
  {
    "objectID": "slides/llm/openai_api.html#lets-get-started",
    "href": "slides/llm/openai_api.html#lets-get-started",
    "title": "The OpenAI API",
    "section": "Let’s get started",
    "text": "Let’s get started\nThe great thing about APIs is that we can start right away without too much preparation!\nIn this sprint, we will use the OpenAI API for completions and embeddings.\nResource: OpenAI API docs"
  },
  {
    "objectID": "slides/llm/openai_api.html#authentication",
    "href": "slides/llm/openai_api.html#authentication",
    "title": "The OpenAI API",
    "section": "Authentication",
    "text": "Authentication\nTypically, it’s as simple as this:\n\n# setting up the client in Python\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\")\n)"
  },
  {
    "objectID": "slides/llm/openai_api.html#authentication-for-the-seminar",
    "href": "slides/llm/openai_api.html#authentication-for-the-seminar",
    "title": "The OpenAI API",
    "section": "Authentication for the seminar",
    "text": "Authentication for the seminar\nFor the sprint, we have hosted some models in Azure.\n\nimport os\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nprint(f\"GPT3: {OpenAIModels.GPT_3.value}\")\nprint(f\"GPT4: {OpenAIModels.GPT_4.value}\")\nprint(f\"Embedding model: {OpenAIModels.EMBED.value}\")\n\nMODEL = OpenAIModels.GPT_4.value\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\nGPT3: gpt3\nGPT4: gpt4\nEmbedding model: embed"
  },
  {
    "objectID": "slides/llm/openai_api.html#creating-a-completion",
    "href": "slides/llm/openai_api.html#creating-a-completion",
    "title": "The OpenAI API",
    "section": "Creating a completion",
    "text": "Creating a completion\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How old is the earth?\",\n        }\n    ],\n    model=MODEL \n)\n\n# check out the type of the response\n\nprint(f\"Response: {type(chat_completion)}\") # a ChatCompletion object\n\nResponse: &lt;class 'openai.types.chat.chat_completion.ChatCompletion'&gt;"
  },
  {
    "objectID": "slides/llm/openai_api.html#retrieving-the-response",
    "href": "slides/llm/openai_api.html#retrieving-the-response",
    "title": "The OpenAI API",
    "section": "Retrieving the response",
    "text": "Retrieving the response\n\n# print the message we want\nprint(f\"\\nResponse message: {chat_completion.choices[0].message.content}\")\n\n# check the tokens used \nprint(f\"\\nTotal tokens used: {chat_completion.usage.total_tokens}\")\n\n\nResponse message: The Earth is approximately 4.54 billion years old.\n\nTotal tokens used: 25\n\n\n\n\n\n\nSeminar: LLM, WiSe 2024/25"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#write-clear-instructions",
    "href": "slides/llm/prompting_functions.html#write-clear-instructions",
    "title": "Prompting & Parameterization",
    "section": "Write clear instructions",
    "text": "Write clear instructions\nThese models can’t read your mind. If outputs are too long, ask for brief replies. If you dislike the format, demonstrate the format you’d like to see.\n\nInclude details in your query to get more relevant answers\nAsk the model to adopt a persona\nUse delimiters to clearly indicate distinct parts of the input\nSpecify the steps required to complete a task\nProvide examples\nSpecify the desired length of the output"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#provide-reference-text",
    "href": "slides/llm/prompting_functions.html#provide-reference-text",
    "title": "Prompting & Parameterization",
    "section": "Provide reference text",
    "text": "Provide reference text\nLanguage models can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to these models can help in answering with fewer fabrications.\n\nInstruct the model to answer using a reference text\nInstruct the model to answer with citations from a reference text"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#split-tasks-into-simpler-subtasks",
    "href": "slides/llm/prompting_functions.html#split-tasks-into-simpler-subtasks",
    "title": "Prompting & Parameterization",
    "section": "Split tasks into simpler subtasks",
    "text": "Split tasks into simpler subtasks\nComplex tasks tend to have higher error rates than simpler tasks. But they can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.\n\nUse intent classification to identify the most relevant instructions for a user query\nFor dialogue applications that require very long conversations, summarize or filter previous dialogue\nSummarize long documents piecewise and construct a full summary recursively"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#give-the-model-time-to-think",
    "href": "slides/llm/prompting_functions.html#give-the-model-time-to-think",
    "title": "Prompting & Parameterization",
    "section": "Give the model time to “think”",
    "text": "Give the model time to “think”\nAsking for a “chain of thought” before an answer can help the model reason its way toward correct answers more reliably.\n\nInstruct the model to work out its own solution before rushing to a conclusion\nUse inner monologue or a sequence of queries to hide the model’s reasoning process\nAsk the model if it missed anything on previous passes"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#use-external-tools",
    "href": "slides/llm/prompting_functions.html#use-external-tools",
    "title": "Prompting & Parameterization",
    "section": "Use external tools",
    "text": "Use external tools\nCompensate for the weaknesses of the model by feeding it the outputs of other tools.\n\nUse embeddings-based search to implement efficient knowledge retrieval\nUse code execution to perform more accurate calculations or call external APIs\nGive the model access to specific functions"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#test-changes-systematically",
    "href": "slides/llm/prompting_functions.html#test-changes-systematically",
    "title": "Prompting & Parameterization",
    "section": "Test changes systematically",
    "text": "Test changes systematically\nImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an “eval”).\n\nEvaluate model outputs with reference to gold-standard answers"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#parameters-1",
    "href": "slides/llm/prompting_functions.html#parameters-1",
    "title": "Prompting & Parameterization",
    "section": "Parameters 1",
    "text": "Parameters 1\nTemperature (temperature):\n\nControls the randomness of the generated text\nLower temperatures = deterministic outputs\nHigher temperatures = more randomness\nBalance between safety and creativity\n\nMax Tokens (max_tokens):\n\nLimits the maximum length of the generated text"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#parameters-2",
    "href": "slides/llm/prompting_functions.html#parameters-2",
    "title": "Prompting & Parameterization",
    "section": "Parameters 2",
    "text": "Parameters 2\nTop P (Nucleus Sampling) (top_p):\n\nDynamically selects a subset of the most likely tokens based on their cumulative probability\nEnsures diversity in the generated text while still prioritizing tokens with higher probabilities\nFor generating diverse and contextually relevant responses\n\nStop Sequence (stop):\n\nSpecifies a sequence of tokens that, if generated by the model, signals it to stop"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#parameters-3",
    "href": "slides/llm/prompting_functions.html#parameters-3",
    "title": "Prompting & Parameterization",
    "section": "Parameters 3",
    "text": "Parameters 3\nFrequency Penalty (frequency_penalty):\n\nPenalizes tokens based on their frequency in the generated text\nDiscourage the model from repeatedly generating common or redundant tokens\nPromote diversity in the generated text\n\nPresence Penalty (presence_penalty):\n\nPenalizes tokens that are already in the input prompt\nDiscourages the model from simply echoing the input text"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#openai-chat-roles",
    "href": "slides/llm/prompting_functions.html#openai-chat-roles",
    "title": "Prompting & Parameterization",
    "section": "OpenAI chat roles",
    "text": "OpenAI chat roles\n\nDefine different roles in the chat\nRoles: system, assistant, user, tools\n\nUser Role\n\nCorresponds to the actual user prompting the model.\nInputs queries or prompts to the model.\n\nAssistant Role\n\nModel responds to user queries or prompts.\nProvides answers and assistance to the user."
  },
  {
    "objectID": "slides/llm/prompting_functions.html#openai-chat-roles-1",
    "href": "slides/llm/prompting_functions.html#openai-chat-roles-1",
    "title": "Prompting & Parameterization",
    "section": "OpenAI chat roles",
    "text": "OpenAI chat roles\nSystem Role\n\nProvides additional instructions to the model.\nNot a user input.\nExample: Setting response style.\n\nTools Role\n\nUsed for debugging or monitoring purposes.\nProvides insights into model behavior or performance."
  },
  {
    "objectID": "slides/llm/prompting_functions.html#code",
    "href": "slides/llm/prompting_functions.html#code",
    "title": "Prompting & Parameterization",
    "section": "Code",
    "text": "Code\n\nimport os\nfrom llm_utils.client import get_openai_client\n\nMODEL = \"gpt4\"\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\ncompletion = client.chat.completions.create(\n  model=\"MODEL\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are an annoyed technician working in a help center for dish washers, who answers in short, unfriendly bursts.\"},\n    {\"role\": \"user\", \"content\": \"My dish washer does not clean the dishes, what could be the reason.\"}\n  ]\n)\n\nprint(completion.choices[0].message.content)\n\n\n\nDirty filter. Broken spray arm. Not enough water. Check all."
  },
  {
    "objectID": "slides/llm/prompting_functions.html#get-more-consistent-output-of-language-models",
    "href": "slides/llm/prompting_functions.html#get-more-consistent-output-of-language-models",
    "title": "Prompting & Parameterization",
    "section": "Get more consistent output of language models",
    "text": "Get more consistent output of language models\n\nSo far: language model “freely” answering\nNot always a practical format if we want to use a language model for very specific purposes\nBusiness applications often require consistent output"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#example-sentiment-analysis",
    "href": "slides/llm/prompting_functions.html#example-sentiment-analysis",
    "title": "Prompting & Parameterization",
    "section": "Example: Sentiment analysis",
    "text": "Example: Sentiment analysis\n\nsentiment_categories = [\"positive\", \"negative\", \"neutral\", \"mixed\"]\n\n \n\nmessages = []\nmessages.append(\n    {\"role\": \"system\", \"content\": f\"Classify the given text into one of the following sentiment categories: {sentiment_categories}.\"}\n)\nmessages.append(\n    {\"role\": \"user\", \"content\": \"I really did not like the movie.\"}\n)\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=MODEL\n)\n\nprint(f\"Response: '{response.choices[0].message.content}'\")\n\n\n\nResponse: 'Category: Negative'"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#function-calling-1",
    "href": "slides/llm/prompting_functions.html#function-calling-1",
    "title": "Prompting & Parameterization",
    "section": "Function calling",
    "text": "Function calling\n\nOpenAI allows for “function calling” or “tool calling”\nThis allows us to specify the output format of GPT\nFunction calling (cookbook)\nFunction calling"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#example-continued",
    "href": "slides/llm/prompting_functions.html#example-continued",
    "title": "Prompting & Parameterization",
    "section": "Example: continued",
    "text": "Example: continued\n\n# this looks intimidating but isn't that complicated\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_sentiment\",\n            \"description\": \"Analyze the sentiment in a given text.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"sentiment\": {\n                        \"type\": \"string\",\n                        \"enum\": sentiment_categories,\n                        \"description\": f\"The sentiment of the text.\"\n                    }\n                },\n                \"required\": [\"sentiment\"],\n            }\n        }\n    }\n]"
  },
  {
    "objectID": "slides/llm/prompting_functions.html#including-multiple-parameters",
    "href": "slides/llm/prompting_functions.html#including-multiple-parameters",
    "title": "Prompting & Parameterization",
    "section": "Including multiple parameters",
    "text": "Including multiple parameters\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_sentiment\",\n            \"description\": \"Analyze the sentiment in a given text.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"sentiment\": {\n                        \"type\": \"string\",\n                        \"enum\": sentiment_categories,\n                        \"description\": f\"The sentiment of the text.\"\n                    },\n                    \"reason\": {\n                        \"type\": \"string\",\n                        \"description\": \"The reason for the sentiment in few words. If there is no information, do not make assumptions and leave blank.\"\n                    }\n                },\n                \"required\": [\"sentiment\", \"reason\"],\n            }\n        }\n    }\n]"
  },
  {
    "objectID": "about/projects.html",
    "href": "about/projects.html",
    "title": "Projects",
    "section": "",
    "text": "In the final part of the seminar we are going to tackle our very own projects involving a language model. At best, you find your ideas and work on them, maybe you even have a work-related application mind. The following list can serve as inspiration.\n\nProject ideas\n\nQuestion-Answering Chatbot: Build a chatbot that can answer questions posed by users on a specific topic provided in form of documents. Users input their questions, the chatbot retrieves relevant information from a pre-defined set of documents, and uses the information to answer the question.\nDocument tagging / classification: Use GPT and its tools (e.g., function calls) and/or embeddings to classify documents or assign tags to them. Example: Sort bug reports or complaints into categories depending on the problem.\nClustering of text-based entities: Create a small tool that can cluster text-based entities based on embeddings, for example, groups of texts or keywords. Example: Structure a folder of text files based on their content.\nText-based RPG Game: Develop a text-based role-playing game where players interact with characters and navigate through a story generated by GPT. Players make choices that influence the direction of the narrative.\nSentiment Analysis Tool: Build an app that analyzes the sentiment of text inputs (e.g., social media posts, customer reviews) using GPT. Users can input text, and the app provides insights into the overall sentiment expressed in the text.\nText Summarization Tool: Create an application that summarizes long blocks of text into shorter, concise summaries. Users can input articles, essays, or documents, and the tool generates a summarized version.\nLanguage Translation Tool: Build a simple translation app that utilizes GPT to translate text between different languages. Users can input text in one language, and the app outputs the translated text in the desired language. Has to include some nice tweaks.\nPersonalized Recipe Generator: Develop an app that generates personalized recipes based on user preferences and dietary restrictions. Users input their preferred ingredients and dietary needs, and the app generates custom recipes using GPT.\nLyrics Generator: Create a lyrics generation tool that generates lyrics based on user input such as themes, music style, emotions, or keywords. Users can explore different poetic styles and themes generated by GPT.\n\n\n\nProject setup\nSee slides.\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "About",
      "Projects"
    ]
  },
  {
    "objectID": "resources/exercises.html",
    "href": "resources/exercises.html",
    "title": "List of exercises",
    "section": "",
    "text": "Introduction to Python\nExercise: Data types\nExercise: String manipulations\nExercise: Lists and loops\nExercise: Conditional statements\nExercise: Functions\nExercise: Dictionaries\nExercise: Classes\n\n\nNatural Language Processing\nExercise: Sentence tokenization\nExercise: TF-IDF\nExercise: Word matching\nExercise: Fuzzy matching\n\n\nLarge Language Models with OpenAI\nExercise: OpenAI - Getting started\nExercise: GPT Chatbot\nExercise: GPT Parameterization\nExercise: NER with tool calling\n\n\nEmbeddings\nExercise: Embedding similarity\n\n\n\n\n Back to top",
    "crumbs": [
      "Resources",
      "List of exercises"
    ]
  },
  {
    "objectID": "resources/slides.html",
    "href": "resources/slides.html",
    "title": "Slides",
    "section": "",
    "text": "About\n\nAbout the Seminar\nProjects\n\n\n\nPython Crash Course\n\nVariables and Data Types\nString Operations\nLists and Loops\nFunctions\nConditional Statements\nDictionaries\nClasses\n\n\n\nNatural Language Processing\n\nA Short History of NLP\nStatistics\nTokenization\n\n\n\nLarge Language Models\n\nIntroduction\nOpenAI API\nPrompting and Tool Calling\n\n\n\nEmbeddings\n\nEmbeddings\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Resources",
      "Slides"
    ]
  },
  {
    "objectID": "python_intro/exercises/lists_and_loops.html",
    "href": "python_intro/exercises/lists_and_loops.html",
    "title": "Exercise: Lists and Loops",
    "section": "",
    "text": "Task: Create a list of 5 of your favorite movies and write a loop to print each movie.\nInstructions:\n\nCreate a list named movies with 5 movie titles.\nWrite a for loop that iterates over the list and prints each movie.\n\n\n\nShow solution\n\n\nmovies = [\"Inception\", \"The Matrix\", \"Interstellar\", \"The Shawshank Redemption\", \"The Godfather\"]\nfor movie in movies:\n    print(movie)\n\nInception\nThe Matrix\nInterstellar\nThe Shawshank Redemption\nThe Godfather",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Lists and Loops"
    ]
  },
  {
    "objectID": "python_intro/exercises/lists_and_loops.html#exercise-1-printing-elements-of-a-list",
    "href": "python_intro/exercises/lists_and_loops.html#exercise-1-printing-elements-of-a-list",
    "title": "Exercise: Lists and Loops",
    "section": "",
    "text": "Task: Create a list of 5 of your favorite movies and write a loop to print each movie.\nInstructions:\n\nCreate a list named movies with 5 movie titles.\nWrite a for loop that iterates over the list and prints each movie.\n\n\n\nShow solution\n\n\nmovies = [\"Inception\", \"The Matrix\", \"Interstellar\", \"The Shawshank Redemption\", \"The Godfather\"]\nfor movie in movies:\n    print(movie)\n\nInception\nThe Matrix\nInterstellar\nThe Shawshank Redemption\nThe Godfather",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Lists and Loops"
    ]
  },
  {
    "objectID": "python_intro/exercises/lists_and_loops.html#exercise-2-summing-numbers-in-a-list",
    "href": "python_intro/exercises/lists_and_loops.html#exercise-2-summing-numbers-in-a-list",
    "title": "Exercise: Lists and Loops",
    "section": "Exercise 2: Summing Numbers in a List",
    "text": "Exercise 2: Summing Numbers in a List\nTask: Create a list of numbers and write a loop to calculate and print the sum of all numbers in the list.\nInstructions:\n\nCreate a list named numbers with at least 5 integer values.\nUse a loop to iterate over the list and calculate the sum.\nPrint the sum after the loop completes.\n\n\n\nShow solution\n\n\nnumbers = [10, 20, 30, 40, 50]\ntotal_sum = 0\nfor num in numbers:\n    total_sum += num # Or: total_sum = total_sum + num\n\nprint(\"The total sum is:\", total_sum)  # Output: The total sum is: 150\n\nThe total sum is: 150",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Lists and Loops"
    ]
  },
  {
    "objectID": "python_intro/exercises/lists_and_loops.html#exercise-3-counting-number-of-characters-in-a-list-of-words",
    "href": "python_intro/exercises/lists_and_loops.html#exercise-3-counting-number-of-characters-in-a-list-of-words",
    "title": "Exercise: Lists and Loops",
    "section": "Exercise 3: Counting Number of Characters in a List of Words",
    "text": "Exercise 3: Counting Number of Characters in a List of Words\nTask: Create a list of words and write a loop to count how many characters (i.e., letters) the list contains in total. Print the count.\nInstructions:\n\nCreate a list named words with several words.\nWrite a loop to iterate through the list and count the letters of each word using the function len(). Sum the count.\nPrint the count.\n\n\n\nShow solution\n\n\nwords = [\"Python\", \"code\", \"Python\", \"learning\", \"tutorial\", \"Python\"]\ncount = 0\nfor word in words:\n    count += len(word)\n\nprint(f\"The list contains a total of {count} characters.\")\n\nThe list contains a total of 38 characters.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Lists and Loops"
    ]
  },
  {
    "objectID": "python_intro/exercises/lists_and_loops.html#exercise-4-doubling-elements-of-a-list",
    "href": "python_intro/exercises/lists_and_loops.html#exercise-4-doubling-elements-of-a-list",
    "title": "Exercise: Lists and Loops",
    "section": "Exercise 4: Doubling Elements of a List",
    "text": "Exercise 4: Doubling Elements of a List\nTask: Create a loop that doubles each number in a list and prints the new list.\nInstructions:\n\nCreate a list named numbers with some integer values.\nUse a loop to create a new list doubled_numbers where each element is double the corresponding element in numbers.\nPrint the new list.\n\n\n\nShow solution\n\n\nnumbers = [1, 3, 5, 7, 9]\ndoubled_numbers = []\nfor num in numbers:\n    doubled_numbers.append(num * 2)\n\nprint(doubled_numbers)  # Output: [2, 6, 10, 14, 18]\n\n[2, 6, 10, 14, 18]",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Lists and Loops"
    ]
  },
  {
    "objectID": "python_intro/exercises/lists_and_loops.html#exercise-5-count-vowels-in-a-list-of-words",
    "href": "python_intro/exercises/lists_and_loops.html#exercise-5-count-vowels-in-a-list-of-words",
    "title": "Exercise: Lists and Loops",
    "section": "Exercise 5: Count Vowels in a List of Words",
    "text": "Exercise 5: Count Vowels in a List of Words\nTask: Write a function that counts the total number of vowels in a list of words.\nInstructions:\n\nDefine a function called count_vowels(word_list) that takes a list of strings word_list.\nInside the function, loop through each word in the list, count the vowels (a, e, i, o, u) in each word, and keep a running total.\nReturn the total count of vowels.\nCall the function with a sample list (e.g., [\"NLP\", \"is\", \"fun\"]) and print the result.\n\n\n\nShow solution\n\n\ndef count_vowels(word_list):\n    vowels = 'aeiouAEIOU'\n    total_vowels = 0\n\n    for word in word_list:\n        for char in word:\n            if char in vowels:\n                total_vowels += 1\n                \n    return total_vowels\n\n# Sample usage\nwords = [\"NLP\", \"is\", \"fun\"]\nresult = count_vowels(words)\nprint(\"Total number of vowels:\", result)  \n\nTotal number of vowels: 2",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Lists and Loops"
    ]
  },
  {
    "objectID": "python_intro/exercises/lists_and_loops.html#exercise-6-filter-long-words",
    "href": "python_intro/exercises/lists_and_loops.html#exercise-6-filter-long-words",
    "title": "Exercise: Lists and Loops",
    "section": "Exercise 6: Filter Long Words",
    "text": "Exercise 6: Filter Long Words\nTask: Create a function that filters out words longer than a specified length from a list.\nInstructions:\n\nDefine a function called filter_long_words(word_list, length) that takes a list of strings word_list and an integer length.\nInside the function, use a loop to create a new list that contains only the words from the original list that have a length less than or equal to length.\nReturn the new list.\nCall the function with a sample list and a length (e.g., [\"Natural\", \"Language\", \"Processing\", \"is\", \"fun\"], length=4) and print the result.\n\n\n\nShow solution\n\n\ndef filter_long_words(word_list, length):\n    short_words = []\n    \n    for word in word_list:\n        if len(word) &lt;= length:\n            short_words.append(word)\n    \n    return short_words\n\n# Sample usage\nwords = [\"Natural\", \"Language\", \"Processing\", \"is\", \"fun\"]\nresult = filter_long_words(words, 4)\nprint(\"Filtered words:\", result)  \n\nFiltered words: ['is', 'fun']",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Lists and Loops"
    ]
  },
  {
    "objectID": "python_intro/exercises/lists_and_loops.html#exercise-7-create-a-word-frequency-dictionary",
    "href": "python_intro/exercises/lists_and_loops.html#exercise-7-create-a-word-frequency-dictionary",
    "title": "Exercise: Lists and Loops",
    "section": "Exercise 7: Create a Word Frequency Dictionary",
    "text": "Exercise 7: Create a Word Frequency Dictionary\nTask: Write a function that creates a frequency dictionary from a list of words.\nInstructions:\n\nDefine a function called word_frequency(word_list) that takes a list of strings word_list.\nInside the function, use a loop to create a dictionary where the keys are the words and the values are the number of times each word appears in the list.\nReturn the frequency dictionary.\nCall the function with a sample list (e.g., [\"NLP\", \"is\", \"fun\", \"NLP\"]) and print the result.\n\n\n\nShow solution\n\n\ndef word_frequency(word_list):\n    frequency_dict = {}\n\n    for word in word_list:\n        if word in frequency_dict:\n            frequency_dict[word] += 1\n        else:\n            frequency_dict[word] = 1\n            \n    return frequency_dict\n\n# Sample usage\nwords = [\"NLP\", \"is\", \"fun\", \"NLP\"]\nresult = word_frequency(words)\nprint(\"Word frequency:\", result)  \n\nWord frequency: {'NLP': 2, 'is': 1, 'fun': 1}",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Lists and Loops"
    ]
  },
  {
    "objectID": "python_intro/exercises/lists_and_loops.html#exercise-8-reverse-each-word-in-a-list",
    "href": "python_intro/exercises/lists_and_loops.html#exercise-8-reverse-each-word-in-a-list",
    "title": "Exercise: Lists and Loops",
    "section": "Exercise 8: Reverse Each Word in a List",
    "text": "Exercise 8: Reverse Each Word in a List\nTask: Create a function that reverses each word in a list.\nInstructions:\n\nDefine a function called reverse_words(word_list) that takes a list of strings word_list.\nInside the function, use a loop to create a new list that contains each word reversed.\nReturn the new list.\nCall the function with a sample list (e.g., [\"NLP\", \"is\", \"fun\"]) and print the result.\n\n\n\nShow solution\n\n\ndef reverse_words(word_list):\n    reversed_list = []\n    \n    for word in word_list:\n        reversed_list.append(word[::-1])\n    \n    return reversed_list\n\n# Sample usage\nwords = [\"NLP\", \"is\", \"fun\"]\nresult = reverse_words(words)\nprint(\"Reversed words:\", result) \n\nReversed words: ['PLN', 'si', 'nuf']",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Lists and Loops"
    ]
  },
  {
    "objectID": "python_intro/exercises/dictionaries.html",
    "href": "python_intro/exercises/dictionaries.html",
    "title": "Exercise: Dictionaries",
    "section": "",
    "text": "Task: Write a function that counts the frequency of each word in a given list.\nInstructions:\n\nDefine a function called word_count(word_list) that takes a list of strings word_list.\nInside the function, create a dictionary to store the word counts.\nLoop through the list and update the dictionary with the frequency of each word.\nReturn the dictionary.\nCall the function with a sample list (e.g., [\"NLP\", \"is\", \"fun\", \"NLP\"]) and print the result.\n\n\n\nShow solution\n\n\ndef word_count(word_list):\n    count_dict = {}\n    \n    for word in word_list:\n        count_dict[word] = count_dict.get(word, 0) + 1\n    \n    return count_dict\n\n# Sample usage\nwords = [\"NLP\", \"is\", \"fun\", \"NLP\"]\nresult = word_count(words)\nprint(\"Word count dictionary:\", result)  \n\nWord count dictionary: {'NLP': 2, 'is': 1, 'fun': 1}",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Dictionaries"
    ]
  },
  {
    "objectID": "python_intro/exercises/dictionaries.html#exercise-1-create-a-word-count-dictionary",
    "href": "python_intro/exercises/dictionaries.html#exercise-1-create-a-word-count-dictionary",
    "title": "Exercise: Dictionaries",
    "section": "",
    "text": "Task: Write a function that counts the frequency of each word in a given list.\nInstructions:\n\nDefine a function called word_count(word_list) that takes a list of strings word_list.\nInside the function, create a dictionary to store the word counts.\nLoop through the list and update the dictionary with the frequency of each word.\nReturn the dictionary.\nCall the function with a sample list (e.g., [\"NLP\", \"is\", \"fun\", \"NLP\"]) and print the result.\n\n\n\nShow solution\n\n\ndef word_count(word_list):\n    count_dict = {}\n    \n    for word in word_list:\n        count_dict[word] = count_dict.get(word, 0) + 1\n    \n    return count_dict\n\n# Sample usage\nwords = [\"NLP\", \"is\", \"fun\", \"NLP\"]\nresult = word_count(words)\nprint(\"Word count dictionary:\", result)  \n\nWord count dictionary: {'NLP': 2, 'is': 1, 'fun': 1}",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Dictionaries"
    ]
  },
  {
    "objectID": "python_intro/exercises/dictionaries.html#exercise-manage-a-simple-chat-log",
    "href": "python_intro/exercises/dictionaries.html#exercise-manage-a-simple-chat-log",
    "title": "Exercise: Dictionaries",
    "section": "Exercise: Manage a Simple Chat Log",
    "text": "Exercise: Manage a Simple Chat Log\nTask: Create a simple chat log data structure using a dictionary to store messages between a user and a chatbot.\nInstructions:\n\nDefine a function called create_chat_log() that initializes an empty chat log dictionary.\nDefine another function called add_message(chat_log, sender, message) that takes the chat log, a sender (“user” or “chatbot”), and the message as parameters.\n\nThe function should append the new message to a list under the appropriate sender in the chat log.\n\nDefine a function called get_chat_log(chat_log) that returns the entire chat log.\nCall the functions to create a chat log, add messages from both the user and the chatbot, and then print the entire chat log.\n\n\nExample structure of the chat log:\n{\n    \"user\": [\"Hello!\", \"What is NLP?\"],\n    \"chatbot\": [\"Hi there!\", \"NLP stands for Natural Language Processing.\"]\n}\n\n&lt;details&gt;\n&lt;summary&gt;Show solution&lt;/summary&gt;\n\n::: {#cell-7 .cell execution_count=2}\n``` {.python .cell-code}\ndef create_chat_log():\n    return {\"user\": [], \"chatbot\": []}\n\ndef add_message(chat_log, sender, message):\n    if sender in chat_log:\n        chat_log[sender].append(message)\n    else:\n        print(\"Sender not recognized. Use 'user' or 'chatbot'.\")\n\ndef get_chat_log(chat_log):\n    return chat_log\n\n# Example usage\nchat_log = create_chat_log()\nadd_message(chat_log, \"user\", \"Hello!\")\nadd_message(chat_log, \"chatbot\", \"Hi there!\")\nadd_message(chat_log, \"user\", \"What is NLP?\")\nadd_message(chat_log, \"chatbot\", \"NLP stands for Natural Language Processing.\")\n\n# Print the chat log\nprint(\"Chat Log:\", get_chat_log(chat_log))\n\nChat Log: {'user': ['Hello!', 'What is NLP?'], 'chatbot': ['Hi there!', 'NLP stands for Natural Language Processing.']}\n\n:::",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Dictionaries"
    ]
  },
  {
    "objectID": "python_intro/exercises/classes.html",
    "href": "python_intro/exercises/classes.html",
    "title": "Exercise: Classes",
    "section": "",
    "text": "Task: Write a class that represents a simple text document.\nInstructions:\n\nDefine a class called TextDocument with the following attributes:\n\ntitle (string)\ncontent (string)\n\nInclude a method called word_count() that returns the number of words in the content.\nCreate an instance of the TextDocument class, set the title and content, and print the word count.\n\n\n\nShow solution\n\n\nclass TextDocument:\n    def __init__(self, title, content):\n        self.title = title\n        self.content = content\n    \n    def word_count(self):\n        return len(self.content.split())\n\n# Sample usage\ndoc = TextDocument(\"My First Document\", \"This is a simple text document for NLP.\")\nprint(f\"Word count: {doc.word_count()}\") \n\nWord count: 8",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Classes"
    ]
  },
  {
    "objectID": "python_intro/exercises/classes.html#exercise-1-create-a-simple-text-class",
    "href": "python_intro/exercises/classes.html#exercise-1-create-a-simple-text-class",
    "title": "Exercise: Classes",
    "section": "",
    "text": "Task: Write a class that represents a simple text document.\nInstructions:\n\nDefine a class called TextDocument with the following attributes:\n\ntitle (string)\ncontent (string)\n\nInclude a method called word_count() that returns the number of words in the content.\nCreate an instance of the TextDocument class, set the title and content, and print the word count.\n\n\n\nShow solution\n\n\nclass TextDocument:\n    def __init__(self, title, content):\n        self.title = title\n        self.content = content\n    \n    def word_count(self):\n        return len(self.content.split())\n\n# Sample usage\ndoc = TextDocument(\"My First Document\", \"This is a simple text document for NLP.\")\nprint(f\"Word count: {doc.word_count()}\") \n\nWord count: 8",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Classes"
    ]
  },
  {
    "objectID": "python_intro/exercises/classes.html#exercise-2-extend-the-text-class-for-analysis",
    "href": "python_intro/exercises/classes.html#exercise-2-extend-the-text-class-for-analysis",
    "title": "Exercise: Classes",
    "section": "Exercise 2: Extend the Text Class for Analysis",
    "text": "Exercise 2: Extend the Text Class for Analysis\nTask: Extend the TextDocument class to include a method that counts the frequency of a specific word.\nInstructions:\n\nAdd a method called count_word_frequency(self, word) to the TextDocument class that counts how many times a specific word appears in the content.\nCreate an instance of the class and call this method with a sample word.\n\n\n\nShow solution\n\n\nclass TextDocument:\n    def __init__(self, title, content):\n        self.title = title\n        self.content = content\n    \n    def word_count(self):\n        return len(self.content.split())\n    \n    def count_word_frequency(self, word):\n        return self.content.lower().split().count(word.lower())\n\n# Sample usage\ndoc = TextDocument(\"My First Document\", \"This is a simple text document for NLP. NLP is fun.\")\nprint(f\"Word count: {doc.word_count()}\") \nprint(f\"Frequency of 'NLP': {doc.count_word_frequency('NLP')}\")  \n\nWord count: 11\nFrequency of 'NLP': 1",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Classes"
    ]
  },
  {
    "objectID": "python_intro/exercises/classes.html#exercise-3-create-a-chat-class",
    "href": "python_intro/exercises/classes.html#exercise-3-create-a-chat-class",
    "title": "Exercise: Classes",
    "section": "Exercise 3: Create a Chat Class",
    "text": "Exercise 3: Create a Chat Class\nTask: Create a class that represents a chat session.\nInstructions:\n\nDefine a class called ChatSession with the following attributes:\n\nmessages (list to store messages)\n\nInclude methods to:\n\nadd_message(sender, message) that appends a message to the messages list.\nget_chat_log() that returns the chat log as a list of strings formatted as “sender: message”.\n\nCreate an instance of the ChatSession class and demonstrate adding messages and retrieving the chat log.\n\n\n\nShow solution\n\n\nclass ChatSession:\n    def __init__(self):\n        self.messages = []\n    \n    def add_message(self, sender, message):\n        self.messages.append(f\"{sender}: {message}\")\n    \n    def get_chat_log(self):\n        return self.messages\n\n# Sample usage\nchat = ChatSession()\nchat.add_message(\"User\", \"Hello!\")\nchat.add_message(\"Chatbot\", \"Hi there! How can I assist you today?\")\nchat.add_message(\"User\", \"What is NLP?\")\nlog = chat.get_chat_log()\nprint(\"Chat Log:\")\nfor message in log:\n    print(message)\n\nChat Log:\nUser: Hello!\nChatbot: Hi there! How can I assist you today?\nUser: What is NLP?",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Classes"
    ]
  },
  {
    "objectID": "python_intro/exercises/conditional_statements.html",
    "href": "python_intro/exercises/conditional_statements.html",
    "title": "Exercise: Conditional Statements",
    "section": "",
    "text": "Task: Write a code snippet that checks if a specific word is present in a given text.\nInstructions:\n\nUse the in operator to check if the word “language” is in the text.\nPrint a message indicating whether the word was found or not.\n\n\n\nShow solution\n\n\ntext = \"Natural language processing is fascinating.\"\nif \"language\" in text:\n    print(\"The word 'language' is present in the text.\")\nelse:\n    print(\"The word 'language' is not present in the text.\")\n\nThe word 'language' is present in the text.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Conditional Statements"
    ]
  },
  {
    "objectID": "python_intro/exercises/conditional_statements.html#exercise-1-check-word-presence",
    "href": "python_intro/exercises/conditional_statements.html#exercise-1-check-word-presence",
    "title": "Exercise: Conditional Statements",
    "section": "",
    "text": "Task: Write a code snippet that checks if a specific word is present in a given text.\nInstructions:\n\nUse the in operator to check if the word “language” is in the text.\nPrint a message indicating whether the word was found or not.\n\n\n\nShow solution\n\n\ntext = \"Natural language processing is fascinating.\"\nif \"language\" in text:\n    print(\"The word 'language' is present in the text.\")\nelse:\n    print(\"The word 'language' is not present in the text.\")\n\nThe word 'language' is present in the text.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Conditional Statements"
    ]
  },
  {
    "objectID": "python_intro/exercises/conditional_statements.html#exercise-2-classify-text-length",
    "href": "python_intro/exercises/conditional_statements.html#exercise-2-classify-text-length",
    "title": "Exercise: Conditional Statements",
    "section": "Exercise 2: Classify Text Length",
    "text": "Exercise 2: Classify Text Length\nTask: Write a code snippet that classifies the length of the text as “short”, “medium”, or “long”.\nInstructions:\n\nUse conditional statements to classify the text based on its length:\n\nShort: less than 20 characters\nMedium: between 20 and 100 characters\nLong: more than 100 characters\n\nPrint a message indicating the classification.\n\n\n\nShow solution\n\n\ntext = \"Natural language processing.\"\nlength = len(text)\n\nif length &lt; 20:\n    print(\"The text is short.\")\nelif 20 &lt;= length &lt;= 100:\n    print(\"The text is medium.\")\nelse:\n    print(\"The text is long.\")\n\nThe text is medium.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Conditional Statements"
    ]
  },
  {
    "objectID": "python_intro/exercises/conditional_statements.html#exercise-3-check-for-uppercase",
    "href": "python_intro/exercises/conditional_statements.html#exercise-3-check-for-uppercase",
    "title": "Exercise: Conditional Statements",
    "section": "Exercise 3: Check for Uppercase",
    "text": "Exercise 3: Check for Uppercase\nTask: Write a code snippet that checks if the first character of a sentence is uppercase.\nInstructions:\n\nUse an if statement to check if the first character of the sentence is uppercase.\nPrint a message indicating whether it is uppercase or not.\n\n\n\nShow solution\n\n\nsentence = \"Hello, world!\"\nif sentence[0].isupper():\n    print(\"The sentence starts with an uppercase letter.\")\nelse:\n    print(\"The sentence does not start with an uppercase letter.\")\n\nThe sentence starts with an uppercase letter.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Conditional Statements"
    ]
  },
  {
    "objectID": "python_intro/exercises/conditional_statements.html#exercise-4-check-palindrome",
    "href": "python_intro/exercises/conditional_statements.html#exercise-4-check-palindrome",
    "title": "Exercise: Conditional Statements",
    "section": "Exercise 4: Check Palindrome",
    "text": "Exercise 4: Check Palindrome\nTask: Write a function that checks if a given word is a palindrome.\nInstructions:\n\nDefine a function called is_palindrome(word) that takes a string parameter word.\nInside the function, use conditional statements to check if the word is the same when reversed.\nReturn True if it is a palindrome and False otherwise.\nCall the function with a sample word (e.g., “radar”) and print the result.\n\n\n\nShow solution\n\n\ndef is_palindrome(word):\n    return word == word[::-1]\n\n# Sample usage\nresult = is_palindrome(\"radar\")\nprint(\"Is 'radar' a palindrome?\", result)  \n\nIs 'radar' a palindrome? True",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Conditional Statements"
    ]
  },
  {
    "objectID": "python_intro/exercises/conditional_statements.html#exercise-5-categorize-word-length",
    "href": "python_intro/exercises/conditional_statements.html#exercise-5-categorize-word-length",
    "title": "Exercise: Conditional Statements",
    "section": "Exercise 5: Categorize Word Length",
    "text": "Exercise 5: Categorize Word Length\nTask: Create a function that categorizes a word as short, medium, or long based on its length.\nInstructions:\n\nDefine a function called categorize_word_length(word) that takes a string parameter word.\nInside the function, use conditional statements to categorize the word based on the following criteria:\n\nShort: 1-3 letters\nMedium: 4-6 letters\nLong: More than 6 letters\n\nReturn the category as a string.\nCall the function with a sample word (e.g., “NLP”) and print the result.\n\n\n\nShow solution\n\n\ndef categorize_word_length(word):\n    length = len(word)\n    \n    if length &lt;= 3:\n        return \"Short\"\n    elif 4 &lt;= length &lt;= 6:\n        return \"Medium\"\n    else:\n        return \"Long\"\n\n# Sample usage\nresult = categorize_word_length(\"NLP\")",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Conditional Statements"
    ]
  },
  {
    "objectID": "python_intro/exercises/conditional_statements.html#exercise-6-grade-classification",
    "href": "python_intro/exercises/conditional_statements.html#exercise-6-grade-classification",
    "title": "Exercise: Conditional Statements",
    "section": "Exercise 6: Grade Classification",
    "text": "Exercise 6: Grade Classification\nTask: Create a function that classifies a score into letter grades.\nInstructions:\n\nDefine a function called grade_classification(score) that takes an integer parameter score.\nInside the function, use conditional statements to classify the score:\n\nA: 90-100\nB: 80-89\nC: 70-79\nD: 60-69\nF: Below 60\n\nReturn the letter grade.\nCall the function with a sample score (e.g., 85) and print the result.\n\n\n\nShow solution\n\n\ndef grade_classification(score):\n    if 90 &lt;= score &lt;= 100:\n        return \"A\"\n    elif 80 &lt;= score &lt; 90:\n        return \"B\"\n    elif 70 &lt;= score &lt; 80:\n        return \"C\"\n    elif 60 &lt;= score &lt; 70:\n        return \"D\"\n    else:\n        return \"F\"\n\n# Sample usage\nresult = grade_classification(85)\nprint(\"The letter grade is:\", result)  \n\nThe letter grade is: B",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Conditional Statements"
    ]
  },
  {
    "objectID": "python_intro/data_types.html",
    "href": "python_intro/data_types.html",
    "title": "Variables and data types",
    "section": "",
    "text": "Before diving into complex tasks like natural language processing (NLP) or interacting with APIs, we need to understand the basics of how Python handles data. At the core of any programming language are variables and data types. Variables allow us to store data, and data types define the kind of data we are working with (like numbers, text, or booleans). In Python, we do not need to explicitly declare the type of a variable – Python figures it out based on what we assign to it.\nLet’s take a look at the most important data types in Python, and how we can use them.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Variables and data types"
    ]
  },
  {
    "objectID": "python_intro/data_types.html#variables",
    "href": "python_intro/data_types.html#variables",
    "title": "Variables and data types",
    "section": "Variables",
    "text": "Variables\nIn Python, variables are used to store information that can be referenced or manipulated later. You create a variable by simply assigning a value to it with the = operator. Once defined, you can use the variable by referring to its name.\n\nExample:\n\n# Assigning values to variables\nmy_number = 10  # an integer\nmy_float = 3.14  # a floating-point number\nmy_text = \"Hello, GPT!\"  # a string of text\nis_valid = True  # a boolean (True/False)\n\n# Printing the variables\nprint(my_number)   # Output: 10\nprint(my_float)    # Output: 3.14\nprint(my_text)     # Output: Hello, GPT!\nprint(is_valid)    # Output: True\n\n10\n3.14\nHello, GPT!\nTrue\n\n\nIn this example:\n\nmy_number is storing an integer value (whole number).\nmy_float is storing a floating-point number (a number with decimal points).\nmy_text stores a string, which is simply a sequence of characters.\nis_valid is a boolean, which can either be True or False.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Variables and data types"
    ]
  },
  {
    "objectID": "python_intro/data_types.html#data-types",
    "href": "python_intro/data_types.html#data-types",
    "title": "Variables and data types",
    "section": "Data Types",
    "text": "Data Types\nPython recognizes several different data types, but let’s focus on the ones you’ll use the most often for basic NLP tasks:\n\nIntegers (int): Whole numbers, like -1, 0, and 100.\nFloats (float): Numbers with decimal points, like 3.14 or -0.001.\nStrings (str): Sequences of characters, like \"OpenAI\" or \"Hello World\".\nBooleans (bool): True or False values, which are often used in conditional statements.\n\n\nWorking with Numbers (Integers and Floats)\nNumbers are straightforward to work with in Python. You can perform basic arithmetic like addition, subtraction, multiplication, and division. The type of number (integer or float) depends on whether it has decimal points.\n\nExample:\n\n# Basic arithmetic operations\na = 10\nb = 4\n\nsum_result = a + b  # Addition\ndifference = a - b  # Subtraction\nproduct = a * b  # Multiplication\nquotient = a / b  # Division (always returns a float)\n\nprint(sum_result)    # Output: 14\nprint(difference)    # Output: 6\nprint(product)       # Output: 40\nprint(quotient)      # Output: 2.5\n\n14\n6\n40\n2.5\n\n\nIn this example, a and b are integers, and we perform basic arithmetic with them. Notice that even though a and b are integers, the result of division is a float (2.5).\n\n\n\nWorking with Strings\nStrings are used to represent text in Python. Strings can be created using single (') or double (\") quotes. Python provides a wide range of operations you can perform on strings, like concatenation (joining two strings) or accessing individual characters.\n\nExample:\n\n# Concatenating strings\nfirst_name = \"John\"\nlast_name = \"Doe\"\nfull_name = first_name + \" \" + last_name  # Concatenating with a space in between\nprint(full_name)  # Output: John Doe\n\n# String interpolation using f-strings\nage = 25\nprint(f\"My name is {full_name} and I am {age} years old.\")\n# Output: My name is John Doe and I am 25 years old.\n\nJohn Doe\nMy name is John Doe and I am 25 years old.\n\n\nIn the example above: - We concatenate the first_name and last_name strings to form the full_name. - We also use f-strings (a modern and convenient way to insert variables into a string) to print a message with the person’s name and age.\n\n\n\nBooleans\nBooleans represent truth values (True or False) and are often used to control the flow of programs. They commonly result from comparison operations (e.g., checking if one number is greater than another).\n\nExample:\n\n# Boolean values\nis_student = True\nis_teacher = False\n\n# Comparison operations\nage = 18\nis_adult = age &gt;= 18  # True if age is greater than or equal to 18\nprint(is_adult)  # Output: True\n\nTrue\n\n\nHere, we create two boolean variables (is_student and is_teacher) and then use a comparison (&gt;=) to check if age qualifies as an adult (the result is stored in is_adult).\n\n\n\nChecking Data Types\nPython allows you to check the type of a variable using the type() function. This is useful when you want to confirm what kind of data a variable is holding.\n\nExample:\n\nx = 10\ny = 3.14\ntext = \"Hello\"\nis_python_fun = True\n\nprint(type(x))  # Output: &lt;class 'int'&gt;\nprint(type(y))  # Output: &lt;class 'float'&gt;\nprint(type(text))  # Output: &lt;class 'str'&gt;\nprint(type(is_python_fun))  # Output: &lt;class 'bool'&gt;\n\n&lt;class 'int'&gt;\n&lt;class 'float'&gt;\n&lt;class 'str'&gt;\n&lt;class 'bool'&gt;\n\n\nThis will print the type of each variable, helping you understand what type of data each one holds.\nVariables and data types are the building blocks of any program. In Python, variables can store different types of data, such as integers, floating-point numbers, strings, and booleans. We’ve seen how to create and manipulate these data types, and how to perform simple operations on them. These concepts will come in handy as we start working with more complex tasks like text processing.\nNext, we’ll explore how to manipulate strings further, as handling text is a core part of natural language processing.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Variables and data types"
    ]
  },
  {
    "objectID": "python_intro/lists_and_loops.html",
    "href": "python_intro/lists_and_loops.html",
    "title": "Lists and Loops",
    "section": "",
    "text": "In Python, a list is one of the most versatile data structures. It allows you to store multiple items in a single variable, like a collection of numbers, words, or any other kind of data. Lists are extremely useful when working with NLP tasks, where you often need to manage and process large amounts of text, words, or other data.\nTo work efficiently with lists, loops are used to iterate over the elements. By combining lists and loops, you can automate repetitive tasks and handle data efficiently. Let’s dive into how lists work and how to use loops to process them.\n\nLists in Python\nA list is an ordered collection of items, which can be of any type (numbers, strings, or even other lists). Lists are created using square brackets [], and each item in the list is separated by a comma.\n\nExample:\n\n# Creating a list\nfruits = [\"apple\", \"banana\", \"cherry\"]\nprint(fruits)  # Output: ['apple', 'banana', 'cherry']\n\n['apple', 'banana', 'cherry']\n\n\nIn this example, we’ve created a list called fruits that contains three strings: \"apple\", \"banana\", and \"cherry\".\n\n\n\nAccessing List Elements\nYou can access individual elements in a list using indexing. Just like strings, list indexing starts at 0 for the first element.\n\nExample:\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nprint(fruits[0])  # Output: apple (first element)\nprint(fruits[1])  # Output: banana (second element)\nprint(fruits[-1])  # Output: cherry (last element using negative index)\n\napple\nbanana\ncherry\n\n\nIn this example, we access the first, second, and last elements of the list.\n\n\n\nModifying List Elements\nYou can change elements in a list by accessing them via their index and assigning a new value.\n\nExample:\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfruits[1] = \"blueberry\"  # Changing 'banana' to 'blueberry'\nprint(fruits)  # Output: ['apple', 'blueberry', 'cherry']\n\n['apple', 'blueberry', 'cherry']\n\n\nHere, the second item in the list, \"banana\", is replaced with \"blueberry\".\n\n\n\nAdding and Removing Elements\nLists are dynamic, which means you can easily add or remove elements as needed.\n\nAdding elements: Use .append() to add an item to the end of a list.\nRemoving elements: Use .remove() to remove a specific item.\n\n\nExample:\n\nfruits = [\"apple\", \"banana\"]\nfruits.append(\"cherry\")  # Adding an item\nprint(fruits)  # Output: ['apple', 'banana', 'cherry']\n\nfruits.remove(\"banana\")  # Removing an item\nprint(fruits)  # Output: ['apple', 'cherry']\n\n['apple', 'banana', 'cherry']\n['apple', 'cherry']\n\n\nIn this example, we add \"cherry\" to the list and remove \"banana\" from it.\n\n\n\nList Operations: Length, Sorting, and Checking Existence\nHere are some common operations you can perform on lists:\n\nFinding the length of a list using len().\nSorting a list using .sort().\nChecking if an item exists in a list using the in operator.\n\n\nExample:\n\nfruits = [\"banana\", \"cherry\", \"apple\"]\n\n# Length of the list\nprint(len(fruits))  # Output: 3\n\n# Sorting the list\nfruits.sort()\nprint(fruits)  # Output: ['apple', 'banana', 'cherry']\n\n# Checking if an item exists\nprint(\"apple\" in fruits)  # Output: True\nprint(\"grape\" in fruits)  # Output: False\n\n3\n['apple', 'banana', 'cherry']\nTrue\nFalse\n\n\nHere, we find the number of items in the list, sort them alphabetically, and check whether \"apple\" and \"grape\" are in the list.\n\n\n\nLoops in Python\nLoops allow you to perform repetitive tasks efficiently. The most common type of loop used in Python for lists is the for loop. This loop allows you to go through each element in the list one by one.\n\n\nFor Loops\nA for loop lets you iterate over all the elements in a list and perform actions on them.\n\nExample:\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor fruit in fruits:\n    print(fruit)\n\napple\nbanana\ncherry\n\n\nThis code will print:\napple\nbanana\ncherry\nIn this example, the loop goes through each item in the list fruits and prints it.\n\n\n\nUsing Loops for Simple Operations\nYou can also use loops to perform more complex operations, such as processing the elements or performing calculations.\n\nExample: Squaring numbers in a list\n\nnumbers = [1, 2, 3, 4]\nsquares = []\nfor number in numbers:\n    squares.append(number ** 2)\n\nprint(squares)  # Output: [1, 4, 9, 16]\n\n[1, 4, 9, 16]\n\n\nIn this example, we loop through the list numbers, square each number, and store the results in a new list called squares.\n\n\n\nLooping with range()\nSometimes, you need to loop a specific number of times rather than over a list. The range() function helps create sequences of numbers for iteration.\n\nExample:\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\nIn this example, the loop runs 5 times, printing the values from 0 to 4 (since Python’s range() is exclusive of the upper bound).\n\n\n\nCommon List Methods\nHere are some of the most commonly used methods with lists:\n\nappend(): Adds an item to the end of the list.\n\nfruits.append(\"orange\")\n\nremove(): Removes the first occurrence of an item.\n\nfruits.remove(\"banana\")\n\ninsert(index, value): Inserts an item at a specific position.\n\nfruits.insert(1, \"blueberry\")\n\npop(): Removes and returns the last item (or an item at a specified index).\n\nlast_fruit = fruits.pop()\n\n\n\n\nSummary\nLists and loops are essential tools in Python, allowing you to store multiple pieces of data and process them efficiently. Lists help you group related items, while loops allow you to perform repetitive tasks, like iterating over each element in a list. Together, they form the foundation of many operations in Python, including more advanced NLP tasks where you’ll work with large datasets and need to process them in an automated way.\nNext, we’ll explore how to define functions in Python to structure your code more effectively and make it reusable.\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Lists and Loops"
    ]
  },
  {
    "objectID": "python_intro/overview.html",
    "href": "python_intro/overview.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Installing Python on Windows and macOS\n\nInstalling Python on Windows\n\nDownload the Installer:\n\nGo to the official Python website.\nClick on the “Download Python” button. This will download the latest version for Windows.\n\nRun the Installer:\n\nLocate the downloaded .exe file in your downloads folder and double-click it to run the installer.\nImportant: Check the box that says “Add Python to PATH” at the bottom of the installation window.\nChoose “Install Now” for a standard installation or “Customize installation” for more options.\n\nVerify Installation:\n\nOpen the Command Prompt by searching for cmd in the Start menu.\nType python --version and press Enter. You should see the installed version of Python.\n\n\n\n\nInstalling Python on macOS\n\nDownload the Installer:\n\nVisit the official Python website.\nClick on the “Download Python” button, which will get the latest version for macOS.\n\nRun the Installer:\n\nLocate the downloaded .pkg file and double-click it to launch the installer.\nFollow the on-screen instructions to complete the installation.\n\nVerify Installation:\n\nOpen the Terminal application (you can find it using Spotlight Search by pressing Command + Space and typing “Terminal”).\nType python3 --version and press Enter. You should see the installed version of Python.\n\n\n\n\nAdditional Setup (Optional)\nAfter installing Python, it’s a good idea to install pip, Python’s package manager, which is included by default in the latest Python versions. You can use pip to install additional libraries and packages as needed.\nFor Windows: - To install a package, open Command Prompt and type: bash   pip install package_name\nFor macOS: - Open Terminal and type: bash   pip3 install package_name\nThat’s it! You’re now ready to start programming in Python.\n\n\n\nUsing VSCode\nVisual Studio Code (VSCode) is a powerful and popular code editor developed by Microsoft. It is highly extensible, lightweight, and supports a wide range of programming languages, including Python. With its robust features such as IntelliSense, debugging capabilities, and integrated terminal, VSCode is an excellent choice for Python development.\n\nGetting Started\nTo start using Python in VSCode, follow these steps:\n\nInstall VSCode: If you haven’t already, download and install Visual Studio Code from the official website.\nInstall the Python Extension:\n\nOpen VSCode.\nGo to the Extensions view by clicking on the Extensions icon in the Activity Bar on the side or pressing Ctrl + Shift + X.\nSearch for “Python” and install the official extension provided by Microsoft. This extension adds rich support for Python development, including IntelliSense and linting.\n\nSelect the Python Interpreter:\n\nAfter installing the extension, you need to select the Python interpreter. Press Ctrl + Shift + P to open the Command Palette, then type and select Python: Select Interpreter.\nChoose the interpreter that matches your Python installation.\n\n\n\n\nWriting and Running Python Code\n\nCreate a New File:\n\nYou can create a new Python file by clicking on File &gt; New File or pressing Ctrl + N.\nSave it with a .py extension (e.g., script.py).\n\nWrite Your Code:\n\nBegin writing your Python code in the editor. For example:\n\nprint(\"Hello, VSCode!\")\nRun Your Code:\n\nThere are multiple ways to run your Python code:\n\nUsing the Terminal: Open the integrated terminal by selecting View &gt; Terminal or pressing Ctrl + ` (backtick). In the terminal, type python script.py (replacing script.py with your file name) to execute the script.\nRun Code Action: You can also run your code directly from the editor by clicking the play button (▶️) that appears above the code or using the shortcut Shift + Enter.\n\n\n\n\n\nDebugging in VSCode\nVSCode provides powerful debugging features to help you troubleshoot your code:\n\nSet Breakpoints: Click in the gutter next to the line numbers to set breakpoints where you want the execution to pause.\nStart Debugging: Press F5 or go to the Debug view by clicking on the Debug icon in the Activity Bar.\nYou can then start debugging your Python script. The Debug Console will allow you to inspect variables, step through code, and evaluate expressions.\n\n\n\nUsing Extensions and Features\nVSCode has a wide variety of extensions to enhance your Python development experience:\n\nLinting: The Python extension includes linting capabilities that help you catch errors and enforce coding standards. You can enable it in the settings (Settings &gt; Python &gt; Linting).\nIntelliSense: Take advantage of IntelliSense for code suggestions, autocompletions, and quick documentation. Simply start typing, and relevant suggestions will appear.\nJupyter Notebooks: If you want to work with Jupyter Notebooks directly in VSCode, install the Jupyter extension. This allows you to create, edit, and run notebooks seamlessly.\n\n\n\n\n\nJupyter Notebooks\nJupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. It is widely used in data science, machine learning, and scientific computing, making it a versatile tool for both beginners and advanced users. In a Jupyter Notebook, you can write and execute code in a variety of programming languages, including Python. It provides an interactive environment where you can document your thought process alongside your code, visualize data, and quickly test ideas without the need for a complete development setup.\n\nGetting Started\nOnce you have Jupyter Notebook up and running, you will typically start by opening a new notebook. Here are the key components and features of Jupyter Notebook to help you navigate and utilize it effectively:\n\n\nThe User Interface\nUpon launching Jupyter Notebook, you’ll be greeted with a dashboard showing your files and notebooks. You can create a new notebook by selecting “New” and then choosing the desired kernel (like Python 3).\n\nNotebook Cells: The main area consists of cells where you can write your code or text. There are two main types of cells:\n\nCode Cells: Where you write and execute code.\nMarkdown Cells: Where you can write formatted text, including headers, lists, and links.\n\n\n\n\nWriting and Executing Code\nTo write code in a code cell:\n\nClick on a cell to make it active.\nType your code into the cell.\n\n\nExample:\n\nprint(\"Hello, Jupyter!\")\n\nHello, Jupyter!\n\n\nTo execute the code, you can either click the “Run” button in the toolbar or press Shift + Enter. This will run the code and display the output directly below the cell.\n\n\n\nUsing Markdown for Documentation\nMarkdown cells allow you to document your code using plain text. You can format your text using Markdown syntax.\n\nExample:\nTo create a markdown cell with a header, simply type:\n# My Jupyter Notebook\nAfter running the cell, it will render as a formatted header.\nYou can also create bullet points, numbered lists, links, and more:\n## Key Features\n- Interactive coding\n- Inline visualizations\n- Rich text support\n\n\n\nVisualization and Output\nJupyter Notebook supports various visualization libraries like Matplotlib, Seaborn, and Plotly, allowing you to create plots and graphs inline.\n\nExample:\n\nimport matplotlib.pyplot as plt\n\n# Sample data\nx = [1, 2, 3, 4, 5]\ny = [2, 3, 5, 7, 11]\n\n# Creating a plot\nplt.plot(x, y)\nplt.title(\"Sample Plot\")\nplt.xlabel(\"X-axis\")\nplt.ylabel(\"Y-axis\")\nplt.show()\n\n\n\n\n\n\n\n\nAfter running this code, the plot will be displayed directly beneath the code cell.\n\n\n\nSaving and Sharing Notebooks\nYou can save your notebook by clicking the save icon or using the shortcut Ctrl + S (or Cmd + S on Mac). Jupyter Notebooks are saved with a .ipynb extension.\nTo share your notebook, you can export it to different formats, such as HTML or PDF, by using the “File” menu. You can also share the .ipynb file directly, which can be opened in any Jupyter environment.\n\n\nKeyboard Shortcuts\nJupyter Notebook has many handy keyboard shortcuts that can improve your efficiency. Here are a few essential ones:\n\nEnter: Edit the selected cell.\nEsc: Command mode (no editing).\nA: Insert a new cell above.\nB: Insert a new cell below.\nDD: Delete the selected cell.\nZ: Undo the last cell deletion.\nShift + Enter: Run the current cell and move to the next one.\nCtrl + Enter: Run the current cell and stay in it.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Introduction to Python"
    ]
  },
  {
    "objectID": "embeddings/exercises/ex_emb_similarity.html",
    "href": "embeddings/exercises/ex_emb_similarity.html",
    "title": "Exercise: Embedding similarity",
    "section": "",
    "text": "Task: Use the OpenAI embeddings API to compute the similarity between two given words or phrases.\nInstructions:\n\nChoose two words or phrases with similar or related meanings.\nUse the OpenAI embeddings API to obtain embeddings for both words or phrases.\nCalculate the cosine similarity between the embeddings to measure their similarity.\nPrint the similarity score and interpret the results.\n\n\n\nShow solution\n\n\nimport numpy as np\n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -&gt; float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\n\nimport os\n\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n# create the embeddings\nword_1 = \"king\"\nword_2 = \"queen\"\n\nresponse_1 = client.embeddings.create(input=word_1, model=MODEL)\nembedding_1 = response_1.data[0].embedding\nresponse_2 = client.embeddings.create(input=word_2, model=MODEL)\nembedding_2 = response_2.data[0].embedding\n\n\n# calculate the distance \ndist_12 = cosine_similarity(embedding_1, embedding_2)\nprint(f\"Cosine similarity between {word_1} and {word_2}: {round(dist_12, 3)}.\")\n\nCosine similarity between king and queen: 0.915.\n\n\n\nword_3 = \"pawn\"\nembedding_3 = client.embeddings.create(input=word_3, model=MODEL).data[0].embedding\n\ndist_13 = cosine_similarity(embedding_1, embedding_3)\nprint(f\"Cosine similarity between {word_1} and {word_3}: {round(dist_13, 3)}.\")\n\nCosine similarity between king and pawn: 0.829.\n\n\n\nTask: Use the OpenAI embeddings API and simple embedding arithmetics to introduce more context to word similarities.\nInstructions:\n\nCreate embeddings for the following three words: python, snake, javascript using the OpenAI API.\nCalculate the cosine similarity between each pair.\nCreate another embedding for the word reptile and add it to python. You can use numpy for this.\nCalculate the cosine similarity between python and this sum. What do you notice?\n\n\n\nShow solution\n\n\nwords = [\"python\", \"snake\", \"javascript\", \"reptile\"]\nresponse = client.embeddings.create(input=words, model=MODEL)\nembeddings = [emb.embedding for emb in response.data]\n\n\nprint(f\"Similarity between '{words[0]}' and '{words[1]}': {round(cosine_similarity(embeddings[0], embeddings[1]), 3)}.\")\nprint(f\"Similarity between '{words[0]}' and '{words[2]}': {round(cosine_similarity(embeddings[0], embeddings[2]), 3)}.\")\nprint(f\"Similarity between '{words[0]} + {words[3]}' and '{words[1]}': {round(cosine_similarity(np.array(embeddings[0]) + np.array(embeddings[3]), embeddings[1]), 3)}.\")\n\nSimilarity between 'python' and 'snake': 0.841.\nSimilarity between 'python' and 'javascript': 0.85.\nSimilarity between 'python + reptile' and 'snake': 0.894.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Exercise: Embedding similarity"
    ]
  },
  {
    "objectID": "embeddings/embeddings.html",
    "href": "embeddings/embeddings.html",
    "title": "Embeddings",
    "section": "",
    "text": "Let us quickly re-visit the concept of embeddings we have already encountered before. We have seen them as a means of transforming text into numerical vectors that can be fed to neural network architectures for language models. But interestingly, we can do a lot more with embeddings than simply this.\nOne of the key benefits of embeddings is their ability to capture semantic similarities and relationships between words. When created with an appropriate model, embeddings do not only transform text into vectors, but they do it while compressing the contained information. More simply put, words with similar meanings or contexts tend to have embeddings that are close together in the vector space, while words with different meanings are farther apart. This enables algorithms (and us!) to perform tasks such as word similarity calculation more effectively. But let’s start with a quick recap of what embeddings are.",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Embeddings"
    ]
  },
  {
    "objectID": "embeddings/embeddings.html#what-are-embeddings",
    "href": "embeddings/embeddings.html#what-are-embeddings",
    "title": "Embeddings",
    "section": "What are embeddings?",
    "text": "What are embeddings?\nAs mentioned, embeddings play a crucial role in representing words as dense vectors in a continuous vector space. While, for example, the bag of words model has been a simple and widely-used approach for representing text, it has its limitations, including a fixed vocabulary and the inability to capture nuanced semantic relationships between words. Embeddings address these shortcomings by leveraging the power of contextual representations. Instead of representing each word in the vocabulary as a one-hot encoded vector, where each word is represented by a binary vector with a dimension equal to the vocabulary size, embeddings generate dense vector representations for words that encode rich semantic information.\nUnlike the bag of words model, embeddings are thus context-aware, meaning they capture the meaning of words based on their surrounding context in the text. This contextual understanding allows embeddings to capture subtle semantic relationships between words, such as synonymy, antonymy, and semantic similarity. Moreover, embeddings offer a more compact representation of words compared to the sparse vectors used in the bag of words model. By compressing the information into dense vectors of fixed dimensionality, embeddings reduce the dimensionality of the input space, making it more manageable for downstream tasks and allowing for more efficient computation.\nThere are plenty of different approaches to generate embeddings, and often embeddings are created as some sort of byproduct of training large. language models. As an example, we will have a quick look at Word2Vec, which is a popular technique for generating word embeddings based on distributed representations of words in a continuous vector space. The key idea behind Word2Vec is to train a neural network model to predict the surrounding words (context) of a given target word in a large corpus of text. This process can be done using either the continuous bag of words (CBOW) or skip-gram architectures. In the CBOW model, the input is the context words, and the output is the target word, while in the skip-gram model, the input is the target word, and the output is the context words. By training the model on a large corpus of text, Word2Vec then learns to encode semantic relationships between words in the form of dense vector representations, our embeddings.\nBut, of course, embeddings can also be obtained by transformer architectures such as GPT. We will use the embeddings provided by OpenAI for some demonstration.",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Embeddings"
    ]
  },
  {
    "objectID": "embeddings/embeddings.html#matching-with-embeddings",
    "href": "embeddings/embeddings.html#matching-with-embeddings",
    "title": "Embeddings",
    "section": "Matching with embeddings",
    "text": "Matching with embeddings\nLet’s do a quick example and re-visit our idea of matching a search prompt with documents. In the previous section we have used a bag of words to compare the three texts to the prompt and realized that this technique is not particularly good. Using embeddings, we can do the same and be a lot better.\n\ntexts = [\n  \"This is the first document.\",\n  \"This document is the second document.\",\n  \"And this is the third one.\"\n]\n\nprompt = \"Is this the first document?\"\n\n\n# prerequisites\n\nimport os\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value # choose the embedding model\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n\n# get the embeddings\nresponse = client.embeddings.create(\n    input=texts,\n    model=MODEL\n)\n\ntext_embeddings = [emb.embedding for emb in response.data]\n\nresponse = client.embeddings.create(\n    input=[prompt],\n    model=MODEL\n)\n\nprompt_embedding = response.data[0].embedding\n\n\nimport numpy as np\n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -&gt; float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\n\nfor text, text_embedding in zip(texts, text_embeddings):\n    similarity = cosine_similarity(text_embedding, prompt_embedding)\n    print(f\"{text}: {round(similarity, 2)}\")\n\nThis is the first document.: 0.95\nThis document is the second document.: 0.88\nAnd this is the third one.: 0.8\n\n\nAs we can see, there is a clear winner in terms of similarity, and that would have been exactly the document we would have needed. So embeddings provide a great tool to identify matching documents (or texts in general), and are applicable in many different use cases. An almost classic one is creating a chatbot, that can answer questions based on documents: When a user provides a prompt, we use embeddings to find the best matching documents, and then use the content to provide an answer. An example can be found here.",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Embeddings"
    ]
  },
  {
    "objectID": "llm/exercises/ex_gpt_parameterization.html",
    "href": "llm/exercises/ex_gpt_parameterization.html",
    "title": "Exercise: GPT Parameterization",
    "section": "",
    "text": "Task: Explore the parameterization possibilities of the OpenAI API for GPT.\nInstructions:\nSome possibilities are:\n\nUse the system role in order to give instructions to the language model before the interaction with the user starts in order to change the response style of the model.\nChange the temparature or top_p parameters and explore the effect on your prompts.\nUse the\n\n\nimport os\nfrom llm_utils.client import get_openai_client\n\nMODEL = \"gpt4\"\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n\n# here goes your code\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Exercise: GPT Parameterization"
    ]
  },
  {
    "objectID": "llm/exercises/ex_gpt_chatbot.html",
    "href": "llm/exercises/ex_gpt_chatbot.html",
    "title": "Exercise: GPT Chatbot",
    "section": "",
    "text": "Task: Create a simple chatbot using the OpenAI chat.completions API.\nInstructions:\n\nUse the chat.completions API to send prompts to GPT, receive the answers and displaying them.\nStop the conversation when the user inputs the word exit instead of a new prompt.\nHint: Remember that GPT has no memory, so you always have to include the previous conversation in your prompts.\n\n\n\nShow solution\n\n\nimport os\nfrom llm_utils.client import get_openai_client\n\nMODEL = \"gpt4o\"\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n\nclass ChatGPT:\n    def __init__(self, model=MODEL):\n        self.model = model\n        self.client = client\n        self.messages = []\n\n    def chat_with_gpt(self, user_input: str):\n        self.messages.append({\n            \"role\": \"user\",\n            \"content\": user_input\n        })\n        response = self._generate_response(self.messages)\n        return response\n\n    def _generate_response(self, messages):\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,        \n            temperature=0.2, \n            max_tokens=150,\n            top_p=1.0\n        )\n        response_message = response.choices[0].message\n        self.messages.append({\n            \"role\": response_message.role,\n            \"content\": response_message.content\n        })\n\n        return response_message.content\n\n\n# Conversation loop\nchat_gpt = ChatGPT(model=\"gpt4\")\n\nwhile True:\n    user_input = input(\"User: \")\n\n    if user_input.lower() == 'exit':\n        break\n    \n    print(\"User:\", user_input)\n    \n    # Get bot response based on user input\n    bot_response = chat_gpt.chat_with_gpt(user_input)\n\n    print(\"Bot:\", bot_response)\n\nUser: Hello\nBot: Hi there! How can I assist you today?\nUser: \nBot: It looks like your message got cut off. How can I help you today?\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Exercise: GPT Chatbot"
    ]
  },
  {
    "objectID": "llm/gpt.html",
    "href": "llm/gpt.html",
    "title": "GPT: Generative Pre-trained Transformer",
    "section": "",
    "text": "In a seminar about natural language processing and language models, the current flagship cannot be left out: GPT. While it is worthwhile having a look at the architecture of GPT itself in order to understand why it its performance has led to the fantastic advancements in the last years, this by far exceeds the scope of this seminar. Instead, we want to learn how to work with it using the OpenAI API in the next sections. However, here is at least a small (and definitely not comprehensive) overview of what GPT is and what distinguishes it from its predecessors.\n\nA short introduction to GPT\nGPT (Generative Pre-trained Transformer) is one of the current state-of-the-art language model architectures. It belongs to the family of transformer-based models, which have revolutionized NLP in recent years. What distinguishes GPT from its predecessors (and the approaches we have discussed before) is its remarkable ability to understand and generate human-like text. One key advantage of GPT over previous approaches lies in its architecture. The transformer architecture, upon which GPT is built, introduces a novel mechanism called self-attention. This mechanism allows the model to weigh the importance of different words in a sentence dynamically, capturing long-range dependencies and contextual information more effectively than earlier models like recurrent neural networks (RNNs) or convolutional neural networks (CNNs). Another factor contributing to GPT’s superiority is its scalability. By leveraging the parallelism inherent in the transformer architecture, GPT can efficiently process and learn from vast amounts of data and hence allowing it to scale to unprecedented sizes. Larger models trained on more data tend to exhibit better performance, as being exposed to a diverse range of linguistic patterns and structures during training let’s them capture more complex patterns and nuances in language. The pre-training works in a similar fashion as we’ve already seen: GPT learns to predict the next word in a sequence given the preceding context. This process enables it to capture the nuances of language across various domains and lets it generate coherent and contextually relevant text across a wide range of topics and writing styles.\n\n\nWhat is a transformer?\nThe transformer architecture has been introduced in the landmark paper “Attention is All You Need” by Vaswani et al. in 2017 and revolutionized the way sequential data, like text, is processed and understood. At the heart of the transformer architecture lies the so-called self-attention mechanism, which is a novel way of capturing the relationships between different elements in a sequence, i.e., individual words or tokens in a sentence. Unlike traditional architectures, where information flow is constrained by fixed-length context windows or recurrent connections, transformers enable each word in a sentence to attend to all other words simultaneously through self-attention.\nThis mechanism allows transformers to dynamically weigh the importance of each word in the context of the entire sequence. This means that words that are semantically related or have a strong contextual influence on each other will receive higher attention weights, while irrelevant or less informative words will receive lower weights. By capturing these dependencies across the entire sequence, transformers excel at capturing long-range dependencies and contextual information, which is crucial for understanding and generating coherent text.\nA very important detail is that the self-attention mechanism enables transformers to process input sequences in parallel rather than sequentially, leading to significant computational advantages. Unlike RNNs, which process inputs one token at a time in a sequential manner, transformers can process all tokens in parallel, making them highly efficient and scalable, especially when dealing with long sequences or large datasets. This parallelism not only speeds up training but also allows transformers to capture complex patterns and relationships in data more effectively.\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "GPT: Generative Pre-trained Transformer"
    ]
  },
  {
    "objectID": "llm/gpt_api.html",
    "href": "llm/gpt_api.html",
    "title": "The OpenAI API",
    "section": "",
    "text": "Note\n\n\n\nResource: OpenAI API docs\n\n\nLet’s finally get started working with GPT. In this seminar, we will use the OpenAI API to work with, but there are many alternatives out there. We have collected a few in the resources.\n\nAuthentication\nGetting started with the OpenAI Chat Completions API requires signing up for an account on the OpenAI platform. Once you’ve registered, you’ll gain access to an API key, which serves as a unique identifier for your application to authenticate requests to the API. This key is essential for ensuring secure communication between your application and OpenAI’s servers. Without proper authentication, your requests will be rejected. You can create your own account, but for the seminar we will provide the client with the credential within the University’s Jupyterlab.\n\n# setting up the client in Python\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\")\n)\n\n\n\nRequesting Completions\nMost interaction with GPT and other models consist in generating completions for prompts, i.e., providing some text with instructions and letting the language model complete the text one token after the other as seen here.\nTo request completions from the OpenAI API, we use Python to send HTTP requests to the designated API endpoint. These requests are structured to include various parameters that guide the generation of text completions. The most fundamental parameter is the prompt text, which sets the context for the completion. Additionally, you can specify the desired model configuration, such as the engine to use (e.g., “gpt-4”), as well as any constraints or preferences for the generated completions, such as the maximum number of tokens or the temperature for controlling creativity (here).\n\n# creating a completion\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How old is the earth?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\" # choose the model\n)\n\n\n\nProcessing\nOnce the OpenAI API receives your request, it proceeds to process the provided prompt using the specified model. This process involves analyzing the context provided by the prompt and leveraging the model’s pre-trained knowledge to generate text completions. The model employs advanced natural language processing techniques to ensure that the generated completions are coherent and contextually relevant. By drawing from its extensive training data and understanding of human language, the model aims to produce responses that closely align with human-like communication.\n\n\nResponse\nAfter processing your request, the OpenAI API returns a response containing the generated text completions. Depending on the specifics of your request, you may receive multiple completions, each accompanied by additional information such as the amount of token processed in the request, the reason why the model stopped the answer etc. This response provides valuable insights into the quality and relevance of the completions, allowing you to tailor your application’s behavior accordingly. Let’s check it out briefly, before you explore the response object more in-depth in your next exercise.\n\n# check out the type of the response\n\nprint(f\"Response object type: {type(chat_completion)}\") # a ChatCompletion object\n\n# print the message we want\nprint(f\"\\nResponse message: {chat_completion.choices[0].message.content}\")\n\n# check the tokens used \nprint(f\"\\nTotal tokens used: {chat_completion.usage.total_tokens}\")\n\nResponse object type: &lt;class 'openai.types.chat.chat_completion.ChatCompletion'&gt;\n\nResponse message: The Earth is estimated to be around 4.5 billion years old.\n\nTotal tokens used: 28\n\n\n\n\nError Handling\nWhile interacting with the OpenAI API (or any API for that matter), it’s crucial to implement some robust error handling mechanisms to manage any potential issues that may arise. The kind of classic errors include providing invalid parameters, experiencing authentication failures due to an incorrect API key, or encountering rate limiting restrictions. But for language models in particular, there are plenty more problems that can arise simply involving the answer we get from the model. Some examples are requests involving explicit language or content or restricted content etc. which are typically blocked by the API. Other times it might simply happen that a model does not respond in a way you expected, for example, just repeating your input instead of responding properly, or not responding in the format you requested. Whenever we are using language model for applications, we need to be aware of this and implement the right measures to handle these situations.\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "The OpenAI API"
    ]
  },
  {
    "objectID": "exercises_no_solution/ex_gpt_parameterization.html",
    "href": "exercises_no_solution/ex_gpt_parameterization.html",
    "title": "Exercise: GPT Parameterization",
    "section": "",
    "text": "Task: Explore the parameterization possibilities of the OpenAI API for GPT.\nInstructions:\nSome possibilities are:\n\nUse the system role in order to give instructions to the language model before the interaction with the user starts in order to change the response style of the model.\nChange the temparature or top_p parameters and explore the effect on your prompts.\nUse the\n\n\nimport os\nfrom llm_utils.client import get_openai_client\n\nMODEL = \"gpt4\"\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n\n# here goes your code\n\n\n\n\n Back to top"
  },
  {
    "objectID": "exercises_no_solution/ex_emb_similarity.html",
    "href": "exercises_no_solution/ex_emb_similarity.html",
    "title": "Exercise: Embedding similarity",
    "section": "",
    "text": "Task: Use the OpenAI embeddings API to compute the similarity between two given words or phrases.\nInstructions:\n\nChoose two words or phrases with similar or related meanings.\nUse the OpenAI embeddings API to obtain embeddings for both words or phrases.\nCalculate the cosine similarity between the embeddings to measure their similarity.\nPrint the similarity score and interpret the results.\n\n\nTask: Use the OpenAI embeddings API and simple embedding arithmetics to introduce more context to word similarities.\nInstructions:\n\nCreate embeddings for the following three words: python, snake, javascript using the OpenAI API.\nCalculate the cosine similarity between each pair.\nCreate another embedding for the word reptile and add it to python. You can use numpy for this.\nCalculate the cosine similarity between python and this sum. What do you notice?\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "exercises_no_solution/ex_tfidf.html",
    "href": "exercises_no_solution/ex_tfidf.html",
    "title": "Exercise: TF-IDF",
    "section": "",
    "text": "Task: Extend the code for the bag of words to TF-IDF (Term Frequency-Inverse Document Frequency) vectors for a given set of documents. TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents. This measure helps in identifying words that are unique and informative to a particular document while downweighting common words that appear across many documents.\nTF-IDF consists of two main components:\nTerm Frequency (TF): This component measures how frequently a term occurs in a document. It is calculated as the ratio of the count of a term in a document to the total number of terms in the document. TF is higher for words that occur more frequently within a document.\nTF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\nInverse Document Frequency (IDF): This component measures the rarity of a term across the entire corpus of documents. It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term, plus one. IDF is higher for terms that are rare across documents but present in only a few documents.\nIDF(t) = log((1 + Total number of documents) / (1 + Number of documents containing term t))\nThe TF-IDF score for a term in a document is obtained by multiplying its TF and IDF scores. This score reflects the importance of the term in the context of the document and the entire corpus.\nInstructions:\n\nImplement functions calculate_tf and calculate_idf to calculate Term Frequency (TF) and Inverse Document Frequency (IDF) respectively.\nWrite a create_tf_idf function to create TF-IDF vectors for a given set of documents. This function should count the frequency of each word in the corpus, calculate TF and IDF, and compute TF-IDF vectors for each document.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "exercises_no_solution/ex_tokenization.html",
    "href": "exercises_no_solution/ex_tokenization.html",
    "title": "Exercise: Sentence tokenization",
    "section": "",
    "text": "Task: Write a sentence tokenizer that takes the given paragraph and tokenizes it into sentences. Then, count the number of sentences and display the result.\nInstructions:\n\nStart with just a simple punctuation (.) as the delimiter for sentences.\nCheck out the regex library re and the function re.split to include also other delimiters. Try out the following regex r'[.:;!?]\\s*'.\n\n\nparagraph = \"The distant planet, its surface shrouded in mystery and intrigue! With its swirling clouds and alien landscapes, the planet: a tantalizing enigma to explorers and scientists alike? Oh, the wonders it conceals: ancient ruins and extraterrestrial life forms, waiting to be discovered! As the spacecraft descended through the atmosphere, anticipation filled the hearts of the crew. Little did they know, their journey was about to unveil secrets beyond their wildest imagination.\"\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "exercises_no_solution/ex_fuzzy_matching.html",
    "href": "exercises_no_solution/ex_fuzzy_matching.html",
    "title": "Exercise: Fuzzy matching",
    "section": "",
    "text": "Task: Write a function that helps finding the most similar words or tokens from a given list based on a user query using rapidfuzz.\nInstructions:\n\nWrite a Python function called find_similar_words(query, word_list) that takes a user query and a list of words or tokens as input.\nInside the function, use rapidfuzz to calculate the similarity between the query and each word/token in the list.\nReturn a list of tuples containing the word/token and its corresponding similarity score, sorted in descending order of similarity.\n\n\nword_list = [\"apple\", \"banana\", \"orange\", \"grape\", \"pineapple\", \"kiwi\"]\nquery = \"appl\"\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "exercises_no_solution/ex_gpt_start.html",
    "href": "exercises_no_solution/ex_gpt_start.html",
    "title": "Exercise: OpenAI - Getting started",
    "section": "",
    "text": "Task: Explore the OpenAI chat.completions API.\nInstructions:\n\nGenerate a chat completion and analyze the response object ChatCompletion. What information do you get with each completion?\nHow can you access the actual completion of your prompt?\nUse the OpenAI API documentation to find out what choices are and how they are used.\nPlay around with the parameters temperature and top_p for a simple prompt. What do you notice?\n\n\n\n\n Back to top"
  },
  {
    "objectID": "exercises_no_solution/ex_gpt_chatbot.html",
    "href": "exercises_no_solution/ex_gpt_chatbot.html",
    "title": "Exercise: GPT Chatbot",
    "section": "",
    "text": "Task: Create a simple chatbot using the OpenAI chat.completions API.\nInstructions:\n\nUse the chat.completions API to send prompts to GPT, receive the answers and displaying them.\nStop the conversation when the user inputs the word exit instead of a new prompt.\nHint: Remember that GPT has no memory, so you always have to include the previous conversation in your prompts.\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "exercises_no_solution/ex_word_matching.html",
    "href": "exercises_no_solution/ex_word_matching.html",
    "title": "Exercise: Word matching",
    "section": "",
    "text": "Task: For each element of the following list of keywords, determine whether it is contained in the text.\nInstructions:\n\nTransform the text to lower case and use a tokenizer to split the text into word tokens.\nFirst, use a simple comparison of strings to check whether the keywords match any token. When does this approach fail?\nLemmatize the tokens from your text in order to handle some more matching cases. When does this approach still fail? Hint: Use the different options for pos in order to handle different types of words such as nouns, verbs etc.\n\n\ntext = \"The company's latest quarterly earnings reports exceeded analysts' expectations, driving up the stock price. However, concerns about future growth prospects weighed on investor sentiment. The CEO announced plans to diversify the company's product portfolio and expand into new markets, aiming to sustain long-term profitability. The marketing team launched a new advertising campaign to promote the company's flagship product, targeting key demographics. Despite challenges in the competitive landscape, the company remains committed to innovation and customer satisfaction.\"\n\n\nkeywords = [\n    \"Announce\", \n    \"Aim\",\n    \"Earnings\",\n    \"Quarter\",\n    \"Report\",\n    \"Investor\",\n    \"Analysis\",\n    \"Market\",\n    \"Diversity\",\n    \"Product portfolio\",\n    \"Advertisment\",\n    \"Stock\",\n    \"Landscpe\" # yes, this is here on purpose\n]\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "exercises_no_solution/ex_gpt_ner_with_function_calls.html",
    "href": "exercises_no_solution/ex_gpt_ner_with_function_calls.html",
    "title": "Exercise: NER with tool calling",
    "section": "",
    "text": "Task: Create a small script that uses tool (or function calling) to extract the following named entities from a given text: City, State, Person.\nInstructions:\n\nDefine an OpenAI tool with a function named_entity_recognition.\nChoose an appropriate output format, for example: {\"named_entities\": [{\"entity\": \"Mike\", \"label\": \"Person}, {\"entity\": \"Münster\", \"label\": \"City\"}]}\nDefine a matching prompt in the role system and the text input for the role user.\nExtract the result.\n\n\n# prerequisites\nimport os\nfrom llm_utils.client import get_openai_client\n\nMODEL = \"gpt4\"\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n# here goes your code\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "llm/parameterization.html",
    "href": "llm/parameterization.html",
    "title": "Parameterization of GPT",
    "section": "",
    "text": "The GPT models provided by OpenAI provide a variety of parameters that can change the way the language model responds. Below you can find a list of the most important ones.",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Parameterization of GPT"
    ]
  },
  {
    "objectID": "llm/parameterization.html#roles",
    "href": "llm/parameterization.html#roles",
    "title": "Parameterization of GPT",
    "section": "Roles",
    "text": "Roles\nIn order to cover most tasks you want to perform using a chat format, the OpenAI API let’s you define different roles in the chat. The available roles are system, assistant, user and tools. You should already be familiar with two of them by now: The user role corresponds to the actual user prompting the language model, all answers are given with the assisstant role.\nThe system role can now be given to provide some additional general instructions to the language model that are typically not a user input, for example, the style in which the model responds. In this case, an example is better than any explanation.\n\nimport os\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.GPT_4o.value\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\ncompletion = client.chat.completions.create(\n  model=\"MODEL\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are an annoyed technician working in a help center for dish washers, who answers in short, unfriendly bursts.\"},\n    {\"role\": \"user\", \"content\": \"My dish washer does not clean the dishes, what could be the reason.\"}\n  ]\n)\n\nprint(completion.choices[0].message.content)\n\nCheck for clogs and clean the filters.",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Parameterization of GPT"
    ]
  },
  {
    "objectID": "llm/parameterization.html#function-calling",
    "href": "llm/parameterization.html#function-calling",
    "title": "Parameterization of GPT",
    "section": "Function calling",
    "text": "Function calling\nAs we have seen, most interactions with a language model happen in form of a chat with almost “free” question or instructions and answers. While this seems the most natural in most cases, it is not always a practical format if we want to use a language model for very specific purposes. This happens particularly often when we want to employ a language model in business situations, where we require a consistent output of the model.\nAs an example, let us try to use GPT for sentiment analysis (see also here). Let’s say we want GPT to classify a text into one of the following four categories:\n\nsentiment_categories = [\n    \"positive\", \n    \"negative\",\n    \"neutral\",\n    \"mixed\"\n]\n\nWe could do the following:\n\nmessages = []\nmessages.append(\n    {\"role\": \"system\", \"content\": f\"Classify the given text into one of the following sentiment categories: {sentiment_categories}.\"}\n)\nmessages.append(\n    {\"role\": \"user\", \"content\": \"I really did not like the movie.\"}\n)\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=MODEL\n)\n\nprint(f\"Response: '{response.choices[0].message.content}'\")\n\n\n\nResponse: 'Category: Negative'\n\n\nIt is easy to spot the problem: GPT does not necessarily answer in the way we expect or want it to. In this case, instead of simply returning the correct category, it also returns the string Category: alongside it (and capitalized Negative). So if we were to use the answer in a program or data base, we’d now again have to use some NLP techniques to parse it in order eventually retrieve exactly the category we were looking for: negative. What we need instead is a way to constrain GPT to a specific way of answering, and this is where functions or tools come into play (see also Function calling and Function calling (cookbook)).\nThis concept allows us to specify the exact output format we expect to receive from GPT (it is called functions since ideally we want to call a function directly on the output of GPT so it has to be in a specific format).\n\n# this looks intimidating but isn't that complicated\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_sentiment\",\n            \"description\": \"Analyze the sentiment in a given text.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"sentiment\": {\n                        \"type\": \"string\",\n                        \"enum\": sentiment_categories,\n                        \"description\": f\"The sentiment of the text.\"\n                    }\n                },\n                \"required\": [\"sentiment\"],\n            }\n        }\n    }\n]\n\n\nmessages = []\nmessages.append(\n    {\"role\": \"system\", \"content\": f\"Classify the given text into one of the following sentiment categories: {sentiment_categories}.\"}\n)\nmessages.append(\n    {\"role\": \"user\", \"content\": \"I really did not like the movie.\"}\n)\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=MODEL,\n    tools=tools,\n    tool_choice={\n        \"type\": \"function\", \n        \"function\": {\"name\": \"analyze_sentiment\"}}\n)\n\nprint(f\"Response: '{response.choices[0].message.tool_calls[0].function.arguments}'\")\n\nResponse: '{\n\"sentiment\": \"negative\"\n}'\n\n\nWe can now easily extract what we need:\n\nimport json \nresult = json.loads(response.choices[0].message.tool_calls[0].function.arguments) # remember that the answer is a string\nprint(result[\"sentiment\"])\n\nnegative\n\n\nWe can also include multiple function parameters if our desired output has multiple components. Let’s try to include another parameter which includes the reason for the sentiment.\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_sentiment\",\n            \"description\": \"Analyze the sentiment in a given text.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"sentiment\": {\n                        \"type\": \"string\",\n                        \"enum\": sentiment_categories,\n                        \"description\": f\"The sentiment of the text.\"\n                    },\n                    \"reason\": {\n                        \"type\": \"string\",\n                        \"description\": \"The reason for the sentiment in few words. If there is no information, do not make assumptions and leave blank.\"\n                    }\n                },\n                \"required\": [\"sentiment\", \"reason\"],\n            }\n        }\n    }\n]\n\n\nmessages = []\nmessages.append(\n    {\"role\": \"system\", \"content\": f\"Classify the given text into one of the following sentiment categories: {sentiment_categories}. If you can, also extract the reason.\"}\n)\nmessages.append(\n    {\"role\": \"user\", \"content\": \"I loved the movie, Johnny Depp is a great actor.\"}\n)\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=MODEL,\n    tools=tools,\n    tool_choice={\n        \"type\": \"function\", \n        \"function\": {\"name\": \"analyze_sentiment\"}}\n)\n\nprint(f\"Response: '{response.choices[0].message.tool_calls[0].function.arguments}'\")\n\nResponse: '{\n\"sentiment\": \"positive\",\n\"reason\": \"Loved the movie and appreciates Johnny Depp's acting\"\n}'\n\n\nHere, again, we could also constrain the possibilities for the reason to a certain set. Hence, functions are great to have more consistent answers of the language model such that we can use it in applications.",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Parameterization of GPT"
    ]
  },
  {
    "objectID": "llm/intro.html",
    "href": "llm/intro.html",
    "title": "Introduction to LLM",
    "section": "",
    "text": "Note\n\n\n\nIn this seminar, we will not go into detail about the technicalities of large language models and, in particular, the transformers used in GPT. However, it is useful to have at least a rough understanding of how modern large language models work in general such that we can both appreciate their capabilities, but also understand and be aware of their limitations. This is the goal of the following sections.\n\n\nWith the rise of machine learning and deep learning in the 2000s, the focus of NLP significantly shifted into this direction as well and researchers began employing neural networks for tasks such as text classification and sentiment analysis. The major difference to other machine learning applications is in the way the input to the neural networks is processed. Traditionally, neural networks require numerical input which is fed into the network and processed by the different layers. In natural language processing, the input data is text, however, so how would we feed text into a neural network?\nLuckily, we have already seen part of the solution to this problem: word and text embeddings or vectorization. A very simple example we have encountered is a Bag of Words, but there are many better ideas available. We will dive more into the details in the section about embeddings, but essentially embeddings constitute a numerical representation of the input text, that we can use them to feed text into a classical neural network.\nA simple architecture could look like this:\n\n\n\n\n\nflowchart LR\n  A(Input Text) --&gt; B(Tokenization)\n  B --&gt; C(Token processing)\n  C --&gt; D(Embedding Layer)\n  D --&gt; E(Hidden Layers)\n  E --&gt; F(Output Layer)\n\n\n\n\n\n\nIn this kind of architecture, the input text serves is the raw text data, which then undergoes preprocessing steps like cleaning, tokenization and stop word removal to make it machine-readable. Next, the token are fed into an embedding layer, where they are transformed into a vector representation, turning text into numerical values. If we do it well, these embeddings capture the semantic meaning of words, enabling the model to understand the text and its context. Recalling or BoW example, the first three stages could look like this:\n\n\n\n\n\nflowchart LR\n  A(A cat does cat things) --&gt; B{\" \"}\n  B --&gt; C(A)\n  B --&gt; D(cat)\n  B --&gt; E(does)\n  B --&gt; F(cat)\n  B --&gt; G(things)\n  D --&gt; H(cat)\n  E --&gt; I(do)\n  F --&gt; J(cat)\n  G --&gt; K(thing)\n\n  H --&gt; L(cat: 2)\n  J --&gt; L\n  I --&gt; M(do: 1)\n  K --&gt; N(thing: 1)\n\n  L --&gt; O(2)\n  M --&gt; P(1)\n  N --&gt; Q(1)\n\n\n\n\n\n\n\nThis numerical representation of our text then flows through the neural network. It passes through one or more hidden layers consisting of neurons that learn patterns and relationships within the input data, helping the model extract relevant features and information. Finally, the processed data reaches the output layer, where the model produces its final output. Depending on the task at hand, this output could take various forms, such as sentiment class probabilities or text labels. As an example, let’s have a look at text classification.\n\nExample: Text classification with a neural network\nThe first step, as usual in machine learning tasks, is to gather a dataset consisting of text documents along with their corresponding labels or categories. For example, if we’re classifying news articles into categories like sports, politics, and entertainment, we would need a dataset where each article is labeled with its respective category. Once we have enough labeled data at hand, the text data needs to be preprocessed to convert it into a format suitable for training. This typically involves the steps we have already encountered like tokenization (splitting text into words or subwords), removing punctuation and stop words, and converting words to lowercase for easier processing. Next, the text data is converted into numerical vectors that can be fed into a neural network. This is usually done by representing each word as a unique index or by using techniques like word embeddings (e.g., Word2Vec, GloVe) to represent words as dense vectors.\nThe model training then follows in a usual setting: The preprocessed and vectorized data is split into training and validation sets. The training set is used to train the neural network, while the validation set is used to evaluate its performance and tune the hyper parameters of the network or its general structure. During training, the neural network learns to map input text vectors to their corresponding class labels by iteratively adjusting the weights of the network to minimize a loss function, which measures the difference between the predicted labels and the true labels. If we think of, for example, think of BoW embeddings again, the frequency of certain words in the input text might increase of decrease the likelihood for certain labels in the output.\n\n\nSequence generation and language modeling\nA special NLP problem that is worth having a look at is the idea of sequence generation, where models are trained to generate sequences of data, such as text, images, or music, based on given input or context. These models learn to understand the underlying patterns and relationships in the data and generate new sequences that resemble the training data. Sequence generation has diverse applications, including natural language generation, image captioning, and music composition. An important part of text sequences (as opposed to, for example, text classification), is that they require an inherent understanding of the structure of the language (because otherwise the text sequences will be nonsense). So if models are able to perform this task, they must have “understood” the concept and structure of language to a certain extent.\nThis idea leads to an even more important concept: language modeling. Language modeling is a fundamental task that aims to understand and predict the structure, context, and semantics of human language. At its core, language modeling involves training a model to predict the probability distribution of words or tokens in a sequence given the preceding context. In simple words, given a sequence of words or token, which tokens are most likely to appear next?\n\n\n\n\n\nflowchart LR\n  A(The) --&gt; B(bird)\n  B --&gt; C(flew)\n  C --&gt; D(over)\n  D --&gt; E(the)\n  E --&gt; F{?}\n  F --&gt; G(\"p(rooftops)=0.31\")\n  F --&gt; H(\"p(trees)=0.14\")\n  F --&gt; J(\"p(guitar)=0.001\")\n\n\n\n\n\n\nThis predictive capability allows language models to generate coherent and contextually relevant text. The training process of a language model typically involves exposing the model to large amounts of text data (also called a corpus) and teaching it to learn the statistical properties of language. This includes capturing syntactic structures, semantic relationships, and contextual nuances present in natural language. The great part is that the training process can happen in an unsupervised fashion: we do not require any labeled data, but can virtually use any type of text available. In particular with the internet as an open source to mainly text data, we have massive amounts of data available to perform language modeling on.\nOne of the key challenges in language modeling, however, is handling the vast and diverse nature of human language. Language exhibits complex patterns, variations, and ambiguities, making it inherently challenging for models to accurately capture its richness and diversity. Additionally, language models must contend with issues such as out-of-vocabulary words, long-range dependencies, and domain-specific knowledge, requiring robust architectures and sophisticated algorithms to address these challenges effectively. Despite these challenges, language modeling continues to be a rapidly evolving field of research, with lots of advancements in model architectures, training techniques, and evaluation methodologies. Recent breakthroughs in deep learning, particularly with transformer-based architectures like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), have significantly pushed the boundaries of language understanding and generation, achieving remarkable performance across a wide range of NLP tasks.\n\n\nFine-tuning models\nIn most applications, a model which has been trained to do language modeling is not the end of the story. Instead, it is often referred to as a “pre-trained” model that can now be fine-tuned to a specific task. Fine-tuning in general is a powerful technique in machine learning that allows practitioners to adapt pre-trained language models to specific tasks. At its core, it aims to leverage the knowledge and representations learned by a pre-trained LLM on a large corpus of text data, and tries to further train it on task-specific labeled data to tailor its capabilities to a particular task. The concept draws inspiration from transfer learning and enables practitioners to capitalize on the wealth of linguistic knowledge encoded within pre-trained models and adapt it to new tasks without the need for extensive training from scratch.\nThe fine-tuning process usually begins in a similar fashion as training a model from scratch. We select a specific downstream task that the model is intended to perform, such as text classification, sentiment analysis or language generation. The next step is to gather task-specific labeled data that is relevant to the chosen task. To recall a known example, if the objective is sentiment analysis, a dataset comprising text samples labeled with sentiment categories (positive, negative, neutral) would be required. With the task-specific data in hand, the fine-tuning process unfolds as follows: first, the pre-trained LLM is initialized with its learned parameters from pre-training, which serves as the starting point for further training. Then, the model is trained on the task-specific data using supervised learning techniques, where the objective is to minimize a task-specific loss function. This process involves adjusting the parameters of the model to better capture the patterns and relationships present in the task-specific data. Fine-tuning a model often involves replacing the output layer and/or freezing or adding hidden layers of the model.\nIn this seminar, we won’t go into the details of fine-tuning, however, it is an important concept that makes the current quality of large language models possible.\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Introduction to LLM"
    ]
  },
  {
    "objectID": "llm/exercises/ex_gpt_start.html",
    "href": "llm/exercises/ex_gpt_start.html",
    "title": "Exercise: OpenAI - Getting started",
    "section": "",
    "text": "Task: Explore the OpenAI chat.completions API.\nInstructions:\n\nGenerate a chat completion and analyze the response object ChatCompletion. What information do you get with each completion?\nHow can you access the actual completion of your prompt?\nUse the OpenAI API documentation to find out what choices are and how they are used.\nPlay around with the parameters temperature and top_p for a simple prompt. What do you notice?\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Exercise: OpenAI - Getting started"
    ]
  },
  {
    "objectID": "llm/exercises/ex_gpt_ner_with_function_calls.html",
    "href": "llm/exercises/ex_gpt_ner_with_function_calls.html",
    "title": "Exercise: NER with tool calling",
    "section": "",
    "text": "Task: Create a small script that uses tool (or function calling) to extract the following named entities from a given text: City, State, Person.\nInstructions:\n\nDefine an OpenAI tool with a function named_entity_recognition.\nChoose an appropriate output format, for example: {\"named_entities\": [{\"entity\": \"Mike\", \"label\": \"Person}, {\"entity\": \"Münster\", \"label\": \"City\"}]}\nDefine a matching prompt in the role system and the text input for the role user.\nExtract the result.\n\n\n# prerequisites\nimport os\nfrom llm_utils.client import get_openai_client\n\nMODEL = \"gpt4\"\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n# here goes your code\n\n\n\nShow solution\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"named_entity_recognition\",\n            \"description\": \"Extract the named entities from the given text.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"named_entities\": {\n                        \"type\": \"array\",\n                        \"description\": \"A list of all extracted named entities in form of dictionaries containing the entity name and the label\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"entity\": {\"type\": \"string\"}, \n                                \"label\": {\"type\": \"string\"}\n                            },\n                            \"required\": [\"entity\", \"label\"]\n                        }\n                    },\n                },\n                \"required\": [\"named_entities\"],\n            },\n        }\n    }\n]\n\n\n# define the prompts\nmessages = []\nmessages.append({\"role\": \"system\", \"content\": \"Extract all named entities from the provided text. Possible labels are 'City', 'State' or 'Person'. If no named entities are contained in the text, do not make assumptions and return nothing.\"})\nmessages.append({\"role\": \"user\", \"content\": \"Leonard Hoffstaedter lives in Pasadena, CA.\"})\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=messages,\n    tools=tools,\n    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"named_entity_recognition\"}}\n)\nresponse\n\n\nChatCompletion(id='chatcmpl-99ALw7LjaBzZ63s5CMt9wDGn3aWhM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_1aw75NLIUiEpdYMztdXRDZEh', function=Function(arguments='{\\n\"named_entities\": [\\n  {\\n    \"entity\": \"Leonard Hoffstaedter\",\\n    \"label\": \"Person\"\\n  },\\n  {\\n    \"entity\": \"Pasadena\",\\n    \"label\": \"City\"\\n  },\\n  {\\n    \"entity\": \"CA\",\\n    \"label\": \"State\"\\n  }\\n]\\n}', name='named_entity_recognition'), type='function')]), content_filter_results={})], created=1711971776, model='gpt-4', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=68, prompt_tokens=142, total_tokens=210), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n\n\n\n# retrieve the result\nimport json \n\nresult = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\nfor named_entity in result[\"named_entities\"]: \n    print(f\"{named_entity['entity']}: {named_entity['label']}\")\n\nLeonard Hoffstaedter: Person\nPasadena: City\nCA: State\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Exercise: NER with tool calling"
    ]
  },
  {
    "objectID": "llm/prompting.html",
    "href": "llm/prompting.html",
    "title": "Prompting",
    "section": "",
    "text": "Learning prompting is a science for itself. The difficulty lies in the probabilistic nature of the language models. That means, small changes to your prompt (that you might even find insignificant) can have a large impact on the result/the answer. In particular, the changes do not have to be “logical”, i.e., depend on your changes in a comprehensible or reproducible way. This can sometimes be frustrating, but can also be avoided in many cases when following the right instructions for prompting. To do so, let’s best follow the creators.\n\n\n\n\n\n\nNote\n\n\n\nThe following is taken from the OpenAI Guide\n\n\n\nWrite clear instructions\nThese models can’t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you’d like to see. The less the model has to guess at what you want, the more likely you’ll get it.\nTactics:\n\nInclude details in your query to get more relevant answers\nAsk the model to adopt a persona\nUse delimiters to clearly indicate distinct parts of the input\nSpecify the steps required to complete a task\nProvide examples\nSpecify the desired length of the output \n\n\n\nProvide reference text\nLanguage models can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to these models can help in answering with fewer fabrications.\nTactics:\n\nInstruct the model to answer using a reference text\nInstruct the model to answer with citations from a reference text \n\n\n\nSplit complex tasks into simpler subtasks\nJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks - submitted to a language model. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.\nTactics:\n\nUse intent classification to identify the most relevant instructions for a user query\nFor dialogue applications that require very long conversations, summarize or filter previous dialogue\nSummarize long documents piecewise and construct a full summary recursively \n\n\n\nGive the model time to “think”\nIf asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, models make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a “chain of thought” before an answer can help the model reason its way toward correct answers more reliably.\nTactics:\n\nInstruct the model to work out its own solution before rushing to a conclusion\nUse inner monologue or a sequence of queries to hide the model’s reasoning process\nAsk the model if it missed anything on previous passes \n\n\n\nUse external tools\nCompensate for the weaknesses of the model by feeding it the outputs of other tools. For example, a text retrieval system (sometimes called RAG or retrieval augmented generation) can tell the model about relevant documents. A code execution engine like OpenAI’s Code Interpreter can help the model do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a language model, offload it to get the best of both.\nTactics:\n\nUse embeddings-based search to implement efficient knowledge retrieval\nUse code execution to perform more accurate calculations or call external APIs\nGive the model access to specific functions \n\n\n\nTest changes systematically\nImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an “eval”).\nTactic:\n\nEvaluate model outputs with reference to gold-standard answers\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Prompting"
    ]
  },
  {
    "objectID": "embeddings/visualization.html",
    "href": "embeddings/visualization.html",
    "title": "Visualization & clustering of embeddings",
    "section": "",
    "text": "Let’s go a little bit further down the road of similarities between words or texts in general. Wouldn’t it be great if we could somehow visualize a text as a point in space and see how other texts relate to it? Embeddings allow us to do that as well, with one caveat: Embeddings have too many dimensions to visualize (usually a few thousand). Luckily, there are tools such as principal component analysis or T-SNE available to reduce the dimension of vectors (for example, to two dimensions), while preserving most of the relations between them. Let’s see how this works.\n\n# prerequisites\n\nimport os\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n\n# Define a list of words to visualize\nwords = [\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"banana\", \"grapes\", \"cat\", \"dog\", \"happy\", \"sad\"]\n\n# Get embeddings for the words\nresponse = client.embeddings.create(\n    input=words,\n    model=MODEL\n)\n\nembeddings = [emb.embedding for emb in response.data]\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n    n_components=2, \n    random_state=42,\n    perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    plt.scatter(x, y, marker='o', color='red')\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\n\n\nt-SNE visualization of word embeddings",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Visualization & clustering of embeddings"
    ]
  },
  {
    "objectID": "embeddings/visualization.html#clustering-of-embeddings",
    "href": "embeddings/visualization.html#clustering-of-embeddings",
    "title": "Visualization & clustering of embeddings",
    "section": "Clustering of embeddings",
    "text": "Clustering of embeddings\nThe great thing is, we can already see that there are some clusters forming. We can again use models like KMeans to find them explicitly. In this case, we obviously have five clusters, so let’s try to identify them.\n\n# do the clustering\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nn_clusters = 5\n\n# define the model\nkmeans = KMeans(\n  n_clusters=n_clusters,\n  n_init=\"auto\",\n  random_state=2 # do this to get the same output\n)\n\n# fit the model to the data\nkmeans.fit(np.array(embeddings))\n\n# get the cluster labels\ncluster_labels = kmeans.labels_\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n  n_components=2, \n  random_state=42,\n  perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Define a color map for clusters\ncolors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    cluster_label = cluster_labels[i]\n    color = colors[cluster_label]\n    plt.scatter(x, y, marker='o', color=color)\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\n\n\nt-SNE visualization of word embedding clusters\n\n\n\n\nThat, again, is great news. Embeddings allow us to find clusters in texts, based on the semantics included. This helps in many applications where documents need to be analyzed without having been seen by a human. Maybe you can use it in your project?",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Visualization & clustering of embeddings"
    ]
  },
  {
    "objectID": "python_intro/classes.html",
    "href": "python_intro/classes.html",
    "title": "Classes",
    "section": "",
    "text": "Classes are a fundamental part of object-oriented programming (OOP) in Python. They allow you to create your own data types that combine data (attributes) and functionality (methods) into a single structure. By using classes, you can model real-world entities, promote code reusability, and make your programs more organized and modular.\nIn Python, a class serves as a blueprint for creating objects, encapsulating attributes and behaviors that are common to all instances of that class. Let’s dive into the concepts of classes, objects, and how to use them in Python.\n\nDefining a Class\nYou define a class using the class keyword followed by the class name. By convention, class names are written in CamelCase.\n\nBasic Syntax:\n\nclass ClassName:\n    # Class attributes and methods go here\n    pass\n\n\n\nExample:\n\nclass Text:\n    pass\n\n# Creating an instance of the Text class\nmy_text = Text()\nprint(my_text)  # Output: &lt;__main__.Text object at 0x...&gt;\n\n&lt;__main__.Text object at 0x137e24dd0&gt;\n\n\nIn this example, we defined a simple class named Text and created an instance of it, which is stored in the variable my_text.\n\n\n\nAdding Attributes and Methods\nAttributes are variables that belong to the class, while methods are functions defined within the class that can manipulate those attributes or perform actions.\nTo define attributes, you typically do this inside a special method called __init__(), which initializes the object’s attributes when it is created.\n\nExample:\n\nclass Text:\n    def __init__(self, content):\n        self.content = content  # Instance attribute\n\n    def word_count(self):  # Method\n        return len(self.content.split())\n\n    def shout(self):  # Another method\n        result = self.content.upper()\n        result = result.replace(\".\", \"!\")\n        return result\n\n# Creating an instance of Text\nmy_text = Text(\"Hello, World! This is a test.\")\nprint(my_text.word_count())  # Output: 6\nprint(my_text.shout())  # Output: HELLO, WORLD! THIS IS A TEST!\n\n6\nHELLO, WORLD! THIS IS A TEST!\n\n\nIn this example, the Text class has an initializer method __init__() that takes content as a parameter and assigns it to an instance attribute. The word_count() method allows you to get the number of words in the text, the shout() method transforms all letters to upper case and replaces dots by an exclamation mark.\n\n\n\nAccessing Attributes and Methods\nYou can access attributes and methods of a class instance using the dot . notation.\n\nExample:\n\nprint(my_text.content)  # Output: Hello, World! This is a test.\n\nHello, World! This is a test.\n\n\nIn this example, we access the content attribute of my_text using dot notation.\n\n\n\nClass vs. Instance Attributes\nAttributes defined inside the __init__() method are called instance attributes because they belong to a specific instance of the class. In contrast, class attributes are shared by all instances of the class and are defined directly within the class body.\n\nExample:\n\nclass Text:\n    language = \"English\"  # Class attribute\n\n    def __init__(self, content):\n        self.content = content  # Instance attribute\n\n# Creating instances\ntext1 = Text(\"Hello, World!\")\ntext2 = Text(\"Bonjour, le monde!\")\n\nprint(text1.language)  # Output: English\nprint(text2.language)  # Output: English\n\n# Changing class attribute\nText.language = \"French\"\nprint(text1.language)  # Output: French\n\nEnglish\nEnglish\nFrench\n\n\nIn this example, we defined a class attribute language that is shared by all instances of the Text class. Changing the class attribute affects all instances.\n\n\n\nInheritance\nInheritance allows you to create a new class that inherits attributes and methods from an existing class. This promotes code reuse and makes it easier to create a hierarchy of classes.\n\nExample:\n\nclass Text:\n    def __init__(self, content):\n        self.content = content\n\n    def word_count(self):\n        return len(self.content.split())\n\nclass FormattedText(Text):  # Derived class\n    def __init__(self, content, format_type):\n        super().__init__(content)  # Call the parent class's constructor\n        self.format_type = format_type\n\n    def display(self):\n        return f\"[{self.format_type}] {self.content}\"\n\n# Creating instances\nmy_text = Text(\"Hello, World!\")\nformatted_text = FormattedText(\"Hello, World!\", \"Bold\")\n\nprint(my_text.word_count())  # Output: 2\nprint(formatted_text.display())  # Output: [Bold] Hello, World!\n\n2\n[Bold] Hello, World!\n\n\nIn this example, we have a base class Text and a derived class FormattedText. The derived class can extend the functionality of the base class.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Classes"
    ]
  },
  {
    "objectID": "python_intro/conditional_statements.html",
    "href": "python_intro/conditional_statements.html",
    "title": "Conditional Statements",
    "section": "",
    "text": "Conditional statements allow you to control the flow of your program based on different conditions. In Python, the most common conditional statement is the if statement, which lets you execute code only when a certain condition is true. This is essential for decision-making in your programs. For example, when working with NLP tasks, you might want to process text differently based on certain conditions, like whether a word is in uppercase, whether a sentence contains a keyword, or whether the length of a string exceeds a certain threshold.\nLet’s explore how conditional statements work in Python.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Conditional Statements"
    ]
  },
  {
    "objectID": "python_intro/conditional_statements.html#the-if-statement",
    "href": "python_intro/conditional_statements.html#the-if-statement",
    "title": "Conditional Statements",
    "section": "The if Statement",
    "text": "The if Statement\nThe if statement checks a condition (usually a comparison between values) and executes a block of code if the condition evaluates to True. If the condition is False, the code inside the if block is skipped.\n\nBasic Syntax:\n\nif condition:\n    # Code to be executed if the condition is True\n    pass\n\n\n\nExample:\n\nx = 10\nif x &gt; 5:\n    print(\"x is greater than 5\")  # Output: x is greater than 5\n\nx is greater than 5\n\n\nIn this example, the condition x &gt; 5 is checked. Since x is 10, which is greater than 5, the print statement is executed.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Conditional Statements"
    ]
  },
  {
    "objectID": "python_intro/conditional_statements.html#the-else-statement",
    "href": "python_intro/conditional_statements.html#the-else-statement",
    "title": "Conditional Statements",
    "section": "The else Statement",
    "text": "The else Statement\nYou can use the else statement to define what happens if the condition in the if statement is False. The else block will run when all previous conditions are false.\n\nExample:\n\nx = 3\nif x &gt; 5:\n    print(\"x is greater than 5\")\nelse:\n    print(\"x is not greater than 5\")  # Output: x is not greater than 5\n\nx is not greater than 5\n\n\nIn this example, since x is 3 (which is not greater than 5), the else block is executed, and the message “x is not greater than 5” is printed.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Conditional Statements"
    ]
  },
  {
    "objectID": "python_intro/conditional_statements.html#the-elif-statement",
    "href": "python_intro/conditional_statements.html#the-elif-statement",
    "title": "Conditional Statements",
    "section": "The elif Statement",
    "text": "The elif Statement\nIf you want to check multiple conditions, you can use elif (short for “else if”). The elif statement allows you to test additional conditions if the previous if condition was False.\n\nExample:\n\nx = 7\nif x &gt; 10:\n    print(\"x is greater than 10\")\nelif x &gt; 5:\n    print(\"x is greater than 5 but less than or equal to 10\")  # Output: x is greater than 5 but less than or equal to 10\nelse:\n    print(\"x is less than or equal to 5\")\n\nx is greater than 5 but less than or equal to 10\n\n\nIn this example, Python first checks if x &gt; 10. Since that condition is false, it moves to the elif condition (x &gt; 5), which is true, so the second block is executed.\n\n\nUsing Comparison Operators\nIn conditional statements, we typically use comparison operators to evaluate the relationship between values. Here are some common comparison operators:\n\n==: Equal to\n!=: Not equal to\n&gt;: Greater than\n&lt;: Less than\n&gt;=: Greater than or equal to\n&lt;=: Less than or equal to\n\n\nExample:\n\nage = 18\n\nif age == 18:\n    print(\"You are 18 years old\")  # Output: You are 18 years old\nif age != 20:\n    print(\"You are not 20 years old\")  # Output: You are not 20 years old\n\nYou are 18 years old\nYou are not 20 years old\n\n\nIn this example, we use the == operator to check if age is exactly 18 and the != operator to check if age is not 20.\n\n\n\nLogical Operators\nPython also provides logical operators to combine multiple conditions:\n\nand: Returns True if both conditions are true.\nor: Returns True if at least one condition is true.\nnot: Inverts the truth value of the condition.\n\n\nExample:\n\nx = 7\n\n# Using 'and' operator\nif x &gt; 5 and x &lt; 10:\n    print(\"x is between 5 and 10\")  # Output: x is between 5 and 10\n\n# Using 'or' operator\nif x &lt; 5 or x &gt; 6:\n    print(\"x is either less than 5 or greater than 6\")  # Output: x is either less than 5 or greater than 6\n\n# Using 'not' operator\nif not x == 5:\n    print(\"x is not equal to 5\")  # Output: x is not equal to 5\n\nx is between 5 and 10\nx is either less than 5 or greater than 6\nx is not equal to 5\n\n\nIn this example, the and operator checks if both conditions (x &gt; 5 and x &lt; 10) are true, the or operator checks if at least one condition is true, and the not operator inverts the condition, making x == 5 false.\n\n\n\nNested if Statements\nYou can place an if statement inside another if statement. This is called nesting and is useful when you need to check multiple conditions that depend on each other.\n\nExample:\n\nx = 10\ny = 20\n\nif x &gt; 5:\n    if y &gt; 15:\n        print(\"x is greater than 5 and y is greater than 15\")  # Output: x is greater than 5 and y is greater than 15\n\nx is greater than 5 and y is greater than 15\n\n\nHere, we first check if x &gt; 5. Since this is true, we then check if y &gt; 15. Since both conditions are true, the inner block is executed.\n\n\n\nChecking Membership with in Operator\nThe in operator is used to check if a value is present in a collection like a list, string, or tuple. This is particularly useful in NLP tasks when checking if a word or character exists in a string or list.\n\nExample:\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\nif \"banana\" in fruits:\n    print(\"Banana is in the list!\")  # Output: Banana is in the list!\n\n# Checking a string\nsentence = \"Hello world\"\nif \"world\" in sentence:\n    print(\"The word 'world' is in the sentence.\")  # Output: The word 'world' is in the sentence.\n\nBanana is in the list!\nThe word 'world' is in the sentence.\n\n\nIn this example, we use the in operator to check whether \"banana\" is present in the fruits list and if the word \"world\" is in the sentence.\n\n\n\nCombining Conditions in Practical Examples\nLet’s take an example where we check whether a user’s input meets certain conditions, like a password validation function.\n\nExample:\n\npassword = \"Hello123\"\n\nif len(password) &gt;= 8 and any(char.isdigit() for char in password):\n    print(\"Password is valid\")\nelse:\n    print(\"Password must be at least 8 characters long and contain a number.\")\n# Output: Password is valid\n\nPassword is valid\n\n\nHere, we check two conditions for the password: it should be at least 8 characters long and must contain a number. The function char.isdigit() checks if there’s any numeric character in the password.\n\n\n\nEdge Cases and Handling Errors\nWhen using conditional statements, it’s important to think about edge cases, i.e., situations where input values might behave differently or cause errors. For example, when dealing with empty strings, null values, or division by zero, you should handle these cases appropriately using conditionals.\n\nExample:\n\n# Checking for division by zero\nnumerator = 10\ndenominator = 0\n\nif denominator != 0:\n    result = numerator / denominator\n    print(result)\nelse:\n    print(\"Cannot divide by zero\")  # Output: Cannot divide by zero\n\nCannot divide by zero\n\n\nIn this example, we use a conditional statement to prevent a division by zero error by checking if the denominator is not zero before performing the division.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Conditional Statements"
    ]
  },
  {
    "objectID": "python_intro/functions.html",
    "href": "python_intro/functions.html",
    "title": "Functions",
    "section": "",
    "text": "Functions are one of the most important building blocks in Python. They allow you to organize your code into reusable chunks. A function is a block of code that performs a specific task, and you can “call” the function whenever you need that task to be done. Using functions helps make your code more modular, readable, and easier to maintain. As we start dealing with more complex tasks, especially in NLP, functions will help simplify and structure your code.\n\nDefining Functions\nYou can define a function using the def keyword, followed by the function name, parentheses (), and a colon :. Inside the function, you write the code that you want the function to execute. Optionally, you can pass arguments to the function to customize its behavior, and the function can return a value after completing its task.\n\nBasic Syntax:\n\ndef function_name():\n    # Code to be executed\n    pass\n\n\n\n\nExample: A Simple Function\nLet’s start with a basic example of a function that prints a greeting message.\n\nExample:\n\ndef greet():\n    print(\"Hello, welcome to Python!\")\n\n# Calling the function\ngreet()  # Output: Hello, welcome to Python!\n\nHello, welcome to Python!\n\n\nHere, we define a function called greet() that prints a message. Every time we call the greet() function, the message is printed.\n\n\n\nFunctions with Arguments\nFunctions become more powerful when you can pass data to them via arguments (also called parameters). You can pass one or more arguments into a function, and the function can use those arguments to perform different actions.\n\nExample:\n\ndef greet(name):\n    print(f\"Hello, {name}!\")\n\n# Calling the function with an argument\ngreet(\"Alice\")  # Output: Hello, Alice!\ngreet(\"Bob\")  # Output: Hello, Bob!\n\nHello, Alice!\nHello, Bob!\n\n\nIn this example, the greet(name) function takes one argument, name, and includes it in the greeting message. When we call the function with \"Alice\" and \"Bob\", it customizes the greeting for each person.\n\n\n\nFunctions with Multiple Arguments\nYou can define functions with multiple arguments by separating them with commas. The function will expect all the arguments when called.\n\nExample:\n\ndef add_numbers(a, b):\n    result = a + b\n    print(f\"The sum is: {result}\")\n\n# Calling the function with two arguments\nadd_numbers(3, 5)  # Output: The sum is: 8\n\nThe sum is: 8\n\n\nHere, the add_numbers(a, b) function takes two arguments, a and b, adds them together, and prints the result.\n\n\n\nReturning Values from Functions\nSometimes, you’ll want your function to return a value instead of just printing something. You can do this using the return keyword. A function can return a value that you can store in a variable or use in further calculations.\n\nExample:\n\ndef add_numbers(a, b):\n    return a + b\n\n# Storing the returned value in a variable\nsum_result = add_numbers(10, 20)\nprint(sum_result)  # Output: 30\n\n30\n\n\nIn this example, the function add_numbers(a, b) returns the sum of a and b, and we store the result in the sum_result variable.\n\n\n\nDefault Arguments\nYou can assign default values to arguments in your function. If the caller doesn’t provide a value for that argument, the default value will be used. This makes your functions more flexible.\n\nExample:\n\ndef greet(name=\"there\"):\n    print(f\"Hello, {name}!\")\n\n# Calling the function without passing an argument\ngreet()  # Output: Hello, there!\n\n# Calling the function with an argument\ngreet(\"Alice\")  # Output: Hello, Alice!\n\nHello, there!\nHello, Alice!\n\n\nIn this example, the greet(name=\"there\") function has a default value of \"there\" for the name argument. If no name is provided, the default message is used.\n\n\n\nFunctions with Keyword Arguments\nYou can also call a function using keyword arguments. This allows you to specify the arguments by name when calling the function, which can make your code more readable, especially when you have many arguments.\n\nExample:\n\ndef describe_person(name, age, city):\n    print(f\"{name} is {age} years old and lives in {city}.\")\n\n# Calling the function using keyword arguments\ndescribe_person(name=\"Alice\", age=30, city=\"New York\")\n# Output: Alice is 30 years old and lives in New York.\n\nAlice is 30 years old and lives in New York.\n\n\nIn this case, keyword arguments make it clear which value corresponds to which parameter.\n\n\n\nScope of Variables\nVariables defined inside a function have local scope, meaning they only exist inside that function. Once the function is finished, the variables are destroyed. However, you can define global variables outside the function if you need them to be accessible throughout your entire program.\n\nExample of Local Scope:\n\ndef my_function():\n    x = 10  # This variable only exists inside the function\n    print(x)\n\nmy_function()  # Output: 10\n# print(x)  # This would cause an error because x is not defined outside the function\n\n10\n\n\n\n\nExample of Global Scope:\n\nx = 5  # Global variable\n\ndef my_function():\n    print(x)  # Accessing the global variable inside the function\n\nmy_function()  # Output: 5\n\n5\n\n\n\n\n\nLambda Functions\nIn addition to regular functions, Python supports lambda functions, which are small anonymous functions that can be written in a single line. Lambda functions are useful for simple operations and are often used in combination with other functions.\n\nExample:\n\n# Regular function to square a number\ndef square(x):\n    return x ** 2\n\n# Lambda function to square a number\nsquare_lambda = lambda x: x ** 2\n\n# Using both functions\nprint(square(5))  # Output: 25\nprint(square_lambda(5))  # Output: 25\n\n25\n25\n\n\nIn this example, both the regular square() function and the lambda function square_lambda return the square of a number.\n\n\n\nFunctions and Lists\nFunctions can be combined with lists to perform operations on groups of data. For instance, you can use a function to process each item in a list using a loop.\n\nExample:\n\ndef square(x):\n    return x ** 2\n\nnumbers = [1, 2, 3, 4, 5]\nsquares = []\n\nfor number in numbers:\n    squares.append(square(number))\n\nprint(squares)  # Output: [1, 4, 9, 16, 25]\n\n[1, 4, 9, 16, 25]\n\n\nIn this example, we define a function square() that squares a number. We then use a loop to apply this function to each item in the numbers list and store the results in the squares list.\n\n\n\nCommon Practices with Functions\nHere are some best practices for working with functions:\n\nUse descriptive names for your functions and arguments so their purpose is clear.\n\nGood: def calculate_area(radius):\nBad: def ca(r):\n\nKeep functions short and focused on a single task. If a function becomes too long, consider breaking it into smaller functions.\nUse comments and doc strings to describe what your function does, especially if it’s not immediately obvious.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Functions"
    ]
  },
  {
    "objectID": "python_intro/dictionaries.html",
    "href": "python_intro/dictionaries.html",
    "title": "Dictionaries",
    "section": "",
    "text": "Dictionaries in Python are a powerful data structure that allows you to store and manage data in key-value pairs. They are highly versatile and widely used, especially in scenarios where you need to associate specific values with unique identifiers, such as words with their definitions or user IDs with user data.\nDictionaries are unordered, which means that the items do not have a specific order, and they are mutable, allowing you to modify them after creation. This makes them an excellent choice for situations where you need fast lookups, insertions, and deletions based on keys.\nLet’s explore how dictionaries work in Python!\n\n1.6.1 Creating a Dictionary\nYou can create a dictionary using curly braces {} with key-value pairs separated by colons. The key and value can be of any data type, including strings, integers, and even other dictionaries.\n\nBasic Syntax:\nmy_dict = {\n    \"key1\": \"value1\",\n    \"key2\": \"value2\",\n    \"key3\": \"value3\"\n}\n\n\n\nExample:\n# Creating a simple dictionary\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\n\nprint(person)  # Output: {'name': 'Alice', 'age': 30, 'city': 'New York'}\nIn this example, we created a dictionary called person with keys like \"name\", \"age\", and \"city\" associated with their respective values.\n\n\n1.6.2 Accessing Values in a Dictionary\nYou can access values in a dictionary by using their corresponding keys inside square brackets []. If the key does not exist, it raises a KeyError.\n\nExample:\n# Accessing values\nprint(person[\"name\"])  # Output: Alice\nprint(person[\"age\"])   # Output: 30\nHere, we accessed the values associated with the keys \"name\" and \"age\".\n\n\n\n1.6.3 Modifying a Dictionary\nDictionaries are mutable, meaning you can add, modify, or remove key-value pairs even after the dictionary has been created.\n\nAdding a Key-Value Pair:\n# Adding a new key-value pair\nperson[\"occupation\"] = \"Engineer\"\nprint(person)  # Output: {'name': 'Alice', 'age': 30, 'city': 'New York', 'occupation': 'Engineer'}\n\n\nModifying a Value:\n# Modifying an existing value\nperson[\"age\"] = 31\nprint(person)  # Output: {'name': 'Alice', 'age': 31, 'city': 'New York', 'occupation': 'Engineer'}\n\n\nRemoving a Key-Value Pair:\n# Removing a key-value pair\ndel person[\"city\"]\nprint(person)  # Output: {'name': 'Alice', 'age': 31, 'occupation': 'Engineer'}\nIn these examples, we added a new key-value pair for \"occupation\", modified the value associated with \"age\", and removed the \"city\" key.\n\n\n\n1.6.4 Dictionary Methods\nPython dictionaries come with various built-in methods that make it easier to manipulate and interact with the data. Here are some common dictionary methods:\n\n1.6.4.1 get() Method\nThe get() method returns the value for a specified key. If the key doesn’t exist, it returns None (or a default value if provided) instead of raising a KeyError.\n\n\nExample:\nage = person.get(\"age\", \"Not Found\")\nprint(age)  # Output: 31\n\n# Trying to access a non-existent key\nlocation = person.get(\"city\", \"Not Found\")\nprint(location)  # Output: Not Found\nIn this example, we used get() to safely access the value for \"age\" and provided a default value for a non-existent key.\n\n\n1.6.4.2 keys() Method\nThe keys() method returns a view object that displays a list of all the keys in the dictionary.\n\n\nExample:\nkeys = person.keys()\nprint(keys)  # Output: dict_keys(['name', 'age', 'occupation'])\n\n\n1.6.4.3 values() Method\nThe values() method returns a view object that displays a list of all the values in the dictionary.\n\n\nExample:\nvalues = person.values()\nprint(values)  # Output: dict_values(['Alice', 31, 'Engineer'])\n\n\n1.6.4.4 items() Method\nThe items() method returns a view object that displays a list of all the key-value pairs in the dictionary as tuples.\n\n\nExample:\nitems = person.items()\nprint(items)  # Output: dict_items([('name', 'Alice'), ('age', 31), ('occupation', 'Engineer')])\nThese methods are helpful when you want to loop through the keys, values, or key-value pairs of a dictionary.\n\n\n\n1.6.5 Looping Through a Dictionary\nYou can loop through a dictionary using a for loop. By default, it iterates over the keys. If you need to access both keys and values, you can use the items() method.\n\nExample:\n# Looping through keys\nfor key in person:\n    print(key)  # Output: name, age, occupation\n\n# Looping through keys and values\nfor key, value in person.items():\n    print(f\"{key}: {value}\")\n# Output:\n# name: Alice\n# age: 31\n# occupation: Engineer\nIn this example, we demonstrate how to loop through the keys and both keys and values of the person dictionary.\n\n\n\n1.6.6 Nesting Dictionaries\nDictionaries can also contain other dictionaries, allowing you to create complex data structures.\n\nExample:\npeople = {\n    \"Alice\": {\n        \"age\": 30,\n        \"city\": \"New York\"\n    },\n    \"Bob\": {\n        \"age\": 25,\n        \"city\": \"Los Angeles\"\n    }\n}\n\nprint(people[\"Alice\"][\"city\"])  # Output: New York\nIn this example, we created a dictionary called people that contains two dictionaries for \"Alice\" and \"Bob\", each with their own attributes.\n\n\n\n1.6.7 Dictionary Comprehensions\nPython supports a concise way to create dictionaries known as dictionary comprehensions. This is a compact way to transform data or filter items.\n\nSyntax:\n{key_expression: value_expression for item in iterable if condition}\n\n\nExample:\n# Creating a dictionary using comprehension\nsquares = {x: x ** 2 for x in range(5)}\nprint(squares)  # Output: {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\nIn this example, we created a dictionary of squares for numbers 0 through 4 using a dictionary comprehension.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Dictionaries"
    ]
  },
  {
    "objectID": "python_intro/exercises/functions.html",
    "href": "python_intro/exercises/functions.html",
    "title": "Exercise: Functions",
    "section": "",
    "text": "Task: Write a function called count_words(text) that takes a string of text as input and returns the number of words in it.\nInstructions:\n\nDefine a function count_words(text).\nSplit the text into words using split().\nReturn the number of words.\nTest the function with different sentences.\n\n\n\nShow solution\n\n\ndef count_words(text):\n    words = text.split()\n    return len(words)\n\nsentence = \"Natural language processing is fun!\"\nprint(count_words(sentence))  # Output: 5\n\n5",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Functions"
    ]
  },
  {
    "objectID": "python_intro/exercises/functions.html#exercise-1-function-to-count-words",
    "href": "python_intro/exercises/functions.html#exercise-1-function-to-count-words",
    "title": "Exercise: Functions",
    "section": "",
    "text": "Task: Write a function called count_words(text) that takes a string of text as input and returns the number of words in it.\nInstructions:\n\nDefine a function count_words(text).\nSplit the text into words using split().\nReturn the number of words.\nTest the function with different sentences.\n\n\n\nShow solution\n\n\ndef count_words(text):\n    words = text.split()\n    return len(words)\n\nsentence = \"Natural language processing is fun!\"\nprint(count_words(sentence))  # Output: 5\n\n5",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Functions"
    ]
  },
  {
    "objectID": "python_intro/exercises/functions.html#exercise-2-function-to-convert-text-to-lowercase",
    "href": "python_intro/exercises/functions.html#exercise-2-function-to-convert-text-to-lowercase",
    "title": "Exercise: Functions",
    "section": "Exercise 2: Function to Convert Text to Lowercase",
    "text": "Exercise 2: Function to Convert Text to Lowercase\nTask: Write a function called convert_to_lowercase(text) that takes a string of text and returns it with all characters in lowercase.\nInstructions:\n\nDefine a function convert_to_lowercase(text).\nUse the .lower() method to convert the text to lowercase.\nReturn the modified text.\nTest the function with various sentences.\n\n\n\nShow solution\n\n\ndef convert_to_lowercase(text):\n    return text.lower()\n\ntext = \"NLP Can Be Challenging But REWARDING!\"\nprint(convert_to_lowercase(text))  # Output: \"nlp can be challenging but rewarding!\"\n\nnlp can be challenging but rewarding!",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Functions"
    ]
  },
  {
    "objectID": "python_intro/exercises/functions.html#exercise-3-function-to-replace-a-word",
    "href": "python_intro/exercises/functions.html#exercise-3-function-to-replace-a-word",
    "title": "Exercise: Functions",
    "section": "Exercise 3: Function to Replace a Word",
    "text": "Exercise 3: Function to Replace a Word\nTask: Create a function called replace_word(text, old_word, new_word) that takes a string of text, a word to be replaced, and the new word, then returns the modified text.\nInstructions:\n\nDefine a function replace_word(text, old_word, new_word).\nUse the .replace() method to replace old_word with new_word.\nReturn the modified text.\nTest the function with different sentences.\n\n\n\nShow solution\n\n\ndef replace_word(text, old_word, new_word):\n    return text.replace(old_word, new_word)\n\ntext = \"Python is fun, and learning Python is rewarding.\"\nprint(replace_word(text, \"Python\", \"NLP\"))  # Output: \"NLP is fun, and learning NLP is rewarding.\"\n\nNLP is fun, and learning NLP is rewarding.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Functions"
    ]
  },
  {
    "objectID": "python_intro/exercises/functions.html#exercise-4-word-frequency-counter",
    "href": "python_intro/exercises/functions.html#exercise-4-word-frequency-counter",
    "title": "Exercise: Functions",
    "section": "Exercise 4: Word Frequency Counter",
    "text": "Exercise 4: Word Frequency Counter\nTask: Create a function to count the frequency of a specific word in a given sentence.\nInstructions:\n\nDefine a function called word_frequency(sentence, word) that takes a string parameter sentence and a string parameter word.\nInside the function, use the split() method to break the sentence into words, then count how many times word appears in the list. Hint: Use the list method .count().\nReturn the count.\nCall the function with a sample sentence and word, then print the result.\n\n\n\nShow solution\n\n\ndef word_frequency(sentence, word):\n    words = sentence.split()\n    return words.count(word)\n\n# Sample usage\nsentence = \"NLP is great and NLP is fun\"\nword = \"NLP\"\nresult = word_frequency(sentence, word)\nprint(f\"The word '{word}' appears {result} times.\")  # Output: The word 'NLP' appears 2 times.\n\nThe word 'NLP' appears 2 times.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Functions"
    ]
  },
  {
    "objectID": "python_intro/exercises/functions.html#exercise-5-sentence-reversal",
    "href": "python_intro/exercises/functions.html#exercise-5-sentence-reversal",
    "title": "Exercise: Functions",
    "section": "Exercise 5: Sentence Reversal",
    "text": "Exercise 5: Sentence Reversal\nTask: Write a function that reverses the words in a sentence.\nInstructions:\n\nDefine a function called reverse_sentence(sentence) that takes a string parameter sentence.\nInside the function, split the sentence into words, reverse the order of the words, and then join them back into a single string.\nReturn the reversed sentence.\nCall the function with a sample sentence and print the result.\n\n\n\nShow solution\n\n\ndef reverse_sentence(sentence):\n    words = sentence.split()\n    reversed_words = words[::-1]\n    return ' '.join(reversed_words)\n\n# Sample usage\nsentence = \"Natural Language Processing is fun\"\nresult = reverse_sentence(sentence)\nprint(\"Reversed sentence:\", result)  # Output: Reversed sentence: fun is Processing Language Natural\n\nReversed sentence: fun is Processing Language Natural",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Functions"
    ]
  },
  {
    "objectID": "python_intro/exercises/functions.html#exercise-6-remove-punctuation",
    "href": "python_intro/exercises/functions.html#exercise-6-remove-punctuation",
    "title": "Exercise: Functions",
    "section": "Exercise 6: Remove Punctuation",
    "text": "Exercise 6: Remove Punctuation\nTask: Create a function to remove punctuation from a given string.\nInstructions:\n\nDefine a function called remove_punctuation(text) that takes a string parameter text.\nInside the function, use a loop or list comprehension to create a new string that only includes alphanumeric characters and spaces. Hint: You can the string methods .isalnum() and .ischar().\nReturn the cleaned text.\nCall the function with a sample string (e.g., “NLP is great! Isn’t it?”) and print the result.\n\n\n\nShow solution\n\n\ndef remove_punctuation(text: str):\n    # Use list comprehension to filter out punctuation\n    cleaned_text = ''.join(char for char in text if char.isalnum() or char.isspace())\n    return cleaned_text\n\n# Sample usage\ntext = \"NLP is great! Isn't it?\"\nresult = remove_punctuation(text)\nprint(\"Cleaned text:\", result)  # Output: Cleaned text: NLP is great Isnt it\n\nCleaned text: NLP is great Isnt it",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Functions"
    ]
  },
  {
    "objectID": "python_intro/exercises/functions.html#exercise-7-average-word-length",
    "href": "python_intro/exercises/functions.html#exercise-7-average-word-length",
    "title": "Exercise: Functions",
    "section": "Exercise 7: Average Word Length",
    "text": "Exercise 7: Average Word Length\nTask: Write a function to calculate the average length of words in a sentence.\nInstructions:\n\nDefine a function called average_word_length(sentence) that takes a string parameter sentence.\nInside the function, split the sentence into words, calculate the average length of the words, and return the result.\nCall the function with a sample sentence (e.g., “Natural Language Processing is powerful”) and print the average length.\n\n\n\nShow solution\n\n\ndef average_word_length(sentence):\n    words = sentence.split()\n    if len(words) == 0:\n        return 0\n    total_length = sum(len(word) for word in words)\n    return total_length / len(words)\n\n# Sample usage\nsentence = \"Natural Language Processing is powerful\"\nresult = average_word_length(sentence)\nprint(\"Average word length:\", result)  \n\nAverage word length: 7.0",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Functions"
    ]
  },
  {
    "objectID": "python_intro/exercises/data_types.html",
    "href": "python_intro/exercises/data_types.html",
    "title": "Exercise: Variables and Data Types",
    "section": "",
    "text": "Task: Create a greeting message using a name.\nInstructions:\n\nDefine a variable called name and assign it your name as a string.\nCreate a variable called greeting that combines “Hello,” with the name.\nPrint the greeting message.\n\n\n\nShow solution\n\n\nname = \"Alice\"\n\n# Combine to create a greeting message\ngreeting = \"Hello, \" + name\n\n# Print the greeting\nprint(greeting)  # Output: Hello, Alice\n\nHello, Alice",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Variables and Data Types"
    ]
  },
  {
    "objectID": "python_intro/exercises/data_types.html#exercise-1-greeting-message",
    "href": "python_intro/exercises/data_types.html#exercise-1-greeting-message",
    "title": "Exercise: Variables and Data Types",
    "section": "",
    "text": "Task: Create a greeting message using a name.\nInstructions:\n\nDefine a variable called name and assign it your name as a string.\nCreate a variable called greeting that combines “Hello,” with the name.\nPrint the greeting message.\n\n\n\nShow solution\n\n\nname = \"Alice\"\n\n# Combine to create a greeting message\ngreeting = \"Hello, \" + name\n\n# Print the greeting\nprint(greeting)  # Output: Hello, Alice\n\nHello, Alice",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Variables and Data Types"
    ]
  },
  {
    "objectID": "python_intro/exercises/data_types.html#exercise-2-word-count",
    "href": "python_intro/exercises/data_types.html#exercise-2-word-count",
    "title": "Exercise: Variables and Data Types",
    "section": "Exercise 2: Word Count",
    "text": "Exercise 2: Word Count\nTask: Count the number of words in a sentence.\nInstructions:\n\nDefine a variable called sentence and assign it a string value (e.g., “Natural Language Processing is fun”).\nUse the split() method to create a list of words from the sentence.\nCalculate the number of words and print the result.\n\n\n\nShow solution\n\n\nsentence = \"Natural Language Processing is fun\"\n\n# Split the sentence into words\nwords = sentence.split()\n\n# Count the number of words\nword_count = len(words)\n\n# Print the word count\nprint(\"Number of words:\", word_count)  # Output: Number of words: 5\n\nNumber of words: 5",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Variables and Data Types"
    ]
  },
  {
    "objectID": "python_intro/exercises/data_types.html#exercise-3-create-a-simple-list",
    "href": "python_intro/exercises/data_types.html#exercise-3-create-a-simple-list",
    "title": "Exercise: Variables and Data Types",
    "section": "Exercise 3: Create a Simple List",
    "text": "Exercise 3: Create a Simple List\nTask: Make a list of your favorite fruits.\nInstructions:\n\nDefine a variable called fruits and assign it a list of your three favorite fruits.\nPrint the list.\n\n\n\nShow solution\n\n\n# Create a list of favorite fruits\nfruits = [\"apple\", \"banana\", \"cherry\"]\n\n# Print the list of fruits\nprint(\"My favorite fruits are:\", fruits)  # Output: My favorite fruits are: ['apple', 'banana', 'cherry']\n\nMy favorite fruits are: ['apple', 'banana', 'cherry']",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Variables and Data Types"
    ]
  },
  {
    "objectID": "python_intro/exercises/data_types.html#exercise-4-check-if-word-is-present",
    "href": "python_intro/exercises/data_types.html#exercise-4-check-if-word-is-present",
    "title": "Exercise: Variables and Data Types",
    "section": "Exercise 4: Check if Word is Present",
    "text": "Exercise 4: Check if Word is Present\nTask: Check if a specific word is in a list.\nInstructions:\n\nDefine a list of common NLP words (e.g., nlp_words = [\"token\", \"entity\", \"vector\", \"model\"]).\nDefine a variable called search_word with a value to check (e.g., \"entity\").\nCreate a boolean variable word_found that checks if search_word is in the nlp_words list.\nPrint the result.\n\n\n\nShow solution\n\n\nnlp_words = [\"token\", \"entity\", \"vector\", \"model\"]\nsearch_word = \"entity\"\n\n# Check if the word is in the list\nword_found = search_word in nlp_words\n\n# Print the result\nprint(f\"Is '{search_word}' in the list? {word_found}\")  # Output: Is 'entity' in the list? True\n\nIs 'entity' in the list? True",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: Variables and Data Types"
    ]
  },
  {
    "objectID": "python_intro/exercises/strings.html",
    "href": "python_intro/exercises/strings.html",
    "title": "Exercise: String Operations",
    "section": "",
    "text": "Task: Change a given string to uppercase.\nInstructions:\n\nDefine a variable called text and assign it a string value (e.g., “natural language processing”).\nUse the .upper() method to convert the string to uppercase.\nPrint the result.\n\n\n\nShow solution\n\n\ntext = \"natural language processing\"\n\n# Convert the string to uppercase\nuppercase_text = text.upper()\n\n# Print the result\nprint(uppercase_text)  # Output: NATURAL LANGUAGE PROCESSING\n\nNATURAL LANGUAGE PROCESSING",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: String Operations"
    ]
  },
  {
    "objectID": "python_intro/exercises/strings.html#exercise-1-convert-to-uppercase",
    "href": "python_intro/exercises/strings.html#exercise-1-convert-to-uppercase",
    "title": "Exercise: String Operations",
    "section": "",
    "text": "Task: Change a given string to uppercase.\nInstructions:\n\nDefine a variable called text and assign it a string value (e.g., “natural language processing”).\nUse the .upper() method to convert the string to uppercase.\nPrint the result.\n\n\n\nShow solution\n\n\ntext = \"natural language processing\"\n\n# Convert the string to uppercase\nuppercase_text = text.upper()\n\n# Print the result\nprint(uppercase_text)  # Output: NATURAL LANGUAGE PROCESSING\n\nNATURAL LANGUAGE PROCESSING",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: String Operations"
    ]
  },
  {
    "objectID": "python_intro/exercises/strings.html#exercise-2-count-vowels",
    "href": "python_intro/exercises/strings.html#exercise-2-count-vowels",
    "title": "Exercise: String Operations",
    "section": "Exercise 2: Count Vowels",
    "text": "Exercise 2: Count Vowels\nTask: Count the number of vowels in a given string.\nInstructions:\n\nDefine a variable called sentence and assign it a string value (e.g., “I love Python programming”).\nCreate a variable called vowel_count to keep track of the number of vowels in the sentence.\nLoop through each character in the sentence and check if it is a vowel (a, e, i, o, u).\nPrint the total number of vowels.\n\n\n\nShow solution\n\n\nsentence = \"I love Python programming\"\n\n# Initialize the vowel count\nvowel_count = 0\n\n# Define the set of vowels\nvowels = \"aeiouAEIOU\"\n\n# Loop through each character in the sentence\nfor char in sentence:\n    if char in vowels:\n        vowel_count += 1\n\n# Print the total number of vowels\nprint(\"Number of vowels:\", vowel_count)  # Output: Number of vowels: 8\n\nNumber of vowels: 7",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: String Operations"
    ]
  },
  {
    "objectID": "python_intro/exercises/strings.html#exercise-3-replace-a-word",
    "href": "python_intro/exercises/strings.html#exercise-3-replace-a-word",
    "title": "Exercise: String Operations",
    "section": "Exercise 3: Replace a Word",
    "text": "Exercise 3: Replace a Word\nTask: Replace a word in a string with another word.\nInstructions:\n\nDefine a variable called text and assign it a string value (e.g., “NLP is fun”).\nUse the .replace() method to change the word “fun” to “awesome”.\nPrint the modified string.\n\n\n\nShow solution\n\n\ntext = \"NLP is fun\"\n\n# Replace the word \"fun\" with \"awesome\"\nmodified_text = text.replace(\"fun\", \"awesome\")\n\n# Print the modified string\nprint(modified_text)  # Output: NLP is awesome\n\nNLP is awesome",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: String Operations"
    ]
  },
  {
    "objectID": "python_intro/exercises/strings.html#exercise-4-extract-substring",
    "href": "python_intro/exercises/strings.html#exercise-4-extract-substring",
    "title": "Exercise: String Operations",
    "section": "Exercise 4: Extract Substring",
    "text": "Exercise 4: Extract Substring\nTask: Extract a substring from a given string.\nInstructions:\n\nDefine a variable called text and assign it a string value (e.g., “Natural Language Processing”).\nUse slicing to extract the word “Language” from the string.\nPrint the extracted substring.\n\n\n\nShow solution\n\n\ntext = \"Natural Language Processing\"\n\n# Extract the substring \"Language\" using slicing\nsubstring = text[8:16]  # The indices may vary based on the string\n\n# Print the extracted substring\nprint(substring)  # Output: Language\n\nLanguage",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "Exercise: String Operations"
    ]
  },
  {
    "objectID": "python_intro/string_operations.html",
    "href": "python_intro/string_operations.html",
    "title": "String operations",
    "section": "",
    "text": "In Python, strings are one of the most fundamental data types used to handle and manipulate text. Since natural language processing (NLP) revolves around working with text, understanding how to work with strings is crucial. A string is simply a sequence of characters, like \"Hello\" or \"GPT\". Strings are used in everything from simple messages to complex text manipulation and analysis.\nLet’s dive into how to create, manipulate, and work with strings in Python.",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "String operations"
    ]
  },
  {
    "objectID": "python_intro/string_operations.html#creating-strings",
    "href": "python_intro/string_operations.html#creating-strings",
    "title": "String operations",
    "section": "Creating Strings",
    "text": "Creating Strings\nIn Python, you can create a string by enclosing text in single (') or double (\") quotes. Both work the same, so you can choose the one that suits your style or context. If your string contains one type of quote, you can use the other to avoid issues with escaping characters.\n\nExample:\n\n# Single and double quotes for strings\nmy_string = \"Hello, world!\"  # using double quotes\nanother_string = 'Python is fun!'  # using single quotes\n\nIf you need to include quotes inside the string, you can mix single and double quotes or use escape characters.\n\n\nExample with quotes inside the string:\n\nquote = 'He said, \"Python is amazing!\"'\nprint(quote)  # Output: He said, \"Python is amazing!\"\n\nHe said, \"Python is amazing!\"\n\n\nYou can also turn most objects in Python into a string using str(), which can be useful when dealing, e.g., with IDs that should not be confused with a number:\n\nuser_id = 213 # is represented as an integer\nconverted_to_string = str(user_id)\nprint(type(converted_to_string))\n\n&lt;class 'str'&gt;",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "String operations"
    ]
  },
  {
    "objectID": "python_intro/string_operations.html#string-operations",
    "href": "python_intro/string_operations.html#string-operations",
    "title": "String operations",
    "section": "String Operations",
    "text": "String Operations\nPython provides many powerful operations and methods to manipulate strings. Let’s explore some of the most useful ones.\n\nString Concatenation\nConcatenation means joining two or more strings together. In Python, you can concatenate strings using the + operator.\n\n\nExample:\n\n# Concatenating two strings\ngreeting = \"Hello\"\nname = \"Alice\"\nmessage = greeting + \", \" + name + \"!\"\nprint(message)  # Output: Hello, Alice!\n\nHello, Alice!\n\n\nIn this example, we combine the strings greeting, name, and a punctuation mark to form a single sentence.\n\n\nString Interpolation (f-strings)\nAn alternative, modern way of formatting strings is using f-strings (formatted string literals). This allows you to insert variables into strings easily.\n\n\nExample:\n\nname = \"Alice\"\nage = 25\nprint(f\"My name is {name} and I am {age} years old.\")\n# Output: My name is Alice and I am 25 years old.\n\nMy name is Alice and I am 25 years old.\n\n\nWith f-strings, you can insert the value of a variable directly into the string by placing it inside curly braces {}.\n\n\nChanging Case: .upper(), .lower(), .capitalize()\nYou can change the case of a string using methods like .upper(), .lower(), and .capitalize().\n\n\nExample:\n\ntext = \"Python is Awesome!\"\nprint(text.upper())    # Output: PYTHON IS AWESOME!\nprint(text.lower())    # Output: python is awesome!\nprint(text.capitalize())  # Output: Python is awesome!\n\nPYTHON IS AWESOME!\npython is awesome!\nPython is awesome!\n\n\n\n.upper() converts all characters to uppercase.\n.lower() converts all characters to lowercase.\n.capitalize() capitalizes only the first letter of the string.\n\n\n\nReplacing Parts of a String: .replace()\nThe .replace() method allows you to replace one part of a string with another.\n\n\nExample:\n\ntext = \"I love Java\"\nnew_text = text.replace(\"Java\", \"Python\")\nprint(new_text)  # Output: I love Python\n\nI love Python\n\n\nIn this case, we replaced \"Java\" with \"Python\" in the original string.\n\n\nSplitting Strings: .split()\nThe .split() method is useful when you need to break a string into a list of words or pieces, based on a specified delimiter (such as spaces, commas, etc.). By default, it splits based on spaces.\n\n\nExample:\n\nsentence = \"I am learning Python\"\nwords = sentence.split()  # Split by spaces\nprint(words)  # Output: ['I', 'am', 'learning', 'Python']\n\n['I', 'am', 'learning', 'Python']\n\n\nIn this example, the string \"I am learning Python\" is split into a list of individual words.\n\n\nJoining Strings: .join()\nIf you have a list of words, you can join them back into a single string using the .join() method. This method takes a list of strings and joins them with the specified separator.\n\n\nExample:\n\nwords = ['I', 'am', 'learning', 'Python']\nsentence = \" \".join(words)  # Join with a space\nprint(sentence)  # Output: I am learning Python\n\nI am learning Python\n\n\n\n\nString Indexing and Slicing\nStrings are sequences of characters, so you can access individual characters (or groups of characters) using indexing and slicing. Indexing starts from 0 for the first character and goes up by 1 for each subsequent character.\n\nExample:\n\ntext = \"Python\"\nprint(text[0])  # Output: P (the first character)\nprint(text[1])  # Output: y (the second character)\n\nP\ny\n\n\nYou can also access characters starting from the end of the string using negative indices:\n\nprint(text[-1])  # Output: n (the last character)\nprint(text[-2])  # Output: o (the second-to-last character)\n\nn\no\n\n\nTo extract a portion of the string, use slicing. Slicing allows you to select a range of characters by specifying the start and end positions.\n\n\nExample:\n\ntext = \"Python Programming\"\nprint(text[0:6])  # Output: Python (characters from index 0 to 5)\nprint(text[-11:])  # Output: Programming (last 11 characters)\n\nPython\nProgramming\n\n\n\n\n\nCommon String Functions\n\nLength of a String: len()\n\nYou can find the length of a string (i.e., how many characters it contains) using the len() function.\n\nExample:\n\ntext = \"Python\"\nprint(len(text))  # Output: 6\n\n6\n\n\nFinding Substrings: .find()\n\nThe .find() method helps locate the position of a substring within a string. It returns the index of the first occurrence or -1 if the substring is not found.\n\nExample:\n\ntext = \"Hello, Python!\"\nposition = text.find(\"Python\")\nprint(position)  # Output: 7\n\n7",
    "crumbs": [
      "Seminar",
      "Python Crash Course",
      "String operations"
    ]
  },
  {
    "objectID": "resources/apis.html",
    "href": "resources/apis.html",
    "title": "Language Model APIs",
    "section": "",
    "text": "Google Cloud Natural Language API Google Cloud Natural Language API offers a suite of powerful natural language processing capabilities, including sentiment analysis, entity recognition, and syntax analysis. While it may not provide pre-trained large language models like GPT, it offers robust support for various NLP tasks through its RESTful API.\nMicrosoft Azure Cognitive Services - Text Analytics Azure Cognitive Services offers Text Analytics, a set of APIs for analyzing unstructured text. It provides functionalities such as sentiment analysis, key phrase extraction, language detection, and entity recognition. While it doesn’t offer large pre-trained language models, it’s suitable for various text analysis tasks and integrates well with other Azure services.\nIBM Watson Natural Language Understanding Watson Natural Language Understanding is part of IBM’s suite of AI-powered tools. It provides advanced text analysis capabilities, including sentiment analysis, entity recognition, concept extraction, and categorization. While it doesn’t offer large-scale language generation models like GPT, it’s suitable for analyzing and extracting insights from text data.\nHugging Face Transformers Hugging Face Transformers is an open-source library that provides a wide range of pre-trained models for natural language processing tasks. It includes popular models like GPT, BERT, and RoBERTa, along with support for fine-tuning and custom model development. While it’s not a hosted API service like OpenAI, it offers powerful tools for developers to work with state-of-the-art NLP models.\nDeepAI Text Generation API DeepAI offers a Text Generation API that allows users to generate human-like text based on a given prompt. While it may not provide the scale or versatility of GPT-like models, it’s suitable for tasks such as generating short-form content, creative writing, and text completion.\nLLama LLama is an AI platform that offers large language models and other NLP capabilities for developers and enterprises. It provides access to pre-trained models, including GPT-like architectures, as well as tools for fine-tuning models on custom datasets. LLama aims to democratize access to advanced AI technologies and support a wide range of NLP applications.\nGemini Gemini by Cortical.io is an AI-based platform that offers natural language understanding and text analytics capabilities. It utilizes semantic folding technology, inspired by the human brain, to analyze and process text data efficiently. While it may not provide large-scale language generation models like GPT, Gemini offers powerful tools for semantic analysis, document clustering, and similarity detection.\n\n\n\n\n Back to top",
    "crumbs": [
      "Resources",
      "Language model APIs"
    ]
  },
  {
    "objectID": "resources/packages.html",
    "href": "resources/packages.html",
    "title": "Python packages",
    "section": "",
    "text": "Here are some short descriptions and links for each of the Python packages used for this script:\n\nJupyter: Jupyter is an open-source project that allows you to create and share documents containing live code, equations, visualizations, and narrative text. These documents, called notebooks, support various programming languages, including Python, R, and Julia. Jupyter notebooks are widely used for data analysis, machine learning, scientific research, and education.\nMatplotlib: Matplotlib is a popular plotting library for Python that enables you to create static, interactive, and publication-quality visualizations. It provides a wide range of plotting functions for creating line plots, scatter plots, bar charts, histograms, and more. Matplotlib is highly customizable and integrates well with other Python libraries like NumPy and pandas.\nPlotly: Plotly is a Python graphing library that produces interactive plots and dashboards. It offers a rich set of chart types, including line charts, scatter plots, bar charts, heatmaps, and 3D plots. Plotly’s interactive features allow users to explore data dynamically, zoom in on specific regions, and add annotations. Plotly can be used both offline and online, and it integrates seamlessly with Jupyter notebooks.\nscikit-learn: Scikit-learn is a comprehensive machine learning library for Python that provides simple and efficient tools for data mining and analysis. It features a wide range of algorithms for classification, regression, clustering, dimensionality reduction, and more. Scikit-learn is built on top of NumPy, SciPy, and matplotlib, making it easy to integrate into existing Python workflows.\nrapidfuzz: RapidFuzz is a fast string matching library for Python that provides various algorithms for fuzzy string matching and string similarity calculations. It offers functions for tasks like approximate string matching, fuzzy searching, and string similarity measurements based on Levenshtein distance, Jaro distance, and cosine similarity. RapidFuzz is useful for tasks such as data deduplication, spell checking, and record linkage.\nNLTK: NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources, such as WordNet. NLTK includes a suite of text processing libraries for tasks like tokenization, stemming, tagging, parsing, and classification. It is widely used in education, research, and industry for natural language processing tasks.\nspaCy: spaCy is an open-source natural language processing library for Python that is designed for efficiency, scalability, and ease of use. It provides pre-trained models for various languages and domains, along with an easy-to-use API for tasks such as tokenization, part-of-speech tagging, named entity recognition, dependency parsing, and text classification. spaCy is known for its speed and performance, making it suitable for both research and production environments.\n\n\n\n\n\n\n\nTip\n\n\n\nThis script has been created with Quarto.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Resources",
      "Python packages"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Seminar: Large Language Models",
    "section": "",
    "text": "Robot by DALL-E\n\n\nHello and welcome to the seminar Large Language Models in the winter semester of 2024/25 at the University of Applied Sciences in Münster. On this website, you will find all the information you need about and around the seminar.\n\nAbout the seminar\nThe seminar is roughly divided into 3 parts of equal size: theory, training and application. In the theoretical part, you will learn about the most important topics and ideas when it comes to natural language processing and large language models. We will discuss topics like tokenization, matching, statistical text analysis and embeddings to get you started before eventually dealing with large language models and their applications themselves. Already during the theory, we will make sure to code in Python alongside all the concepts and see coding examples to get familiar with it.\nAfter each small input session on a new topic, we will get to some hands-on training so that you can consolidate the knowledge you just acquired. You will solve a few (coding) exercises around all the topics yourselves. To get everyone fired up as quickly as possible, we have prepared a Jupyterlab environment that everyone can use for the solution of the exercises.\nIn the final part of the seminar we will go ahead and apply our newly acquired knowledge in our own projects. All participants will team up in teams of 2-3 and try to develop and implement their own little prototype for a small application involving a language model. More information and ideas for these projects can be found here.\nBy the way, you can (and maybe absolutely should) use a language model like ChatGPT also during this seminar and the solution of some of the exercises. However, feel encouraged to try for yourselves first, and make sure you have understood the solution of a language model if you use it.\n\n\nHow to use this script\nThis script is meant to give a comprehensive overview right away from the start. Feel free to browse it even before we have reached a specific topic, in particular, if you already have some prior knowledge in the topic. All exercises that we will solve together in this seminar are contained in this script as well, including their solution. For all exercises, the (or more precisely, a) solution is hidden behind a Show solution button. For the sake of your own learning process, try to solve the exercises yourselves first! If you’re stuck, ask for a quick hint. If you still feel like you do not advance any more, then check out the solution and try to understand it. The solution of the exercises is not part of the evaluation, so it’s really for your own progress! A “summary” of all exercises can be found here.\n\n\n\n\n\n\nImportant\n\n\n\nA small disclaimer: This script is not (yet) ridiculously comprehensive. And, of course, we cannot cover the full realm of NLP and LLM within a 4-days-course. However, you should find everything we will do in the seminar also in this script. If there is something missing, I will make sure to include it as soon as possible, just give me a note.\n\n\n\n\nWhat you will learn\nAs this seminar is meant to be an introduction to understanding and working with language models, so we can obviously not cover everything and offer deep insights into all the details. Instead, we aim to give you a simple overview of all the necessities to start working with language models APIs and understand why things are working the way they do and how you can apply them in your own applications. The content can already be seen from the navigation bar, but here’s a quick walk-through. More precisely, we will walk you through a quick history of natural language processing with some of its challenges and limitations, and introduce you to text processing and analysis techniques such as tokenization, term frequency or bag of words as well as applications such as text classification or sentiment analysis. Afterwards, we will give a short introduction to how modern large language models approach these with more sophisticated techniques based on neural networks and vast amounts of training data, before getting more hands-on with the language model API by OpenAI. Eventually, we will have a quick look into some other applications of embeddings, before quickly discussing some of the ethical considerations when working with language models. Have fun!\n\n\nA rough schedule\n\nIntroduction & Getting to know each other & Survey (experiences & expectations) & Learning goals & Evaluation criteria\nIntroduction to the general topic & Python & Jupyter\nIntroduction NLP (tokenization, matching, statistical analysis)\nIntroduction to LLM & OpenAI API\nPrompting\nEmbeddings\nAdvanced GPT topics (image data, parameterization, tool calling)\nReal-world examples of applications (& implementation) & limitations\nApp concept & Group brainstorming\nProject work on prototype & mentoring\nProject presentations & reflections on the seminar\nBackup: Ethics and data privacy\n\n\nAfter the seminar (~1d):\n\nPrototype refinement\nCode review & documentation\nRefine business case & potential applications of prototype\nReflections & lessons learned → Hand in 2-page summary\n\n\n\n\nEvaluation\nAll seminar participants will be evaluated in the following way.\n\nYour presentation on the last day of the seminar: 25%\nYour prototype: 35%\nYour summary: 25%\nYour activity during the seminar: 15%\n\nI will allow myself to give your evaluation a little extra boost for good activity during the seminar. This seminar is designed for everyone to participate, so the more you do, the more fun it will be!\n\nWhat is the summary?\nAs mentioned above, to finalize our seminar I want to you to take roughly a day to refine your prototype and then write a quick summary your project and your learnings. The summary should be 2-3 pages only (kind of like a small executive summary) and contain the following information: - What is your prototype? What can I do? - What could be a business case for your prototype, or where can it be applied? - What are current limitations of your prototype and how could you overcome them? - What have been your main learnings during the creation of your prototype (and/or) the seminar itself?\nJust hand it in within a couple of weeks after the seminar, it will be a part of your evaluation.\n\n\n\n\n\n\nNote\n\n\n\nHas this seminar been created with a little help of language models? Absolutely, why wouldn’t it? :)\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "About",
      "Seminar: Large Language Models"
    ]
  },
  {
    "objectID": "slides/llm/intro.html#background",
    "href": "slides/llm/intro.html#background",
    "title": "Large Language Models: A Quick Introduction",
    "section": "Background",
    "text": "Background\n\nRise of machine learning and deep learning in the 2000s\n\n\n\nIdea: Use neural networks for NLP tasks\nChallenge: How do we feed text into a neural network?\n\n\n\nAnswer: Text embeddings!"
  },
  {
    "objectID": "slides/llm/intro.html#example-from-bag-of-words",
    "href": "slides/llm/intro.html#example-from-bag-of-words",
    "title": "Large Language Models: A Quick Introduction",
    "section": "Example from Bag of Words",
    "text": "Example from Bag of Words\n\n\n\n\n\nflowchart LR\n  A(A cat does cat things) --&gt; B{\" \"}\n  B --&gt; C(A)\n  B --&gt; D(cat)\n  B --&gt; E(does)\n  B --&gt; F(cat)\n  B --&gt; G(things)\n  D --&gt; H(cat)\n  E --&gt; I(do)\n  F --&gt; J(cat)\n  G --&gt; K(thing)\n\n  H --&gt; L(cat: 2)\n  J --&gt; L\n  I --&gt; M(do: 1)\n  K --&gt; N(thing: 1)\n\n  L --&gt; O(2)\n  M --&gt; P(1)\n  N --&gt; Q(1)"
  },
  {
    "objectID": "slides/llm/intro.html#how-could-a-neural-network-look-like",
    "href": "slides/llm/intro.html#how-could-a-neural-network-look-like",
    "title": "Large Language Models: A Quick Introduction",
    "section": "How could a neural network look like?",
    "text": "How could a neural network look like?\n\n\n\n\n\nflowchart LR\n  A(Input Text) --&gt; B(Tokenization)\n  B --&gt; C(Token processing)\n  C --&gt; D(Embedding Layer)\n  D --&gt; E(Hidden Layers)\n  E --&gt; F(Output Layer)\n\n\n\n\n\n\n\nThe hidden layers and output layers depend on the application\nThe rest of the layers can be pre-trained (later)"
  },
  {
    "objectID": "slides/llm/intro.html#example-text-classification",
    "href": "slides/llm/intro.html#example-text-classification",
    "title": "Large Language Models: A Quick Introduction",
    "section": "Example: Text classification",
    "text": "Example: Text classification\n\n\n\n\n\nflowchart LR\n  A(Input Text) --&gt; B(Tokenization)\n  B --&gt; C(Token processing)\n  C --&gt; D(Embedding Layer)\n  D --&gt; E(Hidden Layers)\n  E --&gt; F(Output Layer)\n\n\n\n\n\n\n\nClassifying news articles into categories (sports, politics, …)\n\n\n\nTraining data: Dataset with corresponding category or label\nData processing: Tokenization, stop words, lower casing etc.\nTraining:\n\nMeasure the difference between predicted and true labels and adjust network weights\nExample: BoW embeddings - frequency of words affects label likelihood."
  },
  {
    "objectID": "slides/llm/intro.html#sequence-generation",
    "href": "slides/llm/intro.html#sequence-generation",
    "title": "Large Language Models: A Quick Introduction",
    "section": "Sequence generation",
    "text": "Sequence generation\n\nIdea: Models are trained to generate sequences of data (mostly: text) based on input/context.\nSequences have to resemble the training data.\nApplication: Text generation, music composition, image captioning\nRequires understanding language structure for meaningful output!"
  },
  {
    "objectID": "slides/llm/intro.html#language-modeling",
    "href": "slides/llm/intro.html#language-modeling",
    "title": "Large Language Models: A Quick Introduction",
    "section": "Language modeling",
    "text": "Language modeling\n \nIdea: Train a model to predict the probability distribution of words or tokens in a sequence given the preceding context!\n \n\n\n\n\n\nflowchart LR\n  A(The) --&gt; B(bird)\n  B --&gt; C(flew)\n  C --&gt; D(over)\n  D --&gt; E(the)\n  E --&gt; F{?}\n  F --&gt; G(\"p(rooftops)=0.31\")\n  F --&gt; H(\"p(trees)=0.14\")\n  F --&gt; J(\"p(guitar)=0.001\")"
  },
  {
    "objectID": "slides/llm/intro.html#training-process",
    "href": "slides/llm/intro.html#training-process",
    "title": "Large Language Models: A Quick Introduction",
    "section": "Training Process",
    "text": "Training Process\n\nExpose the model to large text datasets (great: we have the internet!)\nTeach the model statistical properties of language (Which token comes next?)\nCapture syntactic structures, semantic relationships, and contextual nuances\nTraining happens in an unsupervised fashion (we require no labels!)"
  },
  {
    "objectID": "slides/llm/intro.html#challenges",
    "href": "slides/llm/intro.html#challenges",
    "title": "Large Language Models: A Quick Introduction",
    "section": "Challenges",
    "text": "Challenges\n\nHandling vast and diverse nature of human language.\nComplex patterns, variations, and ambiguities.\nOut-of-vocabulary words, long-range dependencies, domain-specific knowledge.\nRequires robust architectures and sophisticated algorithms.\n\nBUT: They did it and it works!"
  },
  {
    "objectID": "slides/llm/intro.html#what-is-gpt",
    "href": "slides/llm/intro.html#what-is-gpt",
    "title": "Large Language Models: A Quick Introduction",
    "section": "What is GPT?",
    "text": "What is GPT?\n\ncurrent state-of-the-art language model\nintroduced in the paper “Attention is All You Need” by Vaswani et al. in 2017\nGPT belongs to the family of transformer-based models\nkey advantage over previous approaches:\n\nself-attention\nscalability"
  },
  {
    "objectID": "slides/llm/intro.html#what-is-a-transformer",
    "href": "slides/llm/intro.html#what-is-a-transformer",
    "title": "Large Language Models: A Quick Introduction",
    "section": "What is a transformer?",
    "text": "What is a transformer?\n\nTraditional approach:\n\ninformation flow constrained by fixed-length context windows or recurrent connections\nOne token at a time (in RNNs)"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#revisiting-what-we-know",
    "href": "slides/embeddings/embeddings.html#revisiting-what-we-know",
    "title": "Embeddings",
    "section": "Revisiting what we know",
    "text": "Revisiting what we know\nEmbeddings …\n\ntransform text into numerical vectors\nare used in neural network architectures\nKey benefit: Capture semantic similarities and relationships between words\n\n \n\nAlready seen: Bag of Words\nIssue: These embeddings do not compress!"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#what-are-embeddings",
    "href": "slides/embeddings/embeddings.html#what-are-embeddings",
    "title": "Embeddings",
    "section": "What are embeddings?",
    "text": "What are embeddings?\n\nRepresent words and text as dense, numerical vectors\nCapture rich semantic information\nContext-aware, based on surrounding text\nCapture subtle semantic relationships\nCompact representation compared to simple techniques such as bag of words"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#approaches-to-generate-embeddings",
    "href": "slides/embeddings/embeddings.html#approaches-to-generate-embeddings",
    "title": "Embeddings",
    "section": "Approaches to generate embeddings:",
    "text": "Approaches to generate embeddings:\n\nWord2Vec, GloVe, FastText\n\nTrain neural network to predict surrounding words\nCBOW or skip-gram architectures\nLearns semantic relationships in continuous vector space\n\nTransformer architectures like GPT\nWord embeddings provided by OpenAI"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#what-does-it-look-like",
    "href": "slides/embeddings/embeddings.html#what-does-it-look-like",
    "title": "Embeddings",
    "section": "What does it look like?",
    "text": "What does it look like?\nTrain a model to:\n\npredict the target word based on the (surrounding) context words, or\npredict the context words given a target word\n\n\n\n\n\n\nflowchart LR\n  A[\"Input Layer (One Hot)\"] \n  A --&gt; B[\"Embedding Layer\"]\n  B --&gt; C[\"Sum/Average Layer\"]\n  C --&gt; D[\"Output Layer\"]\n\n\n\n\n\n\n\n\nUse of the model\nThrow away the parts after the embedding layer!\n\n\n\n\n\nflowchart LR\n  A[\"Input Layer (One Hot)\"] \n  A --&gt; B[\"Embedding Layer\"]"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#task-find-the-matching-document-for-a-prompt",
    "href": "slides/embeddings/embeddings.html#task-find-the-matching-document-for-a-prompt",
    "title": "Embeddings",
    "section": "Task: Find the matching document for a prompt",
    "text": "Task: Find the matching document for a prompt\n\ntexts = [\n  \"This is the first document.\",\n  \"This document is the second document.\",\n  \"And this is the third one.\"\n]\n\nprompt = \"Is this the first document?\""
  },
  {
    "objectID": "slides/embeddings/embeddings.html#get-the-openai-client",
    "href": "slides/embeddings/embeddings.html#get-the-openai-client",
    "title": "Embeddings",
    "section": "Get the OpenAI client",
    "text": "Get the OpenAI client\n\n# prerequisites\n\nimport os\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value # choose the embedding model\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#get-the-embeddings",
    "href": "slides/embeddings/embeddings.html#get-the-embeddings",
    "title": "Embeddings",
    "section": "Get the embeddings",
    "text": "Get the embeddings\n\n# get the embeddings\nresponse = client.embeddings.create(\n    input=texts,\n    model=MODEL\n)\n\ntext_embeddings = [emb.embedding for emb in response.data]\n\nresponse = client.embeddings.create(\n    input=[prompt],\n    model=MODEL\n)\n\nprompt_embedding = response.data[0].embedding"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#compute-the-similarity",
    "href": "slides/embeddings/embeddings.html#compute-the-similarity",
    "title": "Embeddings",
    "section": "Compute the similarity",
    "text": "Compute the similarity\n\nimport numpy as np \n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -&gt; float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\n\nfor text, text_embedding in zip(texts, text_embeddings):\n    similarity = cosine_similarity(text_embedding, prompt_embedding)\n    print(f\"{text}: {round(similarity, 2)}\")\n\n\n\nThis is the first document.: 0.95\nThis document is the second document.: 0.88\nAnd this is the third one.: 0.8"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#define-some-words-to-visualize",
    "href": "slides/embeddings/embeddings.html#define-some-words-to-visualize",
    "title": "Embeddings",
    "section": "Define some words to visualize",
    "text": "Define some words to visualize\n\n# Define a list of words to visualize\nwords = [\n    \"king\", \"queen\", \"man\", \"woman\", \"apple\", \"banana\", \n    \"grapes\", \"cat\", \"dog\", \"happy\", \"sad\"\n]\n\n# Get embeddings for the words\nresponse = client.embeddings.create(\n    input=words,\n    model=MODEL\n)\n\nembeddings = [emb.embedding for emb in response.data]"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#apply-t-sne-to-the-embedding-vectors",
    "href": "slides/embeddings/embeddings.html#apply-t-sne-to-the-embedding-vectors",
    "title": "Embeddings",
    "section": "Apply T-SNE to the embedding vectors",
    "text": "Apply T-SNE to the embedding vectors\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n    n_components=2, \n    random_state=42,\n    perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(9, 7))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    plt.scatter(x, y, marker='o', color='red')\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#apply-t-sne-to-the-embedding-vectors-output",
    "href": "slides/embeddings/embeddings.html#apply-t-sne-to-the-embedding-vectors-output",
    "title": "Embeddings",
    "section": "Apply T-SNE to the embedding vectors",
    "text": "Apply T-SNE to the embedding vectors"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#cluster-the-embeddings",
    "href": "slides/embeddings/embeddings.html#cluster-the-embeddings",
    "title": "Embeddings",
    "section": "Cluster the embeddings",
    "text": "Cluster the embeddings\n\n# do the clus#| tering\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nn_clusters = 5\n\n# define the model\nkmeans = KMeans(\n  n_clusters=n_clusters,\n  n_init=\"auto\",\n  random_state=2 # do this to get the same output\n)\n\n# fit the model to the data\nkmeans.fit(np.array(embeddings))\n\n# get the cluster labels\ncluster_labels = kmeans.labels_"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#visualize-with-t-sne",
    "href": "slides/embeddings/embeddings.html#visualize-with-t-sne",
    "title": "Embeddings",
    "section": "Visualize with T-SNE",
    "text": "Visualize with T-SNE\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n  n_components=2, \n  random_state=42,\n  perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Define a color map for clusters\ncolors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(9, 7))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    cluster_label = cluster_labels[i]\n    color = colors[cluster_label]\n    plt.scatter(x, y, marker='o', color=color)\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\n\n\n\n\nSeminar: LLM, WiSe 2024/25"
  },
  {
    "objectID": "slides/embeddings/embeddings.html#visualize-with-t-sne-output",
    "href": "slides/embeddings/embeddings.html#visualize-with-t-sne-output",
    "title": "Embeddings",
    "section": "Visualize with T-SNE",
    "text": "Visualize with T-SNE"
  },
  {
    "objectID": "slides/python_intro/variables_data_types.html#introduction-to-variables-and-data-types",
    "href": "slides/python_intro/variables_data_types.html#introduction-to-variables-and-data-types",
    "title": "Variables and Data Types in Python",
    "section": "Introduction to Variables and Data Types",
    "text": "Introduction to Variables and Data Types\n\n\nWhat are Variables?\n\nVariables are containers for storing data values.\nIn Python, variables are created when you assign a value to them:\n\n\nx = 10\nname = \"Alice\"\n\n\nVariables do not need explicit declaration and can change type.\n\n\n\nData Types in Python\n\nData types classify data. In Python, everything is an object, and every object has a type.\nCommon data types include:\n\nIntegers: int\nFloating-point numbers: float\nStrings: str\nBooleans: bool"
  },
  {
    "objectID": "slides/python_intro/variables_data_types.html#variable-declaration-and-assignment",
    "href": "slides/python_intro/variables_data_types.html#variable-declaration-and-assignment",
    "title": "Variables and Data Types in Python",
    "section": "Variable Declaration and Assignment",
    "text": "Variable Declaration and Assignment\n\n\nDeclaration and Assignment\n\nVariables are assigned using the = operator.\nPython infers the data type based on the value assigned.\n\n\nage = 25  # Integer\npi = 3.14  # Float\nname = \"John\"  # String\nis_student = True  # Boolean\n\n\n\nChecking Variable Types\n\nYou can check the type of a variable using the type() function.\n\n\nprint(type(age))  # Output: &lt;class 'int'&gt;\nprint(type(name))  # Output: &lt;class 'str'&gt;\n\n&lt;class 'int'&gt;\n&lt;class 'str'&gt;\n\n\n\nPython is dynamically typed, meaning you don’t have to declare the type."
  },
  {
    "objectID": "slides/python_intro/variables_data_types.html#working-with-numbers",
    "href": "slides/python_intro/variables_data_types.html#working-with-numbers",
    "title": "Variables and Data Types in Python",
    "section": "Working with Numbers",
    "text": "Working with Numbers\n\n\nInteger Operations\n\nIntegers (int) are whole numbers. You can perform arithmetic operations on them.\n\n\nx = 10\ny = 5\nsum = x + y  # Addition\nproduct = x * y  # Multiplication\n\n\nCommon operators: +, -, *, /, % (modulo)\n\n\n\nFloat (Decimal Numbers)\n\nFloats represent numbers with decimal points.\n\n\nprice = 19.99\ndiscount = 0.10\ntotal = price * (1 - discount)\n\n\nOperations on floats behave similarly to integers."
  },
  {
    "objectID": "slides/python_intro/variables_data_types.html#strings-in-python",
    "href": "slides/python_intro/variables_data_types.html#strings-in-python",
    "title": "Variables and Data Types in Python",
    "section": "Strings in Python",
    "text": "Strings in Python\n\n\nString Basics\n\nA string is a sequence of characters enclosed in quotes.\n\n\nmessage = \"Hello, world!\"\nname = 'Alice'\n\n\nStrings can be single or double-quoted.\n\n\n\nString Operations\n\nConcatenation (+):\n\n\ngreeting = \"Hello, \" + name\n\n\nRepetition (*):\n\n\nlaugh = \"ha\" * 3  # Output: \"hahaha\"\n\n\nAccessing characters by index:\n\n\nfirst_letter = name[0]  # Output: 'A'"
  },
  {
    "objectID": "slides/python_intro/variables_data_types.html#boolean-and-none-types",
    "href": "slides/python_intro/variables_data_types.html#boolean-and-none-types",
    "title": "Variables and Data Types in Python",
    "section": "Boolean and None Types",
    "text": "Boolean and None Types\n\n\nBoolean Type\n\nBooleans represent True or False.\n\n\nis_adult = True\nhas_permission = False\n\n\nBoolean values are often used in conditional statements.\n\n\n\nThe None Type\n\nNone represents the absence of a value.\n\n\nresult = None\n\n\nOften used to indicate that a variable holds no value or is a placeholder for future data."
  },
  {
    "objectID": "slides/python_intro/variables_data_types.html#type-conversion",
    "href": "slides/python_intro/variables_data_types.html#type-conversion",
    "title": "Variables and Data Types in Python",
    "section": "Type Conversion",
    "text": "Type Conversion\n\n\nConverting Between Types\n\nYou can convert between different types using built-in functions.\n\n\nx = 10  # int\ny = float(x)  # Converts to float: 10.0\nz = str(x)  # Converts to string: '10'\n\n\n\nWhy Type Conversion Matters\n\nType conversion is important when combining different types in operations.\n\n\nage = 30\nmessage = \"I am \" + str(age) + \" years old.\"\nprint(message)  # Output: I am 30 years old.\n\nI am 30 years old.\n\n\n\n\n\n\n\nSeminar: LLM, WiSe 2024/25"
  },
  {
    "objectID": "slides/python_intro/lists_and_loops.html#lists-in-python",
    "href": "slides/python_intro/lists_and_loops.html#lists-in-python",
    "title": "Lists and Loops in Python",
    "section": "Lists in Python",
    "text": "Lists in Python\n\n\nWhat Are Lists?\n\nA list is an ordered collection of items.\nItems can be any data type: numbers, strings, or other lists.\nLists are mutable, meaning you can modify them after creation.\n\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nprint(fruits)\n\n['apple', 'banana', 'cherry']\n\n\n\n\nWhy Use Lists in NLP?\n\nLists can store words, sentences, or entire texts.\nNLP tasks often require managing large datasets of tokens or documents.\n\n\nsentence = \"Natural language processing is fun\"\ntokens = sentence.split()  # Tokenize sentence into words\nprint(tokens)\n\n['Natural', 'language', 'processing', 'is', 'fun']"
  },
  {
    "objectID": "slides/python_intro/lists_and_loops.html#accessing-list-elements",
    "href": "slides/python_intro/lists_and_loops.html#accessing-list-elements",
    "title": "Lists and Loops in Python",
    "section": "Accessing List Elements",
    "text": "Accessing List Elements\n\n\nIndexing\n\nAccess list elements using indexing.\nPython indexing starts at 0.\n\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nprint(fruits[0])  # Outputs: apple\nprint(fruits[1])  # Outputs: banana\n\napple\nbanana\n\n\n\n\nNegative Indexing\n\nNegative indexing starts from the end of the list.\n\n\nprint(fruits[-1])  # Outputs: cherry (last element)\n\ncherry\n\n\n\nUseful for quickly accessing the last few elements of a list."
  },
  {
    "objectID": "slides/python_intro/lists_and_loops.html#modifying-lists",
    "href": "slides/python_intro/lists_and_loops.html#modifying-lists",
    "title": "Lists and Loops in Python",
    "section": "Modifying Lists",
    "text": "Modifying Lists\n\n\nChanging Elements\n\nModify a list by assigning a new value at a specific index.\n\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfruits[1] = \"blueberry\"  # Replacing 'banana'\nprint(fruits)\n\n['apple', 'blueberry', 'cherry']\n\n\n\n\nReal-World NLP Use\n\nYou might replace tokens or correct errors in a tokenized sentence.\n\n\ntokens = [\"NLP\", \"is\", \"coool\"]\ntokens[2] = \"cool\"  # Fix typo\nprint(tokens)\n\n['NLP', 'is', 'cool']"
  },
  {
    "objectID": "slides/python_intro/lists_and_loops.html#adding-and-removing-elements",
    "href": "slides/python_intro/lists_and_loops.html#adding-and-removing-elements",
    "title": "Lists and Loops in Python",
    "section": "Adding and Removing Elements",
    "text": "Adding and Removing Elements\n\n\nAdding Elements\n\nUse .append() to add items to the end of the list.\n\n\nfruits = [\"apple\", \"banana\"]\nfruits.append(\"cherry\")\nprint(fruits)\n\n['apple', 'banana', 'cherry']\n\n\n\n\nRemoving Elements\n\nUse .remove() to delete specific items.\n\n\nfruits.remove(\"banana\")\nprint(fruits)\n\n['apple', 'cherry']\n\n\n\nLists are flexible, allowing easy manipulation in NLP tasks."
  },
  {
    "objectID": "slides/python_intro/lists_and_loops.html#list-operations-length-sorting-and-checking",
    "href": "slides/python_intro/lists_and_loops.html#list-operations-length-sorting-and-checking",
    "title": "Lists and Loops in Python",
    "section": "List Operations: Length, Sorting, and Checking",
    "text": "List Operations: Length, Sorting, and Checking\n\n\nList Length\n\nUse len() to find the number of elements in a list.\n\n\nfruits = [\"banana\", \"cherry\", \"apple\"]\nprint(len(fruits))\n\n3\n\n\n\n\nSorting and Checking Elements\n\nUse .sort() to sort the list.\nUse in to check if an item exists in a list.\n\n\nfruits.sort()\nprint(fruits)\n\nprint(\"apple\" in fruits)\nprint(\"grape\" in fruits)\n\n['apple', 'banana', 'cherry']\nTrue\nFalse"
  },
  {
    "objectID": "slides/python_intro/lists_and_loops.html#loops-in-python",
    "href": "slides/python_intro/lists_and_loops.html#loops-in-python",
    "title": "Lists and Loops in Python",
    "section": "Loops in Python",
    "text": "Loops in Python\n\n\nWhat Are Loops?\n\nLoops help you iterate over a list and perform actions on each element.\nCommonly used in NLP for tasks like text processing.\n\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor fruit in fruits:\n    print(fruit)\n\napple\nbanana\ncherry\n\n\n\n\nReal-World NLP Example\n\nTokenizing and processing text using loops.\n\n\ntokens = [\"NLP\", \"is\", \"fun\"]\nfor token in tokens:\n    print(token)\n\nNLP\nis\nfun"
  },
  {
    "objectID": "slides/python_intro/lists_and_loops.html#using-loops-for-simple-operations",
    "href": "slides/python_intro/lists_and_loops.html#using-loops-for-simple-operations",
    "title": "Lists and Loops in Python",
    "section": "Using Loops for Simple Operations",
    "text": "Using Loops for Simple Operations\n\n\nSquaring Numbers in a List\n\nLoops allow you to apply operations to list elements.\n\n\nnumbers = [1, 2, 3, 4]\nsquares = []\nfor number in numbers:\n    squares.append(number ** 2)\n\nprint(squares)\n\n[1, 4, 9, 16]\n\n\n\n\nNLP Example: Word Length Calculation\n\nUse loops to calculate the length of each word in a sentence.\n\n\ntokens = [\"NLP\", \"is\", \"great\"]\nfor token in tokens:\n    print(f\"{token}: {len(token)} characters\")\n\nNLP: 3 characters\nis: 2 characters\ngreat: 5 characters"
  },
  {
    "objectID": "slides/python_intro/lists_and_loops.html#looping-with-range",
    "href": "slides/python_intro/lists_and_loops.html#looping-with-range",
    "title": "Lists and Loops in Python",
    "section": "Looping with range()",
    "text": "Looping with range()\n\n\nUsing range()\n\nSometimes you need to iterate a set number of times.\nrange() generates a sequence of numbers.\n\n\nfor i in range(5):\n    print(i)\n\n0\n1\n2\n3\n4\n\n\n\n\nCombining range() with Lists\n\nUse range() to loop over list indices, allowing more control.\n\n\nfruits = [\"apple\", \"banana\", \"cherry\"]\nfor i in range(len(fruits)):\n    print(f\"Index {i}: {fruits[i]}\")\n\nIndex 0: apple\nIndex 1: banana\nIndex 2: cherry"
  },
  {
    "objectID": "slides/python_intro/lists_and_loops.html#common-list-methods",
    "href": "slides/python_intro/lists_and_loops.html#common-list-methods",
    "title": "Lists and Loops in Python",
    "section": "Common List Methods",
    "text": "Common List Methods\n\n\nAdding and Removing Elements\n\n.append(): Adds an item to the end.\n.remove(): Removes a specific item.\n\n\nfruits.append(\"orange\")\nfruits.remove(\"banana\")\nprint(fruits)\n\n['apple', 'cherry', 'orange']\n\n\n\n\nInserting and Popping Elements\n\n.insert(): Inserts an item at a specific index.\n.pop(): Removes and returns the last item or the item at a specific index.\n\n\nfruits.insert(1, \"blueberry\")\nprint(fruits)\n\nlast_fruit = fruits.pop()\nprint(fruits)\n\n['apple', 'blueberry', 'cherry', 'orange']\n['apple', 'blueberry', 'cherry']"
  },
  {
    "objectID": "slides/python_intro/lists_and_loops.html#summary-lists-and-loops",
    "href": "slides/python_intro/lists_and_loops.html#summary-lists-and-loops",
    "title": "Lists and Loops in Python",
    "section": "Summary: Lists and Loops",
    "text": "Summary: Lists and Loops\n\nLists: Allow you to store and manipulate multiple elements, useful in NLP for text processing.\nLoops: Help automate repetitive tasks and process list elements.\nYou can easily modify, sort, and check elements within lists.\nLoops and lists combined allow for efficient handling of data in Python, especially in NLP.\n\n\n\n\n\nSeminar: LLM, WiSe 2024/25"
  },
  {
    "objectID": "slides/python_intro/dictionaries.html#dictionaries-in-python",
    "href": "slides/python_intro/dictionaries.html#dictionaries-in-python",
    "title": "Dictionaries in Python",
    "section": "Dictionaries in Python",
    "text": "Dictionaries in Python\n\n\nWhat Are Dictionaries?\n\nA dictionary is a powerful data structure for storing data in key-value pairs.\nVersatile for associating specific values with unique identifiers (e.g., words with definitions).\nUnordered and mutable, allowing for fast lookups, insertions, and deletions.\n\n\n\nImportance in Data Management\n\nIdeal for scenarios like user IDs with user data or other associations.\nEnable efficient organization and retrieval of related data.\n\n\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\n\nprint(person)  # Output: {'name': 'Alice', 'age': 30, 'city': 'New York'}\n\n{'name': 'Alice', 'age': 30, 'city': 'New York'}"
  },
  {
    "objectID": "slides/python_intro/dictionaries.html#creating-a-dictionary",
    "href": "slides/python_intro/dictionaries.html#creating-a-dictionary",
    "title": "Dictionaries in Python",
    "section": "Creating a Dictionary",
    "text": "Creating a Dictionary\n\n\nBasic Syntax\n\nUse curly braces {} with key-value pairs separated by colons.\n\n\nperson = {\n    \"name\": \"Alice\",\n    \"age\": 30,\n    \"city\": \"New York\"\n}\n\n\n\nExplanation\n\nKeys and values can be of any data type (strings, integers, other dictionaries).\nExample dictionary person contains keys \"name\", \"age\", and \"city\"."
  },
  {
    "objectID": "slides/python_intro/dictionaries.html#accessing-values-in-a-dictionary",
    "href": "slides/python_intro/dictionaries.html#accessing-values-in-a-dictionary",
    "title": "Dictionaries in Python",
    "section": "Accessing Values in a Dictionary",
    "text": "Accessing Values in a Dictionary\n\n\nAccessing Values\n\nUse square brackets [] to retrieve values by their keys.\n\n\nprint(person[\"name\"])  # Output: Alice\nprint(person[\"age\"])   # Output: 30\n\nAlice\n30\n\n\n\n\nKeyError Handling\n\nAccessing a non-existent key raises a KeyError."
  },
  {
    "objectID": "slides/python_intro/dictionaries.html#modifying-a-dictionary",
    "href": "slides/python_intro/dictionaries.html#modifying-a-dictionary",
    "title": "Dictionaries in Python",
    "section": "Modifying a Dictionary",
    "text": "Modifying a Dictionary\n\n\nModifying Dictionary Contents\n\nDictionaries are mutable; you can add, modify, or remove key-value pairs.\n\nAdding a Key-Value Pair:\n\nperson[\"occupation\"] = \"Engineer\"\nprint(person)  # Output: {'name': 'Alice', 'age': 30, 'city': 'New York', 'occupation': 'Engineer'}\n\n{'name': 'Alice', 'age': 30, 'city': 'New York', 'occupation': 'Engineer'}\n\n\n\n\nModifying Values\n\nperson[\"age\"] = 31\nprint(person)  # Output: {'name': 'Alice', 'age': 31, 'city': 'New York', 'occupation': 'Engineer'}\n\n{'name': 'Alice', 'age': 31, 'city': 'New York', 'occupation': 'Engineer'}"
  },
  {
    "objectID": "slides/python_intro/dictionaries.html#dictionary-methods",
    "href": "slides/python_intro/dictionaries.html#dictionary-methods",
    "title": "Dictionaries in Python",
    "section": "Dictionary Methods",
    "text": "Dictionary Methods\n\n\nBuilt-in Methods\n\nPython dictionaries have various built-in methods for data manipulation.\n\nget() Method\n\nReturns the value for a specified key, returning None if the key doesn’t exist.\n\n\nage = person.get(\"age\", \"Not Found\")\nprint(age)  # Output: 31\n\n31\n\n\n\n\nExample with get()\n\nlocation = person.get(\"city\", \"Not Found\")\nprint(location)  # Output: Not Found\n\nNew York"
  },
  {
    "objectID": "slides/python_intro/dictionaries.html#dictionary-keys-values-and-items",
    "href": "slides/python_intro/dictionaries.html#dictionary-keys-values-and-items",
    "title": "Dictionaries in Python",
    "section": "Dictionary Keys, Values, and Items",
    "text": "Dictionary Keys, Values, and Items\n\n\nKeys, Values, and Items Methods\n\nRetrieve keys, values, or key-value pairs as needed.\n\nkeys() Method\n\nkeys = person.keys()\nprint(keys)  # Output: dict_keys(['name', 'age', 'occupation'])\n\ndict_keys(['name', 'age', 'city', 'occupation'])\n\n\n\n\nvalues() Method\n\nvalues = person.values()\nprint(values)  # Output: dict_values(['Alice', 31, 'Engineer'])\n\ndict_values(['Alice', 31, 'New York', 'Engineer'])"
  },
  {
    "objectID": "slides/python_intro/dictionaries.html#looping-through-a-dictionary",
    "href": "slides/python_intro/dictionaries.html#looping-through-a-dictionary",
    "title": "Dictionaries in Python",
    "section": "Looping Through a Dictionary",
    "text": "Looping Through a Dictionary\n\n\nLooping Techniques\n\nLoop through dictionaries using a for loop to access keys or both keys and values.\n\nLooping Through Keys\n\nfor key in person:\n    print(key)  # Output: name, age, occupation\n\nname\nage\ncity\noccupation\n\n\n\n\nLooping Through Keys and Values\n\nfor key, value in person.items():\n    print(f\"{key}: {value}\")\n# Output:\n# name: Alice\n# age: 31\n# occupation: Engineer\n\nname: Alice\nage: 31\ncity: New York\noccupation: Engineer"
  },
  {
    "objectID": "slides/python_intro/dictionaries.html#nesting-dictionaries",
    "href": "slides/python_intro/dictionaries.html#nesting-dictionaries",
    "title": "Dictionaries in Python",
    "section": "Nesting Dictionaries",
    "text": "Nesting Dictionaries\n\n\nWhat Are Nested Dictionaries?\n\nDictionaries can contain other dictionaries, enabling complex data structures.\n\n\npeople = {\n    \"Alice\": {\n        \"age\": 30,\n        \"city\": \"New York\"\n    },\n    \"Bob\": {\n        \"age\": 25,\n        \"city\": \"Los Angeles\"\n    }\n}\n\n\n\nAccessing Nested Values\n\nprint(people[\"Alice\"][\"city\"])  # Output: New York\n\nNew York"
  },
  {
    "objectID": "slides/python_intro/dictionaries.html#dictionary-comprehensions",
    "href": "slides/python_intro/dictionaries.html#dictionary-comprehensions",
    "title": "Dictionaries in Python",
    "section": "Dictionary Comprehensions",
    "text": "Dictionary Comprehensions\n\n\nWhat Are Dictionary Comprehensions?\n\nA concise way to create dictionaries by transforming or filtering data.\n\nSyntax\n{key_expression: value_expression for item in iterable if condition}\n\n\nExample of Dictionary Comprehension\n\nsquares = {x: x ** 2 for x in range(5)}\nprint(squares)  # Output: {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n\n{0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\n\n\n\n\n\n\n\nSeminar: LLM, WiSe 2024/25"
  },
  {
    "objectID": "slides/about/intro.html#why-should-i-be-here",
    "href": "slides/about/intro.html#why-should-i-be-here",
    "title": "Seminar: Large Language Models",
    "section": "Why should I be here?",
    "text": "Why should I be here?\n\n\nUnderstand how LLMs work and where their limitations are!\nLearn how to automate tasks with a language model\nLearn how to process large data amounts with a language model\nLearn how to use a large language model in a product"
  },
  {
    "objectID": "slides/about/intro.html#about-the-seminar",
    "href": "slides/about/intro.html#about-the-seminar",
    "title": "Seminar: Large Language Models",
    "section": "About the seminar",
    "text": "About the seminar\n\nRoughly divided into 3 parts: theory, training, and application\nTheory:\n\nLearn about important topics in natural language processing\nTopics include tokenization, matching, statistical text analysis, language models, and embeddings\nCoding examples in Python alongside theoretical concepts"
  },
  {
    "objectID": "slides/about/intro.html#intended-learning-outcomes-part-1",
    "href": "slides/about/intro.html#intended-learning-outcomes-part-1",
    "title": "Seminar: Large Language Models",
    "section": "Intended learning outcomes (Part 1)",
    "text": "Intended learning outcomes (Part 1)\n\nUnderstand the basics of natural language processing, including its tasks and challenges\nUnderstand the general concept of LLMs and why it makes the above so much easier\nWrite simple Python programs / scripts and use basic data and control structures"
  },
  {
    "objectID": "slides/about/intro.html#intended-learning-outcomes-part-2",
    "href": "slides/about/intro.html#intended-learning-outcomes-part-2",
    "title": "Seminar: Large Language Models",
    "section": "Intended learning outcomes (Part 2)",
    "text": "Intended learning outcomes (Part 2)\n\nAccess an LLM via the OpenAI API and how to work with the result\nUnderstand the concept of text embeddings and use them via the OpenAI API\nUnderstand why and how LLMs can be used for process automation\nUse an LLM in a small application\nHave fun!"
  },
  {
    "objectID": "slides/about/intro.html#content-1",
    "href": "slides/about/intro.html#content-1",
    "title": "Seminar: Large Language Models",
    "section": "Content 1",
    "text": "Content 1\n\nShort introduction to Programming in Python in the context of Natural Language Processing and Large Language Models\n\nBasics (Syntax, Variables, Data Types, Conditional Statements etc.)\nLists & Loops\nDictionaries & Classes"
  },
  {
    "objectID": "slides/about/intro.html#content-2",
    "href": "slides/about/intro.html#content-2",
    "title": "Seminar: Large Language Models",
    "section": "Content 2",
    "text": "Content 2\n\nQuick overview of classic NLP\n\nText processing (Tokenization, Lemmatization, etc.)\nApplications (Classification, Sentiment Analysis, Matching, etc.)\nChallenges\n\nIntroduction to LLM\n\nText processing with neural networks\nSequence generation & language modeling"
  },
  {
    "objectID": "slides/about/intro.html#content-3",
    "href": "slides/about/intro.html#content-3",
    "title": "Seminar: Large Language Models",
    "section": "Content 3",
    "text": "Content 3\n\nIntroduction to the OpenAI API\n\nPrompting\nParameterization\nFunction calling\n\nIntroduction to embeddings\n\nSimilarity\nVisualization & Clustering\n\n(Ethics & Privacy)"
  },
  {
    "objectID": "slides/about/intro.html#after-the-seminar-1d",
    "href": "slides/about/intro.html#after-the-seminar-1d",
    "title": "Seminar: Large Language Models",
    "section": "After the seminar (~1d):",
    "text": "After the seminar (~1d):\n\nPrototype refinement\nCode review & documentation\nRefine business case & potential applications of prototype\nReflections & lessons learned → Hand in 2-page summary"
  },
  {
    "objectID": "slides/about/intro.html#evaluation",
    "href": "slides/about/intro.html#evaluation",
    "title": "Seminar: Large Language Models",
    "section": "Evaluation",
    "text": "Evaluation\n\nYour presentation on the last day of the seminar: 25%\nYour prototype: 35%\nYour summary: 25%\nYour activity during the seminar: 15%"
  },
  {
    "objectID": "slides/about/intro.html#what-is-the-summary",
    "href": "slides/about/intro.html#what-is-the-summary",
    "title": "Seminar: Large Language Models",
    "section": "What is the summary?",
    "text": "What is the summary?\n\n2-3 pages only!\nWhat is your prototype? What can I do?\nWhat could be a business case for your prototype, or where can it be applied?\nWhat are current limitations of your prototype and how could you overcome them?\nWhat have been your main learnings during the creation of your prototype (and/or) the seminar itself?"
  },
  {
    "objectID": "slides/about/intro.html#jupyterlab-exercises",
    "href": "slides/about/intro.html#jupyterlab-exercises",
    "title": "Seminar: Large Language Models",
    "section": "Jupyterlab & Exercises",
    "text": "Jupyterlab & Exercises\n \nJupyterlab\nTo get started right away, we have prepared a Jupyterlab!\n \nExercises\nAll exercises can be solved in the Jupyterlab, all packages and datasets are pre-installed!\n\n\n\n\nSeminar: LLM, WiSe 2024/25"
  },
  {
    "objectID": "slides/nlp/short_history.html#part-of-speech-tagging-pos",
    "href": "slides/nlp/short_history.html#part-of-speech-tagging-pos",
    "title": "A Short History of Natural Language Processing",
    "section": "Part-of-Speech Tagging (POS)",
    "text": "Part-of-Speech Tagging (POS)\n\nLabeling each word with its grammatical category\nCrucial for language understanding, information retrieval, and machine translation\n\n\nThe sun sets behind the mountains, casting a golden glow across the sky."
  },
  {
    "objectID": "slides/nlp/short_history.html#named-entity-recognition-ner",
    "href": "slides/nlp/short_history.html#named-entity-recognition-ner",
    "title": "A Short History of Natural Language Processing",
    "section": "Named-Entity Recognition (NER)",
    "text": "Named-Entity Recognition (NER)\n\nIdentifying and classifying named entities in text\nEssential for information retrieval, document summarization, and question-answering systems\n\n\nApple is considering buying a U.K. based startup called LanguageHero located in London for $1 billion."
  },
  {
    "objectID": "slides/nlp/short_history.html#sentiment-analysis",
    "href": "slides/nlp/short_history.html#sentiment-analysis",
    "title": "A Short History of Natural Language Processing",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\nAnalyzing text to determine sentiment (e.g., positive, negative, neutral)\nUsed for gauging customer satisfaction, monitoring social media sentiment, etc.\n\n\nI love TextBlob! It’s an amazing library for natural language processing."
  },
  {
    "objectID": "slides/nlp/short_history.html#text-classification",
    "href": "slides/nlp/short_history.html#text-classification",
    "title": "A Short History of Natural Language Processing",
    "section": "Text Classification",
    "text": "Text Classification\n\nCategorizing text documents into predefined classes\nWidely used in email spam detection, sentiment analysis, and content categorization"
  },
  {
    "objectID": "slides/nlp/short_history.html#information-extraction",
    "href": "slides/nlp/short_history.html#information-extraction",
    "title": "A Short History of Natural Language Processing",
    "section": "Information Extraction",
    "text": "Information Extraction\n\nExtracting structured information from unstructured text data\nCrucial for knowledge base construction, data integration, and business intelligence"
  },
  {
    "objectID": "slides/nlp/short_history.html#question-answering",
    "href": "slides/nlp/short_history.html#question-answering",
    "title": "A Short History of Natural Language Processing",
    "section": "Question-Answering",
    "text": "Question-Answering\n\nGenerating accurate answers to user queries in natural language\nEssential for information retrieval, virtual assistants, and educational applications"
  },
  {
    "objectID": "slides/nlp/short_history.html#machine-translation",
    "href": "slides/nlp/short_history.html#machine-translation",
    "title": "A Short History of Natural Language Processing",
    "section": "Machine Translation",
    "text": "Machine Translation\n\nAutomatically translating text from one language to another\nFacilitates communication across language barriers"
  },
  {
    "objectID": "slides/nlp/short_history.html#early-days-rule-based-approaches-1960s-1980s",
    "href": "slides/nlp/short_history.html#early-days-rule-based-approaches-1960s-1980s",
    "title": "A Short History of Natural Language Processing",
    "section": "Early Days: Rule-Based Approaches (1960s-1980s)",
    "text": "Early Days: Rule-Based Approaches (1960s-1980s)\n\nRely heavily on rule-based approaches\nSignificant efforts in tasks like part-of-speech tagging, named entity recognition, and machine translation\nStruggled with ambiguity and complexity of natural language"
  },
  {
    "objectID": "slides/nlp/short_history.html#rise-of-statistical-methods-1990s-2000s",
    "href": "slides/nlp/short_history.html#rise-of-statistical-methods-1990s-2000s",
    "title": "A Short History of Natural Language Processing",
    "section": "Rise of Statistical Methods (1990s-2000s)",
    "text": "Rise of Statistical Methods (1990s-2000s)\n\nEmergence of statistical methods\nTechniques like Hidden Markov Models and Conditional Random Fields gained prominence\nImproved performance in tasks such as text classification, sentiment analysis, and information extraction"
  },
  {
    "objectID": "slides/nlp/short_history.html#machine-learning-revolution-2010s",
    "href": "slides/nlp/short_history.html#machine-learning-revolution-2010s",
    "title": "A Short History of Natural Language Processing",
    "section": "Machine Learning Revolution (2010s)",
    "text": "Machine Learning Revolution (2010s)\n\nRise of machine learning, particularly deep learning\nExploration of neural network architectures tailored for NLP tasks\nRecurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) gained traction"
  },
  {
    "objectID": "slides/nlp/short_history.html#large-language-models-transformers-2010s-present",
    "href": "slides/nlp/short_history.html#large-language-models-transformers-2010s-present",
    "title": "A Short History of Natural Language Processing",
    "section": "Large Language Models: Transformers (2010s-Present)",
    "text": "Large Language Models: Transformers (2010s-Present)\n\nRise of large language models, epitomized by the Transformer architecture\nPowered by self-attention mechanisms\nAchieved unprecedented performance across a wide range of NLP tasks"
  },
  {
    "objectID": "slides/nlp/statistics.html#term-frequency-token-counting",
    "href": "slides/nlp/statistics.html#term-frequency-token-counting",
    "title": "Simple statistical text analysis",
    "section": "Term frequency: Token counting",
    "text": "Term frequency: Token counting\n\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\nfrom collections import Counter\nfrom typing import List\n\nfrom nltk.corpus import stopwords\n# python -m nltk.downloader stopwords -&gt; run this in your console once to get the stopwords\n\n\n# load a text from file\ntext = \"\"\nwith open(\"../../assets/chapter1.txt\", \"r\") as file:  \n    for line in file:\n        text += line.strip()\n\n\ndef preprocess_text(text: str) -&gt; List[str]:\n    # tokenize text\n    tokens = wordpunct_tokenize(text.lower())\n\n    # remove punctuation\n    tokens = [t for t in tokens if t not in punctuation]\n\n    # remove stopwords\n    stop_words = stopwords.words(\"english\")\n    tokens = [t for t in tokens if t not in stop_words]\n\n    return tokens\n\n# count the most frequent words\ntokens = preprocess_text(text=text)\n\nfor t in Counter(tokens).most_common(15):\n    print(f\"{t[0]}: {t[1]}\")"
  },
  {
    "objectID": "slides/nlp/statistics.html#term-frequency-token-counting-output",
    "href": "slides/nlp/statistics.html#term-frequency-token-counting-output",
    "title": "Simple statistical text analysis",
    "section": "Term frequency: Token counting",
    "text": "Term frequency: Token counting\n\none: 35\nwinston: 32\nface: 28\neven: 24\n--: 24\nbig: 22\ncould: 19\nparty: 18\nwould: 18\nmoment: 18\nlike: 17\nbrother: 15\ngoldstein: 15\ntelescreen: 14\nseemed: 14"
  },
  {
    "objectID": "slides/nlp/statistics.html#bag-of-words-creating-a-vocabulary",
    "href": "slides/nlp/statistics.html#bag-of-words-creating-a-vocabulary",
    "title": "Simple statistical text analysis",
    "section": "Bag of Words: Creating a vocabulary",
    "text": "Bag of Words: Creating a vocabulary\n\nfrom collections import Counter\n\n\ndef create_bag_of_words(texts):\n    # Count the frequency of each word in the corpus\n    word_counts = Counter()\n    \n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Update word counts\n        word_counts.update(words)\n    \n    # Create vocabulary by sorting the words based on their frequency\n    vocabulary = [word for word, _ in sorted(word_counts.items())]\n    \n    # Create BoW vectors for each document\n    bow_vectors = []\n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Create a Counter object to count word frequencies\n        bow_vector = Counter(words)\n        \n        # Fill in missing words with zero counts\n        for word in vocabulary:\n            if word not in bow_vector:\n                bow_vector[word] = 0\n\n        # Sort the BoW vector based on the vocabulary order\n        sorted_bow_vector = [bow_vector[word] for word in vocabulary]\n        \n        # Append the BoW vector to the list\n        bow_vectors.append(sorted_bow_vector)\n    \n    return vocabulary, bow_vectors\n\n# Example texts\ntexts = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\",\n]\n\n# Create Bag of Words\nvocabulary, bow_vectors = create_bag_of_words(texts)\n\n# Print vocabulary\nprint(\"Vocabulary:\")\nprint(vocabulary)\n\n# Print BoW vectors\nprint(\"\\nBag of Words Vectors:\")\nfor i, bow_vector in enumerate(bow_vectors):\n    print(f\"Document {i + 1}: {bow_vector}\")"
  },
  {
    "objectID": "slides/nlp/statistics.html#bag-of-words-creating-a-vocabulary-output",
    "href": "slides/nlp/statistics.html#bag-of-words-creating-a-vocabulary-output",
    "title": "Simple statistical text analysis",
    "section": "Bag of Words: Creating a vocabulary",
    "text": "Bag of Words: Creating a vocabulary\n\nVocabulary:\n['document', 'first', 'one', 'second', 'third']\n\nBag of Words Vectors:\nDocument 1: [1, 1, 0, 0, 0]\nDocument 2: [2, 0, 0, 1, 0]\nDocument 3: [0, 0, 1, 0, 1]\nDocument 4: [1, 1, 0, 0, 0]"
  },
  {
    "objectID": "slides/nlp/statistics.html#statistical-similarity-of-texts",
    "href": "slides/nlp/statistics.html#statistical-similarity-of-texts",
    "title": "Simple statistical text analysis",
    "section": "Statistical similarity of texts",
    "text": "Statistical similarity of texts\n\nimport numpy as np\n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -&gt; float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\nquery = bow_vectors[3]\n\nsimilarities = []\nfor i, bv in enumerate(bow_vectors):\n    similarity = cosine_similarity(\n            vec1=query, \n            vec2=bv\n        )\n    similarities.append(\n        (texts[i], round(similarity, 2))\n    )\n\nsimilarities\n\n\n\n[('This is the first document.', 1.0),\n ('This document is the second document.', 0.63),\n ('And this is the third one.', 0.0),\n ('Is this the first document?', 1.0)]"
  },
  {
    "objectID": "slides/nlp/statistics.html#vocabulary",
    "href": "slides/nlp/statistics.html#vocabulary",
    "title": "Simple statistical text analysis",
    "section": "Vocabulary",
    "text": "Vocabulary\n\nGrows with corpus size\nLeads to computational inefficiencies and increased memory requirements\nFixed vocabulary after training, handling out-of-vocabulary words difficult"
  },
  {
    "objectID": "slides/nlp/statistics.html#context-structure",
    "href": "slides/nlp/statistics.html#context-structure",
    "title": "Simple statistical text analysis",
    "section": "Context & structure",
    "text": "Context & structure\n\nTreats each word independently\nFails to capture sequential and syntactical relationships\nLimits understanding of sarcasm, irony, and metaphors\nLack of structural awareness (sentences with similar word distributions but differing meanings or intents)"
  },
  {
    "objectID": "slides/nlp/statistics.html#general-idea",
    "href": "slides/nlp/statistics.html#general-idea",
    "title": "Simple statistical text analysis",
    "section": "General idea",
    "text": "General idea\n\nClustering a common technique in text analysis\nGroups similar documents based on word distributions\nEach document represented as high-dimensional vector (e.g., BoW)\nDimensions correspond to unique words\nValues reflect frequency of words in document"
  },
  {
    "objectID": "slides/nlp/statistics.html#approach",
    "href": "slides/nlp/statistics.html#approach",
    "title": "Simple statistical text analysis",
    "section": "Approach",
    "text": "Approach\n\nClustering algorithms applied to partition documents\nSimilarity measured using distance metrics (cosine similarity, Euclidean distance)\nAdvantages:\n\nSimplicity and scalability\nManageable computational complexity\n\nLimitations:\n\nReliance on word frequency alone\nCurse of dimensionality with large vocabularies"
  },
  {
    "objectID": "slides/nlp/statistics.html#code-example",
    "href": "slides/nlp/statistics.html#code-example",
    "title": "Simple statistical text analysis",
    "section": "Code example",
    "text": "Code example\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Example texts representing different topics\ntexts = [\n    \"apple orange banana\",\n    \"apple orange mango\",\n    \"banana apple kiwi\",\n    \"strawberry raspberry blueberry\",\n    \"strawberry raspberry blackberry\"\n]\n\n# create Bag of Words using CountVectorizer\nvectorizer = CountVectorizer()\nbow_matrix = vectorizer.fit_transform(texts)\n\nprint(\"Bag of Word vectors:\")\nprint(bow_matrix.toarray())\n\n# perform K-means clustering\nnum_clusters = 2\nkmeans = KMeans(n_clusters=num_clusters, n_init=\"auto\")\ncluster_labels = kmeans.fit_predict(bow_matrix)\n\nprint(\"\\nCluster labels:\")\nfor i, label in enumerate(cluster_labels):\n    print(f\"Document {i + 1} belongs to Cluster {label + 1}\")"
  },
  {
    "objectID": "slides/nlp/statistics.html#code-example-output",
    "href": "slides/nlp/statistics.html#code-example-output",
    "title": "Simple statistical text analysis",
    "section": "Code example",
    "text": "Code example\n\nBag of Word vectors:\n[[1 1 0 0 0 0 1 0 0]\n [1 0 0 0 0 1 1 0 0]\n [1 1 0 0 1 0 0 0 0]\n [0 0 0 1 0 0 0 1 1]\n [0 0 1 0 0 0 0 1 1]]\n\nCluster labels:\nDocument 1 belongs to Cluster 2\nDocument 2 belongs to Cluster 2\nDocument 3 belongs to Cluster 2\nDocument 4 belongs to Cluster 1\nDocument 5 belongs to Cluster 1"
  },
  {
    "objectID": "ethics/bias.html",
    "href": "ethics/bias.html",
    "title": "Bias",
    "section": "",
    "text": "It is crucial to also explore the concept of bias lurking within language models. While these models have revolutionized various fields and are arguably one of the most impactful new tools of the last few years, they aren’t immune to inheriting and perpetuating biases present in the data they are trained on. So what is Bias in Language Models?\nBias in language models refers to the skewed or unfair representation of certain groups, perspectives, or ideologies within the generated text. These biases can stem from societal stereotypes, historical prejudices, or systemic inequalities embedded in the training data. In particular for models trained on enormous corpora stemming from the internet, it is a nearly impossible task to examine all of the training data for dangerous our otherwise harmful content. And even simply the choice of the training data can create an inherent bias in the models. As an example, consider training a model only on German data, which will inevitably introduce German opinions etc. into the model. When left unchecked, biased language models can reinforce existing prejudices, amplify underrepresented narratives, and marginalize certain communities.\n\nTypes of Bias in Language Models\nThere are plenty of different types of bias that can occur in language models, here are just a few.\n\nGender bias: Language models may exhibit gender bias by associating specific roles, traits, or occupations with a particular gender. For example, phrases like “brilliant scientist” might more frequently generate male pronouns, while “caring nurse” might generate female pronouns, perpetuating stereotypes about gender roles.\nEthnic and racial bias: Language models may reflect ethnic or racial biases present in the training data, leading to stereotypical or discriminatory language towards certain racial or ethnic groups. For instance, associating negative traits with specific racial groups or making assumptions based on names or cultural references.\nSocioeconomic bias: Language models might exhibit biases related to socioeconomic status, such as portraying certain occupations or lifestyles as superior or inferior. This can contribute to the reinforcement of class stereotypes and disparities.\nCultural bias: Language models may demonstrate cultural biases by favoring certain cultural norms, values, or references over others, potentially marginalizing or erasing the perspectives of minority cultures or communities.\nConfirmation bias: Language models can inadvertently reinforce existing beliefs or viewpoints by prioritizing information that aligns with preconceived notions and ignoring contradictory evidence, leading to the perpetuation of misinformation or echo chambers.\n\n\n\nImplications of bias in language models\nThe presence of bias in language models has plenty of implications, in particular when societies start using language models frequently.\n\nReinforcement of stereotypes: Biased language models can perpetuate harmful stereotypes, further entrenching societal prejudices and hindering efforts towards inclusivity and diversity.\nDiscriminatory outcomes: Biased language models may lead to discriminatory outcomes in various applications, including hiring processes, automated decision-making systems, and content moderation algorithms, potentially amplifying existing inequalities.\nUnderrepresentation and marginalization: Language models may marginalize or underrepresent certain groups or perspectives, leading to the erasure of minority voices and experiences from the discourse.\nImpact on society: Biased language models can have far-reaching consequences on society, shaping public opinion, reinforcing power dynamics, and influencing policy decisions, ultimately exacerbating social divisions and injustices.\n\n\n\nAddressing bias in language models\nSo, what can we (or the creators of language models) do?\n\nDiverse and representative data: Ensuring that language models are trained on diverse and representative datasets spanning various demographics, cultures, and perspectives can help mitigate biases by providing a more balanced and inclusive training corpus.\nBias detection and mitigation techniques: Implementing bias detection and mitigation techniques, such as debiasing algorithms, adversarial training, and fairness-aware learning frameworks, can help identify and address biases in language models during the development phase.\nEthical considerations and transparency: Incorporating ethical considerations and promoting transparency in the development and deployment of language models can foster accountability and empower users to critically assess the potential biases and limitations of these models.\nContinuous monitoring and evaluation: Regularly monitoring and evaluating language models for biases in real-world applications can help identify and rectify unintended consequences, ensuring that these models align with ethical standards and promote fairness and inclusivity.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Ethical Considerations",
      "Bias"
    ]
  },
  {
    "objectID": "nlp/tokenization.html",
    "href": "nlp/tokenization.html",
    "title": "Tokenization",
    "section": "",
    "text": "A key element for a computer to understand the words we speak or type is the concept of word tokenization. For a human, the sentence\n\nsentence = \"I love reading science fiction books or books about science.\"\n\nis easy to understand since we are able to split the sentence into its individual parts in order to figure out the meaning of the full sentence. For a computer, the sentence is just a simple string of characters, like any other word or longer text. In order to make a computer understand the meaning of a sentence, we need to help break it down into its relevant parts.\nSimply put, word tokenization is the process of breaking down a piece of text into individual words or so-called tokens. It is like taking a sentence and splitting it into smaller pieces, where each piece represents a word. Word tokenization involves analyzing the text character by character and identifying boundaries between words. It uses various rules and techniques to decide where one word ends and the next one begins. For example, spaces, punctuation marks, and special characters often serve as natural boundaries between words.\nSo let’s start breaking down the sentence into its individual parts.\n\ntokenized_sentence = sentence.split(\" \")\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science.']\n\n\nOnce we have tokenized the sentence, we can start analyzing it with some simple statistical methods. For example, in order to figure out what the sentence might be about, we could count the most frequent words.\n\nfrom collections import Counter\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n\n[('books', 2), ('I', 1)]\n\n\nUnfortunately, we already realize that we have not done the best job with our “tokenizer”: The second occurrence of the word science is missing do to the punctuation. While this is great as it holds information about the ending of a sentence, it disturbs our analysis here, so let’s get rid of it.\n\ntokenized_sentence = sentence.replace(\".\", \" \").split(\" \")\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n\n[('science', 2), ('books', 2)]\n\n\nSo that worked. As you can imagine, tokenization can get increasingly difficult when we have to deal with all sorts of situations in larger corpora of texts (see also the exercise). So it is great that there are already all sorts of libraries available that can help us with this process.\n\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\n\ntokenized_sentence = wordpunct_tokenize(sentence)\ntokenized_sentence = [t for t in tokenized_sentence if t not in punctuation]\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science']",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Tokenization"
    ]
  },
  {
    "objectID": "nlp/tokenization.html#simple-word-tokenization",
    "href": "nlp/tokenization.html#simple-word-tokenization",
    "title": "Tokenization",
    "section": "",
    "text": "A key element for a computer to understand the words we speak or type is the concept of word tokenization. For a human, the sentence\n\nsentence = \"I love reading science fiction books or books about science.\"\n\nis easy to understand since we are able to split the sentence into its individual parts in order to figure out the meaning of the full sentence. For a computer, the sentence is just a simple string of characters, like any other word or longer text. In order to make a computer understand the meaning of a sentence, we need to help break it down into its relevant parts.\nSimply put, word tokenization is the process of breaking down a piece of text into individual words or so-called tokens. It is like taking a sentence and splitting it into smaller pieces, where each piece represents a word. Word tokenization involves analyzing the text character by character and identifying boundaries between words. It uses various rules and techniques to decide where one word ends and the next one begins. For example, spaces, punctuation marks, and special characters often serve as natural boundaries between words.\nSo let’s start breaking down the sentence into its individual parts.\n\ntokenized_sentence = sentence.split(\" \")\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science.']\n\n\nOnce we have tokenized the sentence, we can start analyzing it with some simple statistical methods. For example, in order to figure out what the sentence might be about, we could count the most frequent words.\n\nfrom collections import Counter\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n\n[('books', 2), ('I', 1)]\n\n\nUnfortunately, we already realize that we have not done the best job with our “tokenizer”: The second occurrence of the word science is missing do to the punctuation. While this is great as it holds information about the ending of a sentence, it disturbs our analysis here, so let’s get rid of it.\n\ntokenized_sentence = sentence.replace(\".\", \" \").split(\" \")\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n\n[('science', 2), ('books', 2)]\n\n\nSo that worked. As you can imagine, tokenization can get increasingly difficult when we have to deal with all sorts of situations in larger corpora of texts (see also the exercise). So it is great that there are already all sorts of libraries available that can help us with this process.\n\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\n\ntokenized_sentence = wordpunct_tokenize(sentence)\ntokenized_sentence = [t for t in tokenized_sentence if t not in punctuation]\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science']",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Tokenization"
    ]
  },
  {
    "objectID": "nlp/tokenization.html#advanced-word-tokenization",
    "href": "nlp/tokenization.html#advanced-word-tokenization",
    "title": "Tokenization",
    "section": "Advanced word tokenization",
    "text": "Advanced word tokenization\nAs you can imagine, the idea of tokenization does not stop here, we have only touched the basic idea of it. As a general strategy, we can now start from our word tokens and refine the tokens in a way that we need. A classic problem arising with the token above is that we do not get word tokens in their standardized form. So if, for example, we were to take all our token together and use that as a dictionary, we would get two different token for every words that appears both in singular and plural form (with the added “s”). Or we would receive different tokens for every verb in different conjugations (for example, “speak”, “speaks”, and “spoken”). Depending on our task, a great idea is to try and find the basic form of each token, a process called lemmatization.\n\nLemmatization\nLemmatization is a natural language processing technique used to reduce words to their base or canonical form, known as the lemma. The lemma represents the dictionary form of a word and is typically a valid word that exists in the language. Lemmatization helps in standardizing words so that different forms of the same word are treated as one, simplifying text analysis and improving accuracy in tasks such as text classification, information retrieval, and sentiment analysis.\nUnlike stemming, which simply chops off prefixes or suffixes to derive the root form of a word (sometimes resulting in non-existent or incorrect forms), lemmatization considers the context of the word and applies linguistic rules to transform it into its lemma. This ensures that the resulting lemma is a valid word in the language and retains its semantic meaning.\nLemmatization involves part-of-speech tagging to determine the correct lemma for each word based on its role in the sentence. For example, the word “running” may be lemmatized to “run” as a verb, but to “running” as a noun. However, once we have lemmatized our text, we might lose some information due to the lost context.\nLuckily for us, there are already pre-built packages that we can use to try out lemmatization. Here is a quick example how to do it with nltk.\n\nfrom nltk.stem import WordNetLemmatizer\n\nsentence = \"The three brothers went over three big bridges\"\n\nwnl = WordNetLemmatizer()\n\nlemmatized_sentence_token = [\n    wnl.lemmatize(w, pos=\"n\") for w in sentence.split(\" \")\n]\n\nprint(lemmatized_sentence_token)\n\n['The', 'three', 'brother', 'went', 'over', 'three', 'big', 'bridge']\n\n\nSince we need to include the pos (part-of-speech) tag of each word and only choose a noun (n) here, the lemmatizer only takes care of nouns in the sentence. Let’s try it with verbs.\n\nlemmatized_sentence_token = [\n    wnl.lemmatize(w, pos=\"v\") for w in sentence.split(\" \")\n]\n\nprint(lemmatized_sentence_token)\n\n['The', 'three', 'brothers', 'go', 'over', 'three', 'big', 'bridge']\n\n\nThis works, however, we now encounter a different problem: The word bridges has been turned into bridge, since it exists also as the ver “to bridge”. So, as mentioned above, we need to involve some part of speech tagging in order to do it correctly. Let’s try to fix it manually.\n\npos_dict = {\n  \"brothers\": \"n\", \n  \"went\": \"v\",\n  \"big\": \"a\",\n  \"bridges\": \"n\"\n}\n\nlemmatized_sentence_token = []\nfor token in sentence.split(\" \"):\n    if token in pos_dict:\n        lemma = wnl.lemmatize(token, pos=pos_dict[token])\n    else: \n        lemma = token # leave as it is\n\n    lemmatized_sentence_token.append(lemma)\n\nprint(lemmatized_sentence_token)\n\n['The', 'three', 'brother', 'go', 'over', 'three', 'big', 'bridge']\n\n\nAgain, luckily there also some packages that we can use, spaCy is one example. Their models come with a lot of built-in functionality.\n\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(sentence)\n\nlemmatized_words = [(token.lemma_, token.pos_) for token in doc]\n\nprint(lemmatized_words)\n\n[('the', 'DET'), ('three', 'NUM'), ('brother', 'NOUN'), ('go', 'VERB'), ('over', 'ADP'), ('three', 'NUM'), ('big', 'ADJ'), ('bridge', 'NOUN')]\n\n\nNow that we have successfully lemmatized our document or text, we can start using the lemmas to do some further analysis. For example, we could build a dictionary and do some statistics on it. We will see more about that later.\n\n\nBit Pair Encoding\nThe above ideas illustrate well the idea of tokenization of splitting text into smaller chunks that we can feed to a language model. In practice, especially in models like GPT, a critical component is the vocabulary or the set of unique words or tokens the model understands. Traditional approaches use fixed-size vocabularies, which means every unique word in the corpus has its own representation (index or embedding) in the model’s vocabulary. However, as the vocabulary size increases (for example, by including more languages), so does the memory requirement, which can be impractical for large-scale language models. One solution is the so-called bit-pair encoding. Bit pair encoding is a data compression technique specifically designed to tackle the issue of large vocabularies in language models. Instead of assigning a unique index or embedding to each token, bit pair encoding identifies frequent pairs of characters (bits) within the corpus and represents them as a single token. This effectively reduces the size of the vocabulary while preserving the essential information needed for language modeling tasks.\n\nHow it works\n\nTokenization: The first step in bit pair encoding is tokenization, where the text corpus is broken down into individual tokens. These tokens could be characters, subwords, or words, depending on the tokenization strategy used.\nPair Identification: Next, the algorithm identifies pairs of characters (bits) that occur frequently within the corpus. These pairs are typically consecutive characters in the text.\nReplacement with Single Token: Once frequent pairs are identified, they are replaced with a single token. This effectively reduces the number of unique tokens in the vocabulary.\nIterative Process: The process of identifying frequent pairs and replacing them with single tokens is iterative. It continues until a predefined stopping criterion is met, such as reaching a target vocabulary size or when no more frequent pairs can be found.\nVocabulary Construction: After the iterative process, a vocabulary is constructed, consisting of the single tokens generated through pair replacement, along with any remaining tokens from the original tokenization process.\nEncoding and Decoding: During training and inference, text data is encoded using the constructed vocabulary, where each token is represented by its corresponding index in the vocabulary. During decoding, the indices are mapped back to their respective tokens.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is very illustrative to use the the OpenAI tokenizer to see how a sentence is split up into different token. Try mixing languages and standard as well as more rare words and observe how they are split up.\nAnother detailed example can be found here.\n\n\n\n\nAdvantages of Bit Pair Encoding\n\nEfficient Memory Usage: Bit pair encoding significantly reduces the size of the vocabulary, leading to more efficient memory usage, especially in large-scale language models.\nRetains Information: Despite reducing the vocabulary size, bit pair encoding retains important linguistic information by capturing frequent character pairs.\nFlexible: Bit pair encoding is flexible and can be adapted to different tokenization strategies and corpus characteristics.\n\n\n\nLimitations and Considerations\n\nComputational Overhead: The iterative nature of bit pair encoding can be computationally intensive, especially for large corpora.\nLoss of Granularity: While bit pair encoding reduces vocabulary size, it may lead to a loss of granularity, especially for rare or out-of-vocabulary words.\nTokenization Strategy: The effectiveness of bit pair encoding depends on the tokenization strategy used and the characteristics of the corpus.\n\n\n\n\n\n\n\nTip\n\n\n\nFrom the OpenAI Guide:\nA helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words).",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Tokenization"
    ]
  },
  {
    "objectID": "nlp/exercises/ex_fuzzy_matching.html",
    "href": "nlp/exercises/ex_fuzzy_matching.html",
    "title": "Exercise: Fuzzy matching",
    "section": "",
    "text": "Task: Write a function that helps finding the most similar words or tokens from a given list based on a user query using rapidfuzz.\nInstructions:\n\nWrite a Python function called find_similar_words(query, word_list) that takes a user query and a list of words or tokens as input.\nInside the function, use rapidfuzz to calculate the similarity between the query and each word/token in the list.\nReturn a list of tuples containing the word/token and its corresponding similarity score, sorted in descending order of similarity.\n\n\nword_list = [\"apple\", \"banana\", \"orange\", \"grape\", \"pineapple\", \"kiwi\"]\nquery = \"appl\"\n\n\n\nShow solution\n\n\n# Importing necessary packages\nfrom rapidfuzz import fuzz\n\ndef find_similar_words(query, word_list):\n    # Create an empty list to store word and similarity score tuples\n    similar_words = []\n    \n    # Loop through each word in the word_list\n    for word in word_list:\n        # Calculate the similarity score between the query and the current word\n        similarity = fuzz.ratio(query, word)\n        \n        # Append the word and its similarity score to the list\n        similar_words.append((word, similarity))\n    \n    # Sort the list of tuples based on similarity score in descending order\n    similar_words.sort(key=lambda x: x[1], reverse=True)\n    \n    # Return the sorted list of similar words\n    return similar_words\n\nsimilar_words = find_similar_words(query, word_list)\nprint(\"Similar words to '{}' are:\".format(query))\nfor word, similarity in similar_words:\n    print(\"{} (Similarity: {})\".format(word, round(similarity, 2)))\n\nSimilar words to 'appl' are:\napple (Similarity: 88.89)\npineapple (Similarity: 61.54)\ngrape (Similarity: 44.44)\nbanana (Similarity: 20.0)\norange (Similarity: 20.0)\nkiwi (Similarity: 0.0)\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Exercise: Fuzzy matching"
    ]
  },
  {
    "objectID": "nlp/exercises/ex_tfidf.html",
    "href": "nlp/exercises/ex_tfidf.html",
    "title": "Exercise: TF-IDF",
    "section": "",
    "text": "Task: Extend the code for the bag of words to TF-IDF (Term Frequency-Inverse Document Frequency) vectors for a given set of documents. TF-IDF is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents. This measure helps in identifying words that are unique and informative to a particular document while downweighting common words that appear across many documents.\nTF-IDF consists of two main components:\nTerm Frequency (TF): This component measures how frequently a term occurs in a document. It is calculated as the ratio of the count of a term in a document to the total number of terms in the document. TF is higher for words that occur more frequently within a document.\nTF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\nInverse Document Frequency (IDF): This component measures the rarity of a term across the entire corpus of documents. It is calculated as the logarithm of the ratio of the total number of documents to the number of documents containing the term, plus one. IDF is higher for terms that are rare across documents but present in only a few documents.\nIDF(t) = log((1 + Total number of documents) / (1 + Number of documents containing term t))\nThe TF-IDF score for a term in a document is obtained by multiplying its TF and IDF scores. This score reflects the importance of the term in the context of the document and the entire corpus.\nInstructions:\n\nImplement functions calculate_tf and calculate_idf to calculate Term Frequency (TF) and Inverse Document Frequency (IDF) respectively.\nWrite a create_tf_idf function to create TF-IDF vectors for a given set of documents. This function should count the frequency of each word in the corpus, calculate TF and IDF, and compute TF-IDF vectors for each document.\n\n\n\nShow solution\n\n\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\nfrom typing import List\n\nfrom nltk.corpus import stopwords\n# python -m nltk.downloader stopwords -&gt; run this in your console once to get the stopwords\n\ndef preprocess_text(text: str) -&gt; List[str]:\n    # tokenize text\n    tokens = wordpunct_tokenize(text.lower())\n\n    # remove punctuation\n    tokens = [t for t in tokens if t not in punctuation]\n\n    # remove stopwords\n    stop_words = stopwords.words(\"english\")\n    tokens = [t for t in tokens if t not in stop_words]\n\n    return tokens\n\n\nfrom collections import Counter\nimport math\n\n\ndef calculate_tf(word_counts, total_words):\n    # Calculate Term Frequency (TF)\n    tf = {}\n    for word, count in word_counts.items():\n        tf[word] = count / total_words\n    return tf\n\ndef calculate_idf(word_counts, num_documents):\n    # Calculate Inverse Document Frequency (IDF)\n    idf = {}\n    for word, count in word_counts.items():\n        idf[word] = math.log((1 + num_documents) / (1 + count))\n    return idf\n\ndef create_tf_idf(texts):\n    # Count the frequency of each word in the corpus and total number of words\n    word_counts = Counter()\n    total_words = 0\n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Update word counts and total number of words\n        word_counts.update(words)\n        total_words += len(words)\n    \n    # Create sorted vocabulary\n    vocabulary = sorted(word_counts.keys())\n    \n    # Calculate TF-IDF for each document\n    tf_idf_vectors = []\n    num_documents = len(texts)\n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Calculate TF for the document\n        tf = calculate_tf(Counter(words), len(words))\n        \n        # Calculate IDF based on word counts across all documents\n        idf = calculate_idf(word_counts, num_documents)\n        \n        # Calculate TF-IDF for the document\n        tf_idf_vector = {}\n        for word in vocabulary:\n            tf_idf_vector[word] = round(tf.get(word, 0) * idf[word], 2)\n        \n        # Sort the IFIDF vector based on the vocabulary order\n        sorted_tfidf_vector = [tf_idf_vector[word] for word in vocabulary]\n        \n        # Append the BoW vector to the list\n        tf_idf_vectors.append(sorted_tfidf_vector)\n    \n    return vocabulary, tf_idf_vectors\n\n# Example texts\ntexts = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\",\n]\n\n# Create TF-IDF vectors\nvocabulary, tf_idf_vectors = create_tf_idf(texts)\n\n# Print vocabulary\nprint(\"Vocabulary:\")\nprint(vocabulary)\n\n# Print TF-IDF vectors\nprint(\"\\nTF-IDF Vectors:\")\nfor i, tf_idf_vector in enumerate(tf_idf_vectors):\n    print(f\"Document {i + 1}: {tf_idf_vector}\")\n\nVocabulary:\n['document', 'first', 'one', 'second', 'third']\n\nTF-IDF Vectors:\nDocument 1: [0.0, 0.26, 0.0, 0.0, 0.0]\nDocument 2: [0.0, 0.0, 0.0, 0.31, 0.0]\nDocument 3: [0.0, 0.0, 0.46, 0.0, 0.46]\nDocument 4: [0.0, 0.26, 0.0, 0.0, 0.0]\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Exercise: TF-IDF"
    ]
  },
  {
    "objectID": "nlp/fuzzy_matching.html",
    "href": "nlp/fuzzy_matching.html",
    "title": "Fuzzy matching",
    "section": "",
    "text": "As can be seen from the previous example, the detection of certain keywords from a text can prove more difficult than one might expect. The key issues stem from the fact that natural language has many facets such as conjugation, singular and plural forms, adjectives vs. adverbs etc. But even when these are handled, there remain challenges for keywords detection. In the previous example, our detection still fails when:\n\nkeywords consist of multiple words (product portfolio),\nkeywords have different forms but mean the same (advertisment vs. advertising),\nkeywords have wrong spelling (langscpe vs. landscape),\nkeywords and target words are not exactly the same thing but closely related (analysis vs. analyst).\n\nThe former case can be handled by using so called n-grams. In contrast to the single words we used for word tokens, n-grams are sequences of n consecutive words in a text, thus capturing some more of the context in a simple way. Let’s see a simple example for 2-grams:\n\nfrom nltk import ngrams\n\nsentence = \"The CEO announced plans to diversify the company's product portfolio...\"\n\nfor n_gram in ngrams(sentence.split(\" \"), n=2):\n  print(n_gram)\n\n('The', 'CEO')\n('CEO', 'announced')\n('announced', 'plans')\n('plans', 'to')\n('to', 'diversify')\n('diversify', 'the')\n('the', \"company's\")\n(\"company's\", 'product')\n('product', 'portfolio...')\n\n\nIn order to detect keywords consisting of more than a single word we can now split our text into n-grams for different n(e.g., 2, 3, 4) and compare these to our keywords.\nIn order to handle the other three cases, we a different approach. So let us first notice that, for all three cases, the word we are trying to compare are very similar (in terms of the contained letters) but not exactly equal. So what if we had a way to define a similarity between words and texts or, more generally, between any strings? One solution for this is fuzzy matching. Instead of considering two strings a match if they are exactly equal, fuzzy matching assigns a score to the pair. If the score is high enough, we might consider the pair a match.\n\n\nSome details about fuzzy matching\n\nFuzzy string matching is a technique used to find strings that are approximately similar to a given pattern, even if there are differences in spelling, punctuation, or word order. It is particularly useful in situations where exact string matching is not feasible due to variations or errors in the data. Fuzzy matching algorithms compute a similarity score between pairs of strings, typically based on criteria such as character similarity, substring matching, or token-based similarity. These algorithms often employ techniques like Levenshtein distance, which measures the minimum number of single-character edits required to transform one string into another, or tokenization to compare sets or sorted versions of tokens. Overall, fuzzy string matching enables the identification of similar strings, facilitating tasks such as record linkage, spell checking, and approximate string matching in various applications, including natural language processing, data cleaning, and information retrieval.\n\nLet’s see how this works using the package rapidfuzz.\n\nfrom rapidfuzz import fuzz\n\nword_pairs = [\n  (\"advertisment\", \"advertising\"),\n  (\"landscpe\", \"landscape\"),\n  (\"analysis\", \"analyst\")\n]\n\nfor word_pair in word_pairs:\n  ratio = fuzz.ratio(\n    s1=word_pair[0], \n    s2=word_pair[1]\n  )\n  print(f\"Similarity score '{word_pair[0]} - '{word_pair[1]}': {round(ratio, 2)}.\")\n\nSimilarity score 'advertisment - 'advertising': 78.26.\nSimilarity score 'landscpe - 'landscape': 94.12.\nSimilarity score 'analysis - 'analyst': 80.0.\n\n\nLet us use fuzzy matching on order to detect some of the missing keywords from the previous example.\n\nfrom pprint import pprint\nfrom nltk.tokenize import wordpunct_tokenize\n\ntokenized_text = wordpunct_tokenize(text=text)\n\nmin_score = 75\n\nmatches = []\nfor token in tokenized_text:\n  for keyword in keywords:\n    ratio = fuzz.ratio(\n      s1=token.lower(), \n      s2=keyword.lower()\n    )\n    if ratio &gt;= min_score:\n      matches.append(\n        (keyword, token, round(ratio, 2))\n      )\n\npprint(matches)\n\n[('Quarter', 'quarterly', 87.5),\n ('Earnings', 'earnings', 100.0),\n ('Report', 'reports', 92.31),\n ('Analysis', 'analysts', 87.5),\n ('Stock', 'stock', 100.0),\n ('Investor', 'investor', 100.0),\n ('Announce', 'announced', 94.12),\n ('Diversity', 'diversify', 88.89),\n ('Market', 'markets', 92.31),\n ('Market', 'marketing', 80.0),\n ('Advertisment', 'advertising', 78.26),\n ('Landscpe', 'landscape', 94.12)]\n\n\nAs we can see we have now successfully found most of the keywords we were looking for. However, we can also see a new caveat: We have now detected two possible matches for Market: Marketing and Markets. In this case, we can simply pick the one with the higher score and we are good, but there will be cases where it is more difficult to decide, whether a match, even with a higher score, is actually a match.\nFuzzy matching can, of course, also be used to compare n-grams or even entire texts to each other (see also the documentation of rapidfuzz and the next exercise); however there are certain limits to how practical it can be. But the concept in general already gives us some good evidence that, in order to compare words and text to each other, we would like to be able to somehow calculate with text. In the next sections, we will see ways how to do that more efficiently.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Fuzzy matching"
    ]
  }
]