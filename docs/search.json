[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Resources",
      "Resource 1"
    ]
  },
  {
    "objectID": "test_viz.html",
    "href": "test_viz.html",
    "title": "",
    "section": "",
    "text": "# prerequisites\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.manifold import TSNE\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n# Define a list of words to visualize\nwords = [\"python\", \"javascript\", \"c++\", \"reptile\", \"snake\"]\n\n# Get embeddings for the words\nresponse = client.embeddings.create(\n    input=words,\n    model=MODEL\n)\n\nembeddings = [emb.embedding for emb in response.data]\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n    n_components=2, \n    random_state=42,\n    perplexity=4 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    plt.scatter(x, y, marker='o', color='red')\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "nlp/exercises/ex_word_matching.html",
    "href": "nlp/exercises/ex_word_matching.html",
    "title": "Exercise: Word matching",
    "section": "",
    "text": "Task: For each element of the following list of keywords, determine whether it is contained in the text.\nInstructions:\n\nTransform the text to lower case and use your (or a) tokenizer to split the text into word tokens.\nFirst, use a simple comparison of strings to check whether the keywords match any token. When does this approach fail?\nLemmatize the tokens from your text in order to handle some more matching cases. When does this approach still fail? Hint: Use the different options for pos in order to handle different types of words such as nouns, verbs etc.\n\n\ntext = \"The company's latest quarterly earnings reports exceeded analysts' expectations, driving up the stock price. However, concerns about future growth prospects weighed on investor sentiment. The CEO announced plans to diversify the company's product portfolio and expand into new markets, aiming to sustain long-term profitability. The marketing team launched a new advertising campaign to promote the company's flagship product, targeting key demographics. Despite challenges in the competitive landscape, the company remains committed to innovation and customer satisfaction.\"\n\n\nkeywords = [\n    \"Announce\", \n    \"Aim\",\n    \"Earnings\",\n    \"Quarter\",\n    \"Report\",\n    \"Investor\",\n    \"Analysis\",\n    \"Market\",\n    \"Diversity\",\n    \"Product portfolio\",\n    \"Advertisment\",\n    \"Stock\",\n    \"Landscpe\" # yes, this is here on purpose\n]\n\n\n\nShow solution\n\n\nfrom pprint import pprint\nfrom nltk.tokenize import wordpunct_tokenize\n\ntext_token = wordpunct_tokenize(text=text.lower())\ndetected_words = [\n    (keyword, keyword.lower() in text_token) for keyword in keywords\n]\npprint(detected_words)\nprint(f\"\\nDetected {sum([x[1] for x in detected_words])}/{len(keywords)} words.\")\n\n[('Announce', False),\n ('Aim', False),\n ('Earnings', True),\n ('Quarter', False),\n ('Report', False),\n ('Investor', True),\n ('Analysis', False),\n ('Market', False),\n ('Diversity', False),\n ('Product portfolio', False),\n ('Advertisment', False),\n ('Stock', True),\n ('Landscpe', False)]\n\nDetected 3/13 words.\n\n\n\nfrom nltk.stem import WordNetLemmatizer\n\nwnl = WordNetLemmatizer()\n\nlemmatized_text_token = [\n    wnl.lemmatize(w) for w in text_token\n]\ndetected_words = [\n    (keyword, keyword.lower() in lemmatized_text_token) for keyword in keywords\n]\npprint(detected_words)\nprint(f\"\\nDetected {sum([x[1] for x in detected_words])}/{len(keywords)} words.\")\n\n[('Announce', False),\n ('Aim', False),\n ('Earnings', True),\n ('Quarter', False),\n ('Report', True),\n ('Investor', True),\n ('Analysis', False),\n ('Market', True),\n ('Diversity', False),\n ('Product portfolio', False),\n ('Advertisment', False),\n ('Stock', True),\n ('Landscpe', False)]\n\nDetected 5/13 words.\n\n\n\nfully_lemmatized_text_token = []\n\nfor token in text_token:\n    lemmatized_token = token\n    for pos in [\"n\", \"v\", \"a\"]:\n        lemmatized_token = wnl.lemmatize(token, pos=pos)\n        \n        fully_lemmatized_text_token.append(lemmatized_token)\n\ndetected_words = [\n    (keyword, keyword.lower() in fully_lemmatized_text_token) for keyword in keywords\n]\npprint(detected_words)    \nprint(f\"\\nDetected {sum([x[1] for x in detected_words])}/{len(keywords)} words.\")  \n        \n\n[('Announce', True),\n ('Aim', True),\n ('Earnings', True),\n ('Quarter', False),\n ('Report', True),\n ('Investor', True),\n ('Analysis', False),\n ('Market', True),\n ('Diversity', False),\n ('Product portfolio', False),\n ('Advertisment', False),\n ('Stock', True),\n ('Landscpe', False)]\n\nDetected 7/13 words.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Exercise: Word matching"
    ]
  },
  {
    "objectID": "nlp/exercises/ex_tokenization.html",
    "href": "nlp/exercises/ex_tokenization.html",
    "title": "Exercise: Tokenization",
    "section": "",
    "text": "# write/extend tokenizer \n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Exercise: Tokenization"
    ]
  },
  {
    "objectID": "nlp/statistical_text_analysis.html",
    "href": "nlp/statistical_text_analysis.html",
    "title": "Statistical text analysis",
    "section": "",
    "text": "Term frequency\nBag of Words -&gt; “Simple” form of embeddings\nExercise: Create a bag of words\nExercise: TF-IDF\nClustering? -&gt; Ref to embeddings later on\nTODO: Add problems with statistical analysis.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Statistical text analysis"
    ]
  },
  {
    "objectID": "nlp/statistical_text_analysis.html#term-frequency",
    "href": "nlp/statistical_text_analysis.html#term-frequency",
    "title": "Statistical text analysis",
    "section": "Term frequency",
    "text": "Term frequency\nSo far we have mainly looked at the analysis of single words/token or n-grams. But what about the analysis of a full text? There are many approaches to this but a good way to get into the topic is a simple statistical analysis of a text. For starters, let’s simply count the number of appearances of each word in a text.\n\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\nfrom collections import Counter\nfrom typing import List\n\nfrom nltk.corpus import stopwords\n# python -m nltk.downloader stopwords -&gt; run this in your console once to get the stopwords\n\n\n# load a text from file\ntext = \"\"\nwith open(\"../assets/chapter1.txt\", \"r\") as file:  \n    for line in file:\n        text += line.strip()\n\n\ndef preprocess_text(text: str) -&gt; List[str]:\n    # tokenize text\n    tokens = wordpunct_tokenize(text.lower())\n\n    # remove punctuation\n    tokens = [t for t in tokens if t not in punctuation]\n\n    # remove stopwords\n    stop_words = stopwords.words(\"english\")\n    tokens = [t for t in tokens if t not in stop_words]\n\n    return tokens\n\n# count the most frequent words\ntokens = preprocess_text(text=text)\n\nfor t in Counter(tokens).most_common(15):\n    print(f\"{t[0]}: {t[1]}\")\n\none: 35\nwinston: 32\nface: 28\neven: 24\n--: 24\nbig: 22\ncould: 19\nparty: 18\nwould: 18\nmoment: 18\nlike: 17\nbrother: 15\ngoldstein: 15\ntelescreen: 14\nseemed: 14\n\n\nJust from the most frequent words, can you guess the text?\nIn many cases, just the simple number of appearances of a token in a text can determine its importance. The concept of counting the term frequency is also known as bag of words.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Statistical text analysis"
    ]
  },
  {
    "objectID": "nlp/statistical_text_analysis.html#bag-of-words",
    "href": "nlp/statistical_text_analysis.html#bag-of-words",
    "title": "Statistical text analysis",
    "section": "Bag of Words",
    "text": "Bag of Words\nIf we do the same with multiple texts, we can build up a vocabulary of words and compare different texts to each other based on the appearance of terms.\n\nfrom collections import Counter\n\n\ndef create_bag_of_words(texts):\n    # Count the frequency of each word in the corpus\n    word_counts = Counter()\n    \n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Update word counts\n        word_counts.update(words)\n    \n    # Create vocabulary by sorting the words based on their frequency\n    vocabulary = [word for word, _ in sorted(word_counts.items())]\n    \n    # Create BoW vectors for each document\n    bow_vectors = []\n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Create a Counter object to count word frequencies\n        bow_vector = Counter(words)\n        \n        # Fill in missing words with zero counts\n        for word in vocabulary:\n            if word not in bow_vector:\n                bow_vector[word] = 0\n\n        # Sort the BoW vector based on the vocabulary order\n        sorted_bow_vector = [bow_vector[word] for word in vocabulary]\n        \n        # Append the BoW vector to the list\n        bow_vectors.append(sorted_bow_vector)\n    \n    return vocabulary, bow_vectors\n\n# Example texts\ntexts = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\",\n]\n\n# Create Bag of Words\nvocabulary, bow_vectors = create_bag_of_words(texts)\n\n# Print vocabulary\nprint(\"Vocabulary:\")\nprint(vocabulary)\n\n# Print BoW vectors\nprint(\"\\nBag of Words Vectors:\")\nfor i, bow_vector in enumerate(bow_vectors):\n    print(f\"Document {i + 1}: {bow_vector}\")\n\nVocabulary:\n['document', 'first', 'one', 'second', 'third']\n\nBag of Words Vectors:\nDocument 1: [1, 1, 0, 0, 0]\nDocument 2: [2, 0, 0, 1, 0]\nDocument 3: [0, 0, 1, 0, 1]\nDocument 4: [1, 1, 0, 0, 0]\n\n\nBag of words actually gives us some vector representation of our texts with respect to the given vocabulary. We can even calculate with these vectors and try to determine a similarity between the texts.\n\nimport numpy as np\n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -&gt; float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\n\nquery = bow_vectors[3]\n\nsimilarities = []\nfor i, bv in enumerate(bow_vectors):\n\n    similarity = cosine_similarity(\n            vec1=query, \n            vec2=bv\n        )\n\n    similarities.append(\n        (texts[i], round(similarity, 2))\n    )\n\nsimilarities\n\n[('This is the first document.', 1.0),\n ('This document is the second document.', 0.63),\n ('And this is the third one.', 0.0),\n ('Is this the first document?', 1.0)]\n\n\nIssues:\n\nVocabulary gets very large\nWords not in vocabulary are an issue\nNo context\nNo structure",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Statistical text analysis"
    ]
  },
  {
    "objectID": "nlp/statistical_text_analysis.html#clustering-of-bow-vectors",
    "href": "nlp/statistical_text_analysis.html#clustering-of-bow-vectors",
    "title": "Statistical text analysis",
    "section": "Clustering of BoW vectors",
    "text": "Clustering of BoW vectors",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Statistical text analysis"
    ]
  },
  {
    "objectID": "nlp/overview.html",
    "href": "nlp/overview.html",
    "title": "Overview of NLP",
    "section": "",
    "text": "In order to understand and appreciate very advanced topics such as Large Language Models, it is often helpful to get a quick overview of the history and how things developed. So let’s get started with a few basics.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Overview of NLP"
    ]
  },
  {
    "objectID": "nlp/overview.html#a-short-history-of-natural-language-processing",
    "href": "nlp/overview.html#a-short-history-of-natural-language-processing",
    "title": "Overview of NLP",
    "section": "A short history of Natural Language Processing",
    "text": "A short history of Natural Language Processing\nThe field of Natural Language Processing (NLP) has undergone a remarkable evolution, spanning decades and driven by the convergence of computer science, artificial intelligence, and linguistics. From its nascent stages to its current state, NLP has witnessed transformative shifts, propelled by groundbreaking research and technological advancements. Today, it stands as a testament to humanity’s quest to bridge the gap between human language and machine comprehension. The journey through NLP’s history offers profound insights into its trajectory and the challenges encountered along the way.\n\nEarly Days: Rule-Based Approaches (1960s-1980s)\nIn its infancy, NLP relied heavily on rule-based approaches, where researchers painstakingly crafted sets of linguistic rules to analyze and manipulate text. This period, spanning from the 1960s to the 1980s, saw significant efforts in tasks such as part-of-speech tagging, named entity recognition, and machine translation. However, rule-based systems struggled to cope with the inherent ambiguity and complexity of natural language. Different languages presented unique challenges, necessitating the development of language-specific rulesets. Despite their limitations, rule-based approaches laid the groundwork for future advancements in NLP.\n\n\nRise of Statistical Methods (1990s-2000s)\nThe 1990s marked a pivotal shift in NLP with the emergence of statistical methods as a viable alternative to rule-based approaches. Researchers began harnessing the power of statistics and probabilistic models to analyze large corpora of text. Techniques like Hidden Markov Models and Conditional Random Fields gained prominence, offering improved performance in tasks such as text classification, sentiment analysis, and information extraction. Statistical methods represented a departure from rigid rule-based systems, allowing for greater flexibility and adaptability. However, they still grappled with the nuances and intricacies of human language, particularly in handling ambiguity and context.\n\n\nMachine Learning Revolution (2010s)\nThe advent of the 2010s witnessed a revolution in NLP fueled by the rise of machine learning, particularly deep learning. With the availability of vast amounts of annotated data and unprecedented computational power, researchers explored neural network architectures tailored for NLP tasks. Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) gained traction, demonstrating impressive capabilities in tasks such as sentiment analysis, text classification, and sequence generation. These models represented a significant leap forward in NLP, enabling more nuanced and context-aware language processing.\n\n\nLarge Language Models: Transformers (2010s-Present)\nThe latter half of the 2010s heralded the rise of large language models, epitomized by the revolutionary Transformer architecture. Powered by self-attention mechanisms, Transformers excel at capturing long-range dependencies in text and generating coherent and contextually relevant responses. Pre-trained on massive text corpora, models like GPT (Generative Pre-trained Transformer) have achieved unprecedented performance across a wide range of NLP tasks, including machine translation, question-answering, and language understanding. Their ability to leverage vast amounts of data and learn intricate patterns has propelled NLP to new heights of sophistication.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Overview of NLP"
    ]
  },
  {
    "objectID": "nlp/overview.html#challenges-in-nlp",
    "href": "nlp/overview.html#challenges-in-nlp",
    "title": "Overview of NLP",
    "section": "Challenges in NLP",
    "text": "Challenges in NLP\nDespite the remarkable progress, NLP grapples with a myriad of challenges that continue to shape its trajectory:\n\nAmbiguity of Language: The inherent ambiguity of natural language poses significant challenges in accurately interpreting meaning, especially in tasks like sentiment analysis and named entity recognition.\nDifferent Languages: NLP systems often struggle with languages other than English, facing variations in syntax, semantics, and cultural nuances, requiring tailored approaches for each language.\nBias: NLP models can perpetuate biases present in the training data, leading to unfair or discriminatory outcomes, particularly in tasks like text classification and machine translation.\nImportance of Context: Understanding context is paramount for NLP tasks, as the meaning of words and phrases can vary drastically depending on the surrounding context.\nWorld Knowledge: NLP systems lack comprehensive world knowledge, hindering their ability to understand references, idioms, and cultural nuances embedded in text.\nCommon Sense Reasoning: Despite advancements, NLP models still struggle with common sense reasoning, often producing nonsensical or irrelevant responses in complex scenarios.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Overview of NLP"
    ]
  },
  {
    "objectID": "nlp/overview.html#classic-nlp-tasksapplications",
    "href": "nlp/overview.html#classic-nlp-tasksapplications",
    "title": "Overview of NLP",
    "section": "Classic NLP tasks/applications",
    "text": "Classic NLP tasks/applications\n\nPart-of-Speech Tagging\nPart-of-speech tagging involves labeling each word in a sentence with its corresponding grammatical category, such as noun, verb, adjective, or adverb. For example, in the sentence “The cat is sleeping,” part-of-speech tagging would identify “cat” as a noun and “sleeping” as a verb. This task is crucial for many NLP applications, including language understanding, information retrieval, and machine translation. Accurate part-of-speech tagging lays the foundation for deeper linguistic analysis and improves the performance of downstream tasks.\n\n\nCode example\n\n\nimport spacy\n\n# Load the English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example text\ntext = \"The sun sets behind the mountains, casting a golden glow across the sky.\"\n\n# Process the text with spaCy\ndoc = nlp(text)\n\n# Find the maximum length of token text and POS tag\nmax_token_length = max(len(token.text) for token in doc)\nmax_pos_length = max(len(token.pos_) for token in doc)\n\n# Print each token along with its part-of-speech tag\nfor token in doc:\n    print(f\"Token: {token.text.ljust(max_token_length)} | POS Tag: {token.pos_.ljust(max_pos_length)}\")\n\nToken: The       | POS Tag: DET  \nToken: sun       | POS Tag: NOUN \nToken: sets      | POS Tag: VERB \nToken: behind    | POS Tag: ADP  \nToken: the       | POS Tag: DET  \nToken: mountains | POS Tag: NOUN \nToken: ,         | POS Tag: PUNCT\nToken: casting   | POS Tag: VERB \nToken: a         | POS Tag: DET  \nToken: golden    | POS Tag: ADJ  \nToken: glow      | POS Tag: NOUN \nToken: across    | POS Tag: ADP  \nToken: the       | POS Tag: DET  \nToken: sky       | POS Tag: NOUN \nToken: .         | POS Tag: PUNCT\n\n\n\n\n\nNamed Entity Recognition\nNamed Entity Recognition (NER) involves identifying and classifying named entities in text, such as people, organizations, locations, dates, and more. For instance, in the sentence “Apple is headquartered in Cupertino,” NER would identify “Apple” as an organization and “Cupertino” as a location. NER is essential for various applications, including information retrieval, document summarization, and question-answering systems. Accurate NER enables machines to extract meaningful information from unstructured text data.\n\n\nCode example\n\n\nimport spacy\n\n# Load the English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example text\ntext = \"Apple is considering buying a startup called U.K. based company in London for $1 billion.\"\n\n# Process the text with spaCy\ndoc = nlp(text)\n\n# Print each token along with its Named Entity label\nfor ent in doc.ents:\n    print(f\"Entity: {ent.text.ljust(20)} | Label: {ent.label_}\")\n\nEntity: Apple                | Label: ORG\nEntity: U.K.                 | Label: GPE\nEntity: London               | Label: GPE\nEntity: $1 billion           | Label: MONEY\n\n\n\n\n\nMachine Translation\nMachine Translation (MT) aims to automatically translate text from one language to another, facilitating communication across language barriers. For example, translating a sentence from English to Spanish or vice versa. MT systems utilize sophisticated algorithms and linguistic models to generate accurate translations while preserving the original meaning and nuances of the text. MT has numerous practical applications, including cross-border communication, localization of software and content, and global commerce.\n\n\nSentiment Analysis\nSentiment Analysis involves analyzing text data to determine the sentiment or opinion expressed within it, such as positive, negative, or neutral. For instance, analyzing product reviews to gauge customer satisfaction or monitoring social media sentiment towards a brand. Sentiment Analysis employs machine learning algorithms to classify text based on sentiment, enabling businesses to understand customer feedback, track public opinion, and make data-driven decisions.\n\n\nCode example\n\n\n# python -m textblob.download_corpora\n\nfrom textblob import TextBlob\n\n# Example text\ntext = \"I love TextBlob! It's an amazing library for natural language processing.\"\n\n# Perform sentiment analysis with TextBlob\nblob = TextBlob(text)\nsentiment_score = blob.sentiment.polarity\n\n# Determine sentiment label based on sentiment score\nif sentiment_score &gt; 0:\n    sentiment_label = \"Positive\"\nelif sentiment_score &lt; 0:\n    sentiment_label = \"Negative\"\nelse:\n    sentiment_label = \"Neutral\"\n\n# Print sentiment analysis results\nprint(f\"Text: {text}\")\nprint(f\"Sentiment Score: {sentiment_score:.2f}\")\nprint(f\"Sentiment Label: {sentiment_label}\")\n\nText: I love TextBlob! It's an amazing library for natural language processing.\nSentiment Score: 0.44\nSentiment Label: Positive\n\n\n\n\n\nText Classification\nText Classification is the task of automatically categorizing text documents into predefined categories or classes. For example, classifying news articles into topics like politics, sports, or entertainment. Text Classification is widely used in various domains, including email spam detection, sentiment analysis, and content categorization. It enables organizations to organize and process large volumes of textual data efficiently, leading to improved decision-making and information retrieval.\n\n\nCode example\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder\n\n# Example labeled dataset\ntexts = [\n    \"I love this product!\",\n    \"This product is terrible.\",\n    \"Great service, highly recommended.\",\n    \"I had a bad experience with this company.\",\n]\nlabels = [\n    \"Positive\",\n    \"Negative\",\n    \"Positive\",\n    \"Negative\",\n]\n\n# Create a TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n\n# Encode labels as integers\nlabel_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(labels)\n\n# Create a pipeline with TF-IDF vectorizer and SVM classifier\nclassifier = make_pipeline(vectorizer, SVC(kernel='linear'))\n\n# Train the classifier\nclassifier.fit(texts, encoded_labels)\n\n# Example test text\ntest_text = \"This product exceeded my expectations.\"\n\n# Predict the label for the test text\npredicted_label = classifier.predict([test_text])[0]\n\n# Decode the predicted label back to original label\npredicted_label_text = label_encoder.inverse_transform([predicted_label])[0]\n\n# Print the predicted label\nprint(f\"Text: {test_text}\")\nprint(f\"Predicted Label: {predicted_label_text}\")\n\nText: This product exceeded my expectations.\nPredicted Label: Negative\n\n\n\n\n\nInformation Extraction\nInformation Extraction involves automatically extracting structured information from unstructured text data, such as documents, articles, or web pages. This includes identifying entities, relationships, and events mentioned in the text. For example, extracting names of people mentioned in news articles or detecting company acquisitions from financial reports. Information Extraction plays a crucial role in tasks like knowledge base construction, data integration, and business intelligence.\n\n\nQuestion-Answering\nQuestion-Answering (QA) systems aim to automatically generate accurate answers to user queries posed in natural language. These systems comprehend the meaning of questions and retrieve relevant information from a knowledge base or text corpus to provide precise responses. For example, answering factual questions like “Who is the president of the United States?” or “What is the capital of France?”. QA systems are essential for information retrieval, virtual assistants, and educational applications, enabling users to access information quickly and efficiently.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Overview of NLP"
    ]
  },
  {
    "objectID": "ethics/data_privacy.html",
    "href": "ethics/data_privacy.html",
    "title": "Data Privacy",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Seminar",
      "Ethical Considerations",
      "Data Privacy"
    ]
  },
  {
    "objectID": "about/projects.html",
    "href": "about/projects.html",
    "title": "Projects",
    "section": "",
    "text": "In the final part of the seminar we are going to tackle our very own projects involving a language model. At best, you find your ideas and work on them, maybe you even have a work-related application mind. The following list can serve as inspiration.\n\nProject ideas\n\nQuestion-Answering Chatbot: Build a chatbot that can answer questions posed by users on a specific topic provided in form of documents. Users input their questions, the chatbot retrieves relevant information from a pre-defined set of documents, and uses the information to answer the question.\nDocument tagging / classification: Use GPT and its tools (e.g., function calls) and/or embeddings to classify documents or assign tags to them. Example: Sort bug reports or complaints into categories depending on the problem.\nClustering of text-based entities: Create a small tool that can cluster text-based entities based on embeddings, for example, groups of texts or keywords. Example: Structure a folder of text files based on their content.\nText-based RPG Game: Develop a text-based role-playing game where players interact with characters and navigate through a story generated by GPT. Players make choices that influence the direction of the narrative.\nSentiment Analysis Tool: Build an app that analyzes the sentiment of text inputs (e.g., social media posts, customer reviews) using GPT. Users can input text, and the app provides insights into the overall sentiment expressed in the text.\nText Summarization Tool: Create an application that summarizes long blocks of text into shorter, concise summaries. Users can input articles, essays, or documents, and the tool generates a summarized version.\nLanguage Translation Tool: Build a simple translation app that utilizes GPT to translate text between different languages. Users can input text in one language, and the app outputs the translated text in the desired language. Has to include some nice tweaks.\nPersonalized Recipe Generator: Develop an app that generates personalized recipes based on user preferences and dietary restrictions. Users input their preferred ingredients and dietary needs, and the app generates custom recipes using GPT.\nLyrics Generator: Create a lyrics generation tool that generates lyrics based on user input such as themes, music style, emotions, or keywords. Users can explore different poetic styles and themes generated by GPT.\n\n\n\nProject setup\nTODO: Describe the idea of Dash and the app in Jupyterlab.\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "About",
      "Projects"
    ]
  },
  {
    "objectID": "embeddings/exercises/ex_emb_similarity.html",
    "href": "embeddings/exercises/ex_emb_similarity.html",
    "title": "Exercise: Embedding similarity",
    "section": "",
    "text": "Task: Use the OpenAI embeddings API to compute the similarity between two given words or phrases.\nInstructions:\n\nChoose two words or phrases with similar or related meanings.\nUse the OpenAI embeddings API to obtain embeddings for both words or phrases.\nCalculate the cosine similarity between the embeddings to measure their similarity.\nPrint the similarity score and interpret the results.\n\n\n\nShow solution\n\n\n\nCode\nimport numpy as np\n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -&gt; float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\n\n\n\nCode\nimport os\n\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n# create the embeddings\nword_1 = \"king\"\nword_2 = \"queen\"\n\nresponse_1 = client.embeddings.create(input=word_1, model=MODEL)\nembedding_1 = response_1.data[0].embedding\nresponse_2 = client.embeddings.create(input=word_2, model=MODEL)\nembedding_2 = response_2.data[0].embedding\n\n\n\n\nCode\n# calculate the distance \ndist_12 = cosine_similarity(embedding_1, embedding_2)\nprint(f\"Cosine similarity between {word_1} and {word_2}: {round(dist_12, 3)}.\")\n\n\nCosine similarity between king and queen: 0.915.\n\n\n\n\nCode\nword_3 = \"pawn\"\nembedding_3 = client.embeddings.create(input=word_3, model=MODEL).data[0].embedding\n\ndist_13 = cosine_similarity(embedding_1, embedding_3)\nprint(f\"Cosine similarity between {word_1} and {word_3}: {round(dist_13, 3)}.\")\n\n\nCosine similarity between king and pawn: 0.829.\n\n\n\nTask: Use the OpenAI embeddings API and simple embedding arithmetics to introduce more context to word similarities.\nInstructions:\n\nCreate embeddings for the following three words: python, snake, javascript using the OpenAI API.\nCalculate the cosine similarity between each pair.\nCreate another embedding for the word reptile and add it to python. You can use numpy for this.\nCalculate the cosine similarity between python and this sum. What do you notice?\n\n\n\nShow solution\n\n\n\nCode\nwords = [\"python\", \"snake\", \"javascript\", \"reptile\"]\nresponse = client.embeddings.create(input=words, model=MODEL)\nembeddings = [emb.embedding for emb in response.data]\n\n\n\n\nCode\nprint(f\"Similarity between '{words[0]}' and '{words[1]}': {round(cosine_similarity(embeddings[0], embeddings[1]), 3)}.\")\nprint(f\"Similarity between '{words[0]}' and '{words[2]}': {round(cosine_similarity(embeddings[0], embeddings[2]), 3)}.\")\nprint(f\"Similarity between '{words[0]} + {words[3]}' and '{words[1]}': {round(cosine_similarity(np.array(embeddings[0]) + np.array(embeddings[3]), embeddings[1]), 3)}.\")\n\n\nSimilarity between 'python' and 'snake': 0.841.\nSimilarity between 'python' and 'javascript': 0.85.\nSimilarity between 'python + reptile' and 'snake': 0.894.\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Exercise: Embedding similarity"
    ]
  },
  {
    "objectID": "embeddings/visualization.html",
    "href": "embeddings/visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "# prerequisites\n\nimport os\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n\n# Define a list of words to visualize\nwords = [\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"banana\", \"grapes\", \"cat\", \"dog\", \"happy\", \"sad\"]\n\n# Get embeddings for the words\nresponse = client.embeddings.create(\n    input=words,\n    model=MODEL\n)\n\nembeddings = [emb.embedding for emb in response.data]\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n    n_components=2, \n    random_state=42,\n    perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    plt.scatter(x, y, marker='o', color='red')\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\n\n\nt-SNE visualization of word embeddings\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Visualization"
    ]
  },
  {
    "objectID": "embeddings/clustering.html",
    "href": "embeddings/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "# prerequisites\n\nimport os\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value\n\n# get the OpenAI client\nclient = get_openai_client(\n  model=MODEL,\n  config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n\n# Define a list of words to cluster\nwords = [\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"banana\", \"grapes\", \"cat\", \"dog\", \"happy\", \"sad\"]\n\n# Get embeddings for the words\nresponse = client.embeddings.create(\n  input=words,\n  model=MODEL\n)\n\nembeddings = [emb.embedding for emb in response.data]\n\n\n# do the clustering\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nn_clusters = 5\n\n# define the model\nkmeans = KMeans(\n  n_clusters=n_clusters,\n  n_init=\"auto\",\n  random_state=2 # do this to get the same output\n)\n\n# fit the model to the data\nkmeans.fit(np.array(embeddings))\n\n# get the cluster labels\ncluster_labels = kmeans.labels_\n\n::: {#cell-tsne-visualization of clustering .cell execution_count=5}\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n  n_components=2, \n  random_state=42,\n  perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Define a color map for clusters\ncolors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    cluster_label = cluster_labels[i]\n    color = colors[cluster_label]\n    plt.scatter(x, y, marker='o', color=color)\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\n\n\nt-SNE visualization of clustering word embeddings\n\n\n\n:::\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Clustering"
    ]
  },
  {
    "objectID": "llm/exercises/ex_gpt_parameterization.html",
    "href": "llm/exercises/ex_gpt_parameterization.html",
    "title": "Exercise: GPT Parameterization",
    "section": "",
    "text": "Task: Explore the parameterization possibilities of the OpenAI API for GPT.\nInstructions:\nSome possibilities are:\n\nUse the system role in order to give instructions to the language model before the interaction with the user starts in order to change the response style of the model.\nChange the temparature or top_p parameters and explore the effect on your prompts.\nUse the\n\n\nimport os\nfrom llm_utils.client import get_openai_client\n\nMODEL = \"gpt4\"\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n\n# here goes your code\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Exercise: GPT Parameterization"
    ]
  },
  {
    "objectID": "llm/exercises/ex_gpt_chatbot.html",
    "href": "llm/exercises/ex_gpt_chatbot.html",
    "title": "Exercise: GPT Chatbot",
    "section": "",
    "text": "Task: Create a simple chatbot using the OpenAI chat.completions API.\nInstructions:\n\nUse the chat.completions API to send prompts to GPT, receive the answers and displaying them.\nStop the conversation when the user inputs the word exit instead of a new prompt.\nHint: Remember that GPT has no memory, so you always have to include the previous conversation in your prompts.\n\n\n\nShow solution\n\n\nimport os\nfrom llm_utils.client import get_openai_client\n\nMODEL = \"gpt4\"\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n\nclass ChatGPT:\n    def __init__(self, model=MODEL):\n        self.model = model\n        self.client = client\n        self.messages = []\n\n    def chat_with_gpt(self, user_input: str):\n        self.messages.append({\n            \"role\": \"user\",\n            \"content\": user_input\n        })\n        response = self._generate_response(self.messages)\n        return response\n\n    def _generate_response(self, messages):\n        response = self.client.chat.completions.create(\n            model=self.model,\n            messages=messages,        \n            temperature=0.2, \n            max_tokens=150,\n            top_p=1.0\n        )\n        response_message = response.choices[0].message\n        self.messages.append({\n            \"role\": response_message.role,\n            \"content\": response_message.content\n        })\n\n        return response_message.content\n\n\n# Conversation loop\nchat_gpt = ChatGPT(model=\"gpt4\")\n\nwhile True:\n    user_input = input(\"User: \")\n\n    if user_input.lower() == 'exit':\n        break\n    \n    print(\"User:\", user_input)\n    \n    # Get bot response based on user input\n    bot_response = chat_gpt.chat_with_gpt(user_input)\n\n    print(\"Bot:\", bot_response)\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Exercise: GPT Chatbot"
    ]
  },
  {
    "objectID": "llm/gpt.html",
    "href": "llm/gpt.html",
    "title": "GPT",
    "section": "",
    "text": "Definition of GPT: GPT is a state-of-the-art large language model developed by OpenAI. It belongs to the family of Transformer-based architectures and is renowned for its ability to generate coherent and contextually relevant text across a wide range of tasks.\nKey Features of GPT: Highlight the key features that distinguish GPT from other LLMs, such as its autoregressive nature, the use of self-attention mechanisms, and the ability to generate text of variable length.\nPre-training Objective: GPT is pre-trained using an unsupervised learning objective known as language modeling. During pre-training, it learns to predict the next word in a sequence based on the preceding context, which enables it to capture the statistical properties of natural language.\nArchitecture of GPT: Provide an overview of the architecture of GPT, which consists of multiple layers of Transformer blocks. Each block includes self-attention layers, feed-forward neural networks, and layer normalization, allowing GPT to process input sequences and generate output sequences effectively.\nFine-tuning and Adaptation: GPT can be fine-tuned on specific tasks or domains with labeled data to adapt its pre-trained knowledge to new tasks. This fine-tuning process allows GPT to achieve state-of-the-art performance on a wide range of natural language processing tasks.\nApplications of GPT: Discuss the diverse applications of GPT across various domains, including text generation, summarization, translation, question-answering, conversation generation, and more. Highlight real-world examples and use cases where GPT has been deployed successfully.\nRecent Advancements and Versions: Mention the evolution of GPT over time, including the release of different versions such as GPT-1, GPT-2, GPT-3, and any subsequent versions or variants. Discuss the improvements and advancements introduced in each iteration.\nChallenges and Limitations: Acknowledge the challenges and limitations associated with GPT, such as the potential for generating biased or inappropriate content, the need for large-scale computational resources, and the difficulty of fine-tuning for specific tasks without overfitting.",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "GPT"
    ]
  },
  {
    "objectID": "llm/gpt.html#a-simple-introduction-to-gpt",
    "href": "llm/gpt.html#a-simple-introduction-to-gpt",
    "title": "GPT",
    "section": "",
    "text": "Definition of GPT: GPT is a state-of-the-art large language model developed by OpenAI. It belongs to the family of Transformer-based architectures and is renowned for its ability to generate coherent and contextually relevant text across a wide range of tasks.\nKey Features of GPT: Highlight the key features that distinguish GPT from other LLMs, such as its autoregressive nature, the use of self-attention mechanisms, and the ability to generate text of variable length.\nPre-training Objective: GPT is pre-trained using an unsupervised learning objective known as language modeling. During pre-training, it learns to predict the next word in a sequence based on the preceding context, which enables it to capture the statistical properties of natural language.\nArchitecture of GPT: Provide an overview of the architecture of GPT, which consists of multiple layers of Transformer blocks. Each block includes self-attention layers, feed-forward neural networks, and layer normalization, allowing GPT to process input sequences and generate output sequences effectively.\nFine-tuning and Adaptation: GPT can be fine-tuned on specific tasks or domains with labeled data to adapt its pre-trained knowledge to new tasks. This fine-tuning process allows GPT to achieve state-of-the-art performance on a wide range of natural language processing tasks.\nApplications of GPT: Discuss the diverse applications of GPT across various domains, including text generation, summarization, translation, question-answering, conversation generation, and more. Highlight real-world examples and use cases where GPT has been deployed successfully.\nRecent Advancements and Versions: Mention the evolution of GPT over time, including the release of different versions such as GPT-1, GPT-2, GPT-3, and any subsequent versions or variants. Discuss the improvements and advancements introduced in each iteration.\nChallenges and Limitations: Acknowledge the challenges and limitations associated with GPT, such as the potential for generating biased or inappropriate content, the need for large-scale computational resources, and the difficulty of fine-tuning for specific tasks without overfitting.",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "GPT"
    ]
  },
  {
    "objectID": "llm/gpt.html#completions-and-how-they-work",
    "href": "llm/gpt.html#completions-and-how-they-work",
    "title": "GPT",
    "section": "Completions and how they work",
    "text": "Completions and how they work\n\n1. Prompt:\nThe prompt serves as the cornerstone of completion generation, acting as the initial input or context upon which the model bases its predictions and generates completions. Its significance lies in its ability to set the tone, theme, and direction for the subsequent text generation process. Prompts can vary widely in length and complexity, ranging from concise prompts that elicit specific responses to more extensive prompts that allow for nuanced and detailed completions. The effectiveness of the prompt in guiding the completion generation process depends on its clarity, relevance, and specificity to the desired task or objective.\n\n\n2. Model Architecture:\nCompletions derive their power from sophisticated machine learning models, with transformer-based architectures like GPT (Generative Pre-trained Transformer) leading the forefront. These models undergo extensive training on vast amounts of text data, spanning diverse domains and languages, to develop a deep understanding of human language. Through this training process, the models learn to capture the intricacies of grammar, syntax, semantics, and context inherent in natural language. The architecture of these models is designed to efficiently process and analyze input text, enabling them to capture long-range dependencies within text and generate coherent completions that align with the provided prompt.\n\n\n3. Tokenization:\nBefore processing the prompt and generating completions, the input text undergoes tokenization, a crucial preprocessing step that breaks it down into smaller units known as tokens. These tokens typically represent words or subwords and serve as the fundamental building blocks for the model’s understanding of the text. Tokenization enables the model to analyze the underlying structure of the text at a granular level, facilitating more effective learning and prediction. Each token encapsulates a discrete unit of meaning within the text and serves as input to the model during the completion generation process.\n\n\n4. Probability Distribution:\nCentral to the completion generation process is the prediction of the likelihood of each possible token that could follow the prompt. This prediction is based on the model’s learned parameters and contextual understanding of the input text. The model computes a probability distribution over the vocabulary of tokens, assigning a probability score to each token to indicate its likelihood of occurrence given the context provided by the prompt. This probability distribution guides the selection of tokens during the completion generation process, ensuring that the generated completions are coherent and contextually relevant.\n\n\n5. Sampling Strategy:\nTo generate completions, the model employs various sampling strategies to select tokens from the probability distribution. Greedy sampling, for example, selects the token with the highest probability at each step, favoring the most probable tokens but potentially leading to repetitive or predictable completions. In contrast, random sampling randomly selects tokens according to their probabilities, introducing variability and unpredictability into the generated completions. Top-k sampling restricts token selection to the top-k most probable tokens, striking a balance between diversity and coherence in the completions. Each sampling strategy offers unique trade-offs in terms of diversity, coherence, and computational efficiency, allowing users to tailor the completion generation process to their specific needs and preferences.\n\n\nConclusion:\nCompletions represent a sophisticated approach to natural language processing, leveraging advanced machine learning models and algorithms to generate coherent and contextually relevant text based on given input. By understanding the underlying components and mechanisms of completions, users can harness their power to develop innovative applications and solutions across a wide range of domains and use cases. As research in NLP continues to advance, the capabilities and applications of completions are expected to evolve, driving further innovation and exploration in the field of human-computer interaction.",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "GPT"
    ]
  },
  {
    "objectID": "llm/gpt_api.html",
    "href": "llm/gpt_api.html",
    "title": "The OpenAI API",
    "section": "",
    "text": "Note\n\n\n\nResource: OpenAI API docs\n\n\nLet’s get started with the OpenAI API for GPT.\n\nAuthentication\nGetting started with the OpenAI Chat Completions API requires signing up for an account on the OpenAI platform. Once you’ve registered, you’ll gain access to an API key, which serves as a unique identifier for your application to authenticate requests to the API. This key is essential for ensuring secure communication between your application and OpenAI’s servers. Without proper authentication, your requests will be rejected. You can create your own account, but for the seminar we will provide the client with the credential within the Jupyterlab (TODO: Link).\n\n# setting up the client in Python\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\")\n)\n\n\n\nRequesting Completions\nMost interaction with GPT and other models consist in generating completions for certain tasks (TODO: Link to completions)\nTo request completions from the OpenAI API, we use Python to send HTTP requests to the designated API endpoint. These requests are structured to include various parameters that guide the generation of text completions. The most fundamental parameter is the prompt text, which sets the context for the completion. Additionally, you can specify the desired model configuration, such as the engine to use (e.g., “gpt-4”), as well as any constraints or preferences for the generated completions, such as the maximum number of tokens or the temperature for controlling creativity (TODO: Link parameterization)\n\n# creating a completion\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How old is the earth?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\"\n)\n\n\n\nProcessing\nOnce the OpenAI API receives your request, it proceeds to process the provided prompt using the specified model. This process involves analyzing the context provided by the prompt and leveraging the model’s pre-trained knowledge to generate text completions. The model employs advanced natural language processing techniques to ensure that the generated completions are coherent and contextually relevant. By drawing from its extensive training data and understanding of human language, the model aims to produce responses that closely align with human-like communication.\n\n\nResponse\nAfter processing your request, the OpenAI API returns a JSON-formatted response containing the generated text completions. Depending on the specifics of your request, you may receive multiple completions, each accompanied by additional information such as a confidence score indicating the model’s level of certainty in the generated text. This response provides valuable insights into the quality and relevance of the completions, allowing you to tailor your application’s behavior accordingly.\n\n\nError Handling\nWhile interacting with the OpenAI API, it’s crucial to implement robust error handling mechanisms to gracefully manage any potential issues that may arise. Common errors include providing invalid parameters, experiencing authentication failures due to an incorrect API key, or encountering rate limiting restrictions. B y handling errors effectively, you can ensure the reliability and resilience of your application, minimizing disruptions to the user experience and maintaining smooth operation under varying conditions. Implementing proper error handling practices is essential for building robust and dependable applications that leverage the capabilities of the OpenAI Chat Completions API effectively.\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "The OpenAI API"
    ]
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/idna-3.6.dist-info/LICENSE.html",
    "href": "script_venv/lib/python3.8/site-packages/idna-3.6.dist-info/LICENSE.html",
    "title": "",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2023, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "href": "script_venv/lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "title": "UAT for NbAgg backend.",
    "section": "",
    "text": "from imp import reload\nThe first line simply reloads matplotlib, uses the nbagg backend and then reloads the backend, just to ensure we have the latest modification to the backend code. Note: The underlying JavaScript will not be updated by this process, so a refresh of the browser after clearing the output and saving is necessary to clear everything fully.\nimport matplotlib\nreload(matplotlib)\n\nmatplotlib.use('nbagg')\n\nimport matplotlib.backends.backend_nbagg\nreload(matplotlib.backends.backend_nbagg)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "href": "script_venv/lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "title": "UAT for NbAgg backend.",
    "section": "UAT 13 - Animation",
    "text": "UAT 13 - Animation\nThe following should generate an animated line:\n\nimport matplotlib.animation as animation\nimport numpy as np\n\nfig, ax = plt.subplots()\n\nx = np.arange(0, 2*np.pi, 0.01)        # x-array\nline, = ax.plot(x, np.sin(x))\n\ndef animate(i):\n    line.set_ydata(np.sin(x+i/10.0))  # update the data\n    return line,\n\n#Init only required for blitting to give a clean slate.\ndef init():\n    line.set_ydata(np.ma.array(x, mask=True))\n    return line,\n\nani = animation.FuncAnimation(fig, animate, np.arange(1, 200), init_func=init,\n                              interval=100., blit=True)\nplt.show()\n\n\nUAT 14 - Keyboard shortcuts in IPython after close of figure\nAfter closing the previous figure (with the close button above the figure) the IPython keyboard shortcuts should still function.\n\n\nUAT 15 - Figure face colours\nThe nbagg honours all colours apart from that of the figure.patch. The two plots below should produce a figure with a red background. There should be no yellow figure.\n\nimport matplotlib\nmatplotlib.rcParams.update({'figure.facecolor': 'red',\n                            'savefig.facecolor': 'yellow'})\nplt.figure()\nplt.plot([3, 2, 1])\n\nplt.show()\n\n\n\nUAT 16 - Events\nPressing any keyboard key or mouse button (or scrolling) should cycle the line while the figure has focus. The figure should have focus by default when it is created and re-gain it by clicking on the canvas. Clicking anywhere outside of the figure should release focus, but moving the mouse out of the figure should not release focus.\n\nimport itertools\nfig, ax = plt.subplots()\nx = np.linspace(0,10,10000)\ny = np.sin(x)\nln, = ax.plot(x,y)\nevt = []\ncolors = iter(itertools.cycle(['r', 'g', 'b', 'k', 'c']))\ndef on_event(event):\n    if event.name.startswith('key'):\n        fig.suptitle('%s: %s' % (event.name, event.key))\n    elif event.name == 'scroll_event':\n        fig.suptitle('%s: %s' % (event.name, event.step))\n    else:\n        fig.suptitle('%s: %s' % (event.name, event.button))\n    evt.append(event)\n    ln.set_color(next(colors))\n    fig.canvas.draw()\n    fig.canvas.draw_idle()\n\nfig.canvas.mpl_connect('button_press_event', on_event)\nfig.canvas.mpl_connect('button_release_event', on_event)\nfig.canvas.mpl_connect('scroll_event', on_event)\nfig.canvas.mpl_connect('key_press_event', on_event)\nfig.canvas.mpl_connect('key_release_event', on_event)\n\nplt.show()\n\n\n\nUAT 17 - Timers\nSingle-shot timers follow a completely different code path in the nbagg backend than regular timers (such as those used in the animation example above.) The next set of tests ensures that both “regular” and “single-shot” timers work properly.\nThe following should show a simple clock that updates twice a second:\n\nimport time\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\n\ndef update(text):\n    text.set(text=time.ctime())\n    text.axes.figure.canvas.draw()\n    \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\ntimer.start()\nplt.show()\n\nHowever, the following should only update once and then stop:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center') \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\n\nplt.show()\n\nAnd the next two examples should never show any visible text at all:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\n\nUAT 18 - stopping figure when removed from DOM\nWhen the div that contains from the figure is removed from the DOM the figure should shut down it’s comm, and if the python-side figure has no more active comms, it should destroy the figure. Repeatedly running the cell below should always have the same figure number\n\nfig, ax = plt.subplots()\nax.plot(range(5))\nplt.show()\n\nRunning the cell below will re-show the figure. After this, re-running the cell above should result in a new figure number.\n\nfig.canvas.manager.reshow()\n\n\n\nUAT 19 - Blitting\nClicking on the figure should plot a green horizontal line moving up the axes.\n\nimport itertools\n\ncnt = itertools.count()\nbg = None\n\ndef onclick_handle(event):\n    \"\"\"Should draw elevating green line on each mouse click\"\"\"\n    global bg\n    if bg is None:\n        bg = ax.figure.canvas.copy_from_bbox(ax.bbox) \n    ax.figure.canvas.restore_region(bg)\n\n    cur_y = (next(cnt) % 10) * 0.1\n    ln.set_ydata([cur_y, cur_y])\n    ax.draw_artist(ln)\n    ax.figure.canvas.blit(ax.bbox)\n\nfig, ax = plt.subplots()\nax.plot([0, 1], [0, 1], 'r')\nln, = ax.plot([0, 1], [0, 0], 'g', animated=True)\nplt.show()\nax.figure.canvas.draw()\n\nax.figure.canvas.mpl_connect('button_press_event', onclick_handle)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/pyzmq-25.1.2.dist-info/AUTHORS.html",
    "href": "script_venv/lib/python3.8/site-packages/pyzmq-25.1.2.dist-info/AUTHORS.html",
    "title": "",
    "section": "",
    "text": "This project was started and continues to be led by Brian E. Granger (ellisonbg AT gmail DOT com). Min Ragan-Kelley (benjaminrk AT gmail DOT com) is the primary developer of pyzmq at this time.\nThe following people have contributed to the project:\n\nAlexander Else (alexander DOT else AT team DOT telstra DOT com)\nAlexander Pyhalov (apyhalov AT gmail DOT com)\nAlexandr Emelin (frvzmb AT gmail DOT com)\nAmr Ali (amr AT ledgerx DOT com)\nAndre Caron (andre DOT l DOT caron AT gmail DOT com)\nAndrea Crotti (andrea DOT crotti DOT 0 AT gmail DOT com)\nAndrew Gwozdziewycz (git AT apgwoz DOT com)\nBaptiste Lepilleur (baptiste DOT lepilleur AT gmail DOT com)\nBrandyn A. White (bwhite AT dappervision DOT com)\nBrian E. Granger (ellisonbg AT gmail DOT com)\nBrian Hoffman (hoffman_brian AT bah DOT com)\nCarlos A. Rocha (carlos DOT rocha AT gmail DOT com)\nChris Laws (clawsicus AT gmail DOT com)\nChristian Wyglendowski (christian AT bu DOT mp)\nChristoph Gohlke (cgohlke AT uci DOT edu)\nCurtis (curtis AT tinbrain DOT net)\nCyril Holweck (cyril DOT holweck AT free DOT fr)\nDan Colish (dcolish AT gmail DOT com)\nDaniel Lundin (dln AT eintr DOT org)\nDaniel Truemper (truemped AT googlemail DOT com)\nDouglas Creager (douglas DOT creager AT redjack DOT com)\nEduardo Stalinho (eduardooc DOT 86 AT gmail DOT com)\nEren Güven (erenguven0 AT gmail DOT com)\nErick Tryzelaar (erick DOT tryzelaar AT gmail DOT com)\nErik Tollerud (erik DOT tollerud AT gmail DOT com)\nFELD Boris (lothiraldan AT gmail DOT com)\nFantix King (fantix DOT king AT gmail DOT com)\nFelipe Cruz (felipecruz AT loogica DOT net)\nFernando Perez (Fernando DOT Perez AT berkeley DOT edu)\nFrank Wiles (frank AT revsys DOT com)\nFélix-Antoine Fortin (felix DOT antoine DOT fortin AT gmail DOT com)\nGavrie Philipson (gavriep AT il DOT ibm DOT com)\nGodefroid Chapelle (gotcha AT bubblenet DOT be)\nGreg Banks (gbanks AT mybasis DOT com)\nGreg Ward (greg AT gerg DOT ca)\nGuido Goldstein (github AT a-nugget DOT de)\nIan Lee (IanLee1521 AT gmail DOT com)\nIonuț Arțăriși (ionut AT artarisi DOT eu)\nIvo Danihelka (ivo AT danihelka DOT net)\nIyed (iyed DOT bennour AT gmail DOT com)\nJim Garrison (jim AT garrison DOT cc)\nJohn Gallagher (johnkgallagher AT gmail DOT com)\nJulian Taylor (jtaylor DOT debian AT googlemail DOT com)\nJustin Bronder (jsbronder AT gmail DOT com)\nJustin Riley (justin DOT t DOT riley AT gmail DOT com)\nMarc Abramowitz (marc AT marc-abramowitz DOT com)\nMatthew Aburn (mattja6 AT gmail DOT com)\nMichel Pelletier (pelletier DOT michel AT gmail DOT com)\nMichel Zou (xantares09 AT hotmail DOT com)\nMin Ragan-Kelley (benjaminrk AT gmail DOT com)\nNell Hardcastle (nell AT dev-nell DOT com)\nNicholas Pilkington (nicholas DOT pilkington AT gmail DOT com)\nNicholas Piël (nicholas AT nichol DOT as)\nNick Pellegrino (npellegrino AT mozilla DOT com)\nNicolas Delaby (nicolas DOT delaby AT ezeep DOT com)\nOndrej Certik (ondrej AT certik DOT cz)\nPaul Colomiets (paul AT colomiets DOT name)\nPawel Jasinski (pawel DOT jasinski AT gmail DOT com)\nPhus Lu (phus DOT lu AT gmail DOT com)\nRobert Buchholz (rbu AT goodpoint DOT de)\nRobert Jordens (jordens AT gmail DOT com)\nRyan Cox (ryan DOT a DOT cox AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nScott Maxwell (scott AT codecobblers DOT com)\nScott Sadler (github AT mashi DOT org)\nSimon Knight (simon DOT knight AT gmail DOT com)\nStefan Friesel (sf AT cloudcontrol DOT de)\nStefan van der Walt (stefan AT sun DOT ac DOT za)\nStephen Diehl (stephen DOT m DOT diehl AT gmail DOT com)\nSylvain Corlay (scorlay AT bloomberg DOT net)\nThomas Kluyver (takowl AT gmail DOT com)\nThomas Spura (tomspur AT fedoraproject DOT org)\nTigger Bear (Tigger AT Tiggers-Mac-mini DOT local)\nTorsten Landschoff (torsten DOT landschoff AT dynamore DOT de)\nVadim Markovtsev (v DOT markovtsev AT samsung DOT com)\nYannick Hold (yannickhold AT gmail DOT com)\nZbigniew Jędrzejewski-Szmek (zbyszek AT in DOT waw DOT pl)\nhugo shi (hugoshi AT bleb2 DOT (none))\njdgleeson (jdgleeson AT mac DOT com)\nkyledj (kyle AT bucebuce DOT com)\nspez (steve AT hipmunk DOT com)\nstu (stuart DOT axon AT jpcreative DOT co DOT uk)\nxantares (xantares AT fujitsu-l64 DOT (none))\n\nas reported by:\ngit log --all --format='- %aN (%aE)' | sort -u | sed 's/@/ AT /1' | sed -e 's/\\.\\([^ ]\\)/ DOT \\1/g'\nwith some adjustments.\n\n\n\nBrandon Craig-Rhodes (brandon AT rhodesmill DOT org)\nEugene Chernyshov (chernyshov DOT eugene AT gmail DOT com)\nCraig Austin (craig DOT austin AT gmail DOT com)\n\n\n\n\n\nTravis Cline (travis DOT cline AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nZachary Voase (z AT zacharyvoase DOT com)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/pyzmq-25.1.2.dist-info/AUTHORS.html#authors",
    "href": "script_venv/lib/python3.8/site-packages/pyzmq-25.1.2.dist-info/AUTHORS.html#authors",
    "title": "",
    "section": "",
    "text": "This project was started and continues to be led by Brian E. Granger (ellisonbg AT gmail DOT com). Min Ragan-Kelley (benjaminrk AT gmail DOT com) is the primary developer of pyzmq at this time.\nThe following people have contributed to the project:\n\nAlexander Else (alexander DOT else AT team DOT telstra DOT com)\nAlexander Pyhalov (apyhalov AT gmail DOT com)\nAlexandr Emelin (frvzmb AT gmail DOT com)\nAmr Ali (amr AT ledgerx DOT com)\nAndre Caron (andre DOT l DOT caron AT gmail DOT com)\nAndrea Crotti (andrea DOT crotti DOT 0 AT gmail DOT com)\nAndrew Gwozdziewycz (git AT apgwoz DOT com)\nBaptiste Lepilleur (baptiste DOT lepilleur AT gmail DOT com)\nBrandyn A. White (bwhite AT dappervision DOT com)\nBrian E. Granger (ellisonbg AT gmail DOT com)\nBrian Hoffman (hoffman_brian AT bah DOT com)\nCarlos A. Rocha (carlos DOT rocha AT gmail DOT com)\nChris Laws (clawsicus AT gmail DOT com)\nChristian Wyglendowski (christian AT bu DOT mp)\nChristoph Gohlke (cgohlke AT uci DOT edu)\nCurtis (curtis AT tinbrain DOT net)\nCyril Holweck (cyril DOT holweck AT free DOT fr)\nDan Colish (dcolish AT gmail DOT com)\nDaniel Lundin (dln AT eintr DOT org)\nDaniel Truemper (truemped AT googlemail DOT com)\nDouglas Creager (douglas DOT creager AT redjack DOT com)\nEduardo Stalinho (eduardooc DOT 86 AT gmail DOT com)\nEren Güven (erenguven0 AT gmail DOT com)\nErick Tryzelaar (erick DOT tryzelaar AT gmail DOT com)\nErik Tollerud (erik DOT tollerud AT gmail DOT com)\nFELD Boris (lothiraldan AT gmail DOT com)\nFantix King (fantix DOT king AT gmail DOT com)\nFelipe Cruz (felipecruz AT loogica DOT net)\nFernando Perez (Fernando DOT Perez AT berkeley DOT edu)\nFrank Wiles (frank AT revsys DOT com)\nFélix-Antoine Fortin (felix DOT antoine DOT fortin AT gmail DOT com)\nGavrie Philipson (gavriep AT il DOT ibm DOT com)\nGodefroid Chapelle (gotcha AT bubblenet DOT be)\nGreg Banks (gbanks AT mybasis DOT com)\nGreg Ward (greg AT gerg DOT ca)\nGuido Goldstein (github AT a-nugget DOT de)\nIan Lee (IanLee1521 AT gmail DOT com)\nIonuț Arțăriși (ionut AT artarisi DOT eu)\nIvo Danihelka (ivo AT danihelka DOT net)\nIyed (iyed DOT bennour AT gmail DOT com)\nJim Garrison (jim AT garrison DOT cc)\nJohn Gallagher (johnkgallagher AT gmail DOT com)\nJulian Taylor (jtaylor DOT debian AT googlemail DOT com)\nJustin Bronder (jsbronder AT gmail DOT com)\nJustin Riley (justin DOT t DOT riley AT gmail DOT com)\nMarc Abramowitz (marc AT marc-abramowitz DOT com)\nMatthew Aburn (mattja6 AT gmail DOT com)\nMichel Pelletier (pelletier DOT michel AT gmail DOT com)\nMichel Zou (xantares09 AT hotmail DOT com)\nMin Ragan-Kelley (benjaminrk AT gmail DOT com)\nNell Hardcastle (nell AT dev-nell DOT com)\nNicholas Pilkington (nicholas DOT pilkington AT gmail DOT com)\nNicholas Piël (nicholas AT nichol DOT as)\nNick Pellegrino (npellegrino AT mozilla DOT com)\nNicolas Delaby (nicolas DOT delaby AT ezeep DOT com)\nOndrej Certik (ondrej AT certik DOT cz)\nPaul Colomiets (paul AT colomiets DOT name)\nPawel Jasinski (pawel DOT jasinski AT gmail DOT com)\nPhus Lu (phus DOT lu AT gmail DOT com)\nRobert Buchholz (rbu AT goodpoint DOT de)\nRobert Jordens (jordens AT gmail DOT com)\nRyan Cox (ryan DOT a DOT cox AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nScott Maxwell (scott AT codecobblers DOT com)\nScott Sadler (github AT mashi DOT org)\nSimon Knight (simon DOT knight AT gmail DOT com)\nStefan Friesel (sf AT cloudcontrol DOT de)\nStefan van der Walt (stefan AT sun DOT ac DOT za)\nStephen Diehl (stephen DOT m DOT diehl AT gmail DOT com)\nSylvain Corlay (scorlay AT bloomberg DOT net)\nThomas Kluyver (takowl AT gmail DOT com)\nThomas Spura (tomspur AT fedoraproject DOT org)\nTigger Bear (Tigger AT Tiggers-Mac-mini DOT local)\nTorsten Landschoff (torsten DOT landschoff AT dynamore DOT de)\nVadim Markovtsev (v DOT markovtsev AT samsung DOT com)\nYannick Hold (yannickhold AT gmail DOT com)\nZbigniew Jędrzejewski-Szmek (zbyszek AT in DOT waw DOT pl)\nhugo shi (hugoshi AT bleb2 DOT (none))\njdgleeson (jdgleeson AT mac DOT com)\nkyledj (kyle AT bucebuce DOT com)\nspez (steve AT hipmunk DOT com)\nstu (stuart DOT axon AT jpcreative DOT co DOT uk)\nxantares (xantares AT fujitsu-l64 DOT (none))\n\nas reported by:\ngit log --all --format='- %aN (%aE)' | sort -u | sed 's/@/ AT /1' | sed -e 's/\\.\\([^ ]\\)/ DOT \\1/g'\nwith some adjustments.\n\n\n\nBrandon Craig-Rhodes (brandon AT rhodesmill DOT org)\nEugene Chernyshov (chernyshov DOT eugene AT gmail DOT com)\nCraig Austin (craig DOT austin AT gmail DOT com)\n\n\n\n\n\nTravis Cline (travis DOT cline AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nZachary Voase (z AT zacharyvoase DOT com)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/httpcore-1.0.4.dist-info/licenses/LICENSE.html",
    "href": "script_venv/lib/python3.8/site-packages/httpcore-1.0.4.dist-info/licenses/LICENSE.html",
    "title": "",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/cffi/recompiler.html",
    "href": "script_venv/lib/python3.8/site-packages/cffi/recompiler.html",
    "title": "",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html",
    "href": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html",
    "title": "Authors",
    "section": "",
    "text": "pyqode.qt: Colin Duquesnoy (@ColinDuquesnoy)\nspyderlib.qt: Pierre Raybaut (@PierreRaybaut)\nqt-helpers: Thomas Robitaille (@astrofrog)\n\n\n\n\n\nDaniel Althviz (@dalthviz)\nCarlos Cordoba (@ccordoba12)\nC.A.M. Gerlach (@CAM-Gerlach)\nSpyder Development Team (Spyder-IDE)\n\n\n\n\n\nThe QtPy Contributors"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#original-authors",
    "href": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#original-authors",
    "title": "Authors",
    "section": "",
    "text": "pyqode.qt: Colin Duquesnoy (@ColinDuquesnoy)\nspyderlib.qt: Pierre Raybaut (@PierreRaybaut)\nqt-helpers: Thomas Robitaille (@astrofrog)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#current-maintainers",
    "href": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#current-maintainers",
    "title": "Authors",
    "section": "",
    "text": "Daniel Althviz (@dalthviz)\nCarlos Cordoba (@ccordoba12)\nC.A.M. Gerlach (@CAM-Gerlach)\nSpyder Development Team (Spyder-IDE)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#contributors",
    "href": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#contributors",
    "title": "Authors",
    "section": "",
    "text": "The QtPy Contributors"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/wasabi/tests/test-data/wasabi-test-notebook.html",
    "href": "script_venv/lib/python3.8/site-packages/wasabi/tests/test-data/wasabi-test-notebook.html",
    "title": "",
    "section": "",
    "text": "import sys\nimport wasabi\n\nwasabi.msg.warn(\"This is a test. This is only a test.\")\nif sys.version_info &gt;= (3, 7):\n    assert wasabi.util.supports_ansi()\n\nprint(sys.stdout)\n\n\n\n\n Back to top"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/soupsieve-2.5.dist-info/licenses/LICENSE.html",
    "href": "script_venv/lib/python3.8/site-packages/soupsieve-2.5.dist-info/licenses/LICENSE.html",
    "title": "",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2023 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n\n Back to top"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html",
    "href": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html",
    "title": "Natural Language Toolkit (NLTK) Authors",
    "section": "",
    "text": "Steven Bird stevenbird1@gmail.com\nEdward Loper edloper@gmail.com\nEwan Klein ewan@inf.ed.ac.uk\n\n\n\n\n\nTom Aarsen\nRami Al-Rfou’\nMark Amery\nGreg Aumann\nIvan Barria\nIngolf Becker\nYonatan Becker\nPaul Bedaride\nSteven Bethard\nRobert Berwick\nDan Blanchard\nNathan Bodenstab\nAlexander Böhm\nFrancis Bond\nPaul Bone\nJordan Boyd-Graber\nDaniel Blanchard\nPhil Blunsom\nLars Buitinck\nCristian Capdevila\nSteve Cassidy\nChen-Fu Chiang\nDmitry Chichkov\nJinyoung Choi\nAndrew Clausen\nLucas Champollion\nGraham Christensen\nTrevor Cohn\nDavid Coles\nTom Conroy https://github.com/tconroy\nClaude Coulombe\nLucas Cooper\nRobin Cooper\nChris Crowner\nJames Curran\nArthur Darcet\nDariel Dato-on\nSelina Dennis\nLeon Derczynski\nAlexis Dimitriadis\nNikhil Dinesh\nLiang Dong\nDavid Doukhan\nRebecca Dridan\nPablo Duboue\nLong Duong\nChristian Federmann\nCampion Fellin\nMichelle Fullwood\nDan Garrette\nMaciej Gawinecki\nJean Mark Gawron\nSumukh Ghodke\nYoav Goldberg\nMichael Wayne Goodman\nDougal Graham\nBrent Gray\nSimon Greenhill\nClark Grubb\nEduardo Pereira Habkost\nMasato Hagiwara\nLauri Hallila\nMichael Hansen\nYurie Hara\nWill Hardy\nTyler Hartley\nPeter Hawkins\nSaimadhav Heblikar\nFredrik Hedman\nHelder\nMichael Heilman\nOfer Helman\nChristopher Hench\nBruce Hill\nAmy Holland\nKristy Hollingshead\nMarcus Huderle\nBaden Hughes\nNancy Ide\nRebecca Ingram\nEdward Ivanovic\nThomas Jakobsen\nNick Johnson\nEric Kafe\nPiotr Kasprzyk\nAngelos Katharopoulos\nSudharshan Kaushik\nChris Koenig\nMikhail Korobov\nDenis Krusko\nIlia Kurenkov\nStefano Lattarini\nPierre-François Laquerre\nStefano Lattarini\nHaejoong Lee\nJackson Lee\nMax Leonov\nChris Liechti\nHyuckin David Lim\nTom Lippincott\nPeter Ljunglöf\nAlex Louden\nJoseph Lynch\nNitin Madnani\nFelipe Madrigal\nBjørn Mæland\nDean Malmgren\nChristopher Maloof\nRob Malouf\nIker Manterola\nCarl de Marcken\nMitch Marcus\nTorsten Marek\nRobert Marshall\nMarius Mather\nDuncan McGreggor\nDavid McClosky\nXinfan Meng\nDmitrijs Milajevs\nMargaret Mitchell\nTomonori Nagano\nJason Narad\nShari A’aidil Nasruddin\nLance Nathan\nMorten Neergaard\nDavid Nemeskey\nEric Nichols\nJoel Nothman\nAlireza Nourian\nAlexander Oleynikov\nPierpaolo Pantone\nTed Pedersen\nJacob Perkins\nAlberto Planas\nOndrej Platek\nAlessandro Presta\nQi Liu\nMartin Thorsen Ranang\nMichael Recachinas\nBrandon Rhodes\nJoshua Ritterman\nWill Roberts\nStuart Robinson\nCarlos Rodriguez\nLorenzo Rubio\nAlex Rudnick\nJussi Salmela\nGeoffrey Sampson\nKepa Sarasola\nKevin Scannell\nNathan Schneider\nRico Sennrich\nThomas Skardal\nEric Smith\nLynn Soe\nRob Speer\nPeter Spiller\nRichard Sproat\nCeri Stagg\nPeter Stahl\nOliver Steele\nThomas Stieglmaier\nJan Strunk\nLiling Tan\nClaire Taylor\nLouis Tiao\nSteven Tomcavage\nTiago Tresoldi\nMarcus Uneson\nYu Usami\nPetro Verkhogliad\nPeter Wang\nZhe Wang\nCharlotte Wilson\nChuck Wooters\nSteven Xu\nBeracah Yankama\nLei Ye (叶磊)\nPatrick Ye\nGeraldine Sim Wei Ying\nJason Yoder\nThomas Zieglier\n0ssifrage\nducki13\nkiwipi\nlade\nisnowfy\nonesandzeros\npquentin\nwvanlint\nÁlvaro Justen https://github.com/turicas\nbjut-hz\nSergio Oller\nWill Monroe\nElijah Rippeth\nEmil Manukyan\nCasper Lehmann-Strøm\nAndrew Giel\nTanin Na Nakorn\nLinghao Zhang\nColin Carroll\nHeguang Miao\nHannah Aizenman (story645)\nGeorge Berry\nAdam Nelson\nJ Richard Snape\nAlex Constantin alex@keyworder.ch\nTsolak Ghukasyan\nPrasasto Adi\nSafwan Kamarrudin\nArthur Tilley\nVilhjalmur Thorsteinsson\nJaehoon Hwang https://github.com/jaehoonhwang\nChintan Shah https://github.com/chintanshah24\nsbagan\nZicheng Xu\nAlbert Au Yeung https://github.com/albertauyeung\nShenjian Zhao\nDeng Wang https://github.com/lmatt-bit\nAli Abdullah\nStoytcho Stoytchev\nLakhdar Benzahia\nKheireddine Abainia https://github.com/xprogramer\nYibin Lin https://github.com/yibinlin\nArtiem Krinitsyn\nBjörn Mattsson\nOleg Chislov\nPavan Gururaj Joshi https://github.com/PavanGJ\nEthan Hill https://github.com/hill1303\nVivek Lakshmanan\nSomnath Rakshit https://github.com/somnathrakshit\nAnlan Du\nPulkit Maloo https://github.com/pulkitmaloo\nBrandon M. Burroughs https://github.com/brandonmburroughs\nJohn Stewart https://github.com/free-variation\nIaroslav Tymchenko https://github.com/myproblemchild\nAleš Tamchyna\nTim Gianitsos https://github.com/timgianitsos\nPhilippe Partarrieu https://github.com/ppartarr\nAndrew Owen Martin\nAdrian Ellis https://github.com/adrianjellis\nNat Quayle Nelson https://github.com/nqnstudios\nYanpeng Zhao https://github.com/zhaoyanpeng\nMatan Rak https://github.com/matanrak\nNick Ulle https://github.com/nick-ulle\nUday Krishna https://github.com/udaykrishna\nOsman Zubair https://github.com/okz12\nViresh Gupta https://github.com/virresh\nOndřej Cífka https://github.com/cifkao\nIris X. Zhou https://github.com/irisxzhou\nDevashish Lal https://github.com/BLaZeKiLL\nGerhard Kremer https://github.com/GerhardKa\nNicolas Darr https://github.com/ndarr\nHervé Nicol https://github.com/hervenicol\nAlexandre H. T. Dias https://github.com/alexandredias3d\nDaksh Shah https://github.com/Daksh\nJacob Weightman https://github.com/jacobdweightman\nBonifacio de Oliveira https://github.com/Bonifacio2\nArmins Bagrats Stepanjans https://github.com/ab-10\nVassilis Palassopoulos https://github.com/palasso\nRam Rachum https://github.com/cool-RR\nOr Sharir https://github.com/orsharir\nDenali Molitor https://github.com/dmmolitor\nJacob Moorman https://github.com/jdmoorman\nCory Nezin https://github.com/corynezin\nMatt Chaput\nDanny Sepler https://github.com/dannysepler\nAkshita Bhagia https://github.com/AkshitaB\nPratap Yadav https://github.com/prtpydv\nHiroki Teranishi https://github.com/chantera\nRuben Cartuyvels https://github.com/rubencart\nDalton Pearson https://github.com/daltonpearson\nRobby Horvath https://github.com/robbyhorvath\nGavish Poddar https://github.com/gavishpoddar\nSaibo Geng https://github.com/Saibo-creator\nAhmet Yildirim https://github.com/RnDevelover\nYuta Nakamura https://github.com/yutanakamura-tky\nAdam Hawley https://github.com/adamjhawley\nPanagiotis Simakis https://github.com/sp1thas\nRichard Wang https://github.com/richarddwang\nAlexandre Perez-Lebel https://github.com/aperezlebel\nFernando Carranza https://github.com/fernandocar86\nMartin Kondratzky https://github.com/martinkondra\nHeungson Lee https://github.com/heungson\nM.K. Pawelkiewicz https://github.com/hamiltonianflow\nSteven Thomas Smith https://github.com/essandess\nJan Lennartz https://github.com/Madnex\n\n\n\n\n\n\n\nMartin Porter\nVivake Gupta\nBarry Wilkins\nHiranmay Ghosh\nChris Emerson\n\n\n\n\n\nAssem Chelli\nAbdelkrim Aries\nLakhdar Benzahia"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#original-authors",
    "href": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#original-authors",
    "title": "Natural Language Toolkit (NLTK) Authors",
    "section": "",
    "text": "Steven Bird stevenbird1@gmail.com\nEdward Loper edloper@gmail.com\nEwan Klein ewan@inf.ed.ac.uk"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#contributors",
    "href": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#contributors",
    "title": "Natural Language Toolkit (NLTK) Authors",
    "section": "",
    "text": "Tom Aarsen\nRami Al-Rfou’\nMark Amery\nGreg Aumann\nIvan Barria\nIngolf Becker\nYonatan Becker\nPaul Bedaride\nSteven Bethard\nRobert Berwick\nDan Blanchard\nNathan Bodenstab\nAlexander Böhm\nFrancis Bond\nPaul Bone\nJordan Boyd-Graber\nDaniel Blanchard\nPhil Blunsom\nLars Buitinck\nCristian Capdevila\nSteve Cassidy\nChen-Fu Chiang\nDmitry Chichkov\nJinyoung Choi\nAndrew Clausen\nLucas Champollion\nGraham Christensen\nTrevor Cohn\nDavid Coles\nTom Conroy https://github.com/tconroy\nClaude Coulombe\nLucas Cooper\nRobin Cooper\nChris Crowner\nJames Curran\nArthur Darcet\nDariel Dato-on\nSelina Dennis\nLeon Derczynski\nAlexis Dimitriadis\nNikhil Dinesh\nLiang Dong\nDavid Doukhan\nRebecca Dridan\nPablo Duboue\nLong Duong\nChristian Federmann\nCampion Fellin\nMichelle Fullwood\nDan Garrette\nMaciej Gawinecki\nJean Mark Gawron\nSumukh Ghodke\nYoav Goldberg\nMichael Wayne Goodman\nDougal Graham\nBrent Gray\nSimon Greenhill\nClark Grubb\nEduardo Pereira Habkost\nMasato Hagiwara\nLauri Hallila\nMichael Hansen\nYurie Hara\nWill Hardy\nTyler Hartley\nPeter Hawkins\nSaimadhav Heblikar\nFredrik Hedman\nHelder\nMichael Heilman\nOfer Helman\nChristopher Hench\nBruce Hill\nAmy Holland\nKristy Hollingshead\nMarcus Huderle\nBaden Hughes\nNancy Ide\nRebecca Ingram\nEdward Ivanovic\nThomas Jakobsen\nNick Johnson\nEric Kafe\nPiotr Kasprzyk\nAngelos Katharopoulos\nSudharshan Kaushik\nChris Koenig\nMikhail Korobov\nDenis Krusko\nIlia Kurenkov\nStefano Lattarini\nPierre-François Laquerre\nStefano Lattarini\nHaejoong Lee\nJackson Lee\nMax Leonov\nChris Liechti\nHyuckin David Lim\nTom Lippincott\nPeter Ljunglöf\nAlex Louden\nJoseph Lynch\nNitin Madnani\nFelipe Madrigal\nBjørn Mæland\nDean Malmgren\nChristopher Maloof\nRob Malouf\nIker Manterola\nCarl de Marcken\nMitch Marcus\nTorsten Marek\nRobert Marshall\nMarius Mather\nDuncan McGreggor\nDavid McClosky\nXinfan Meng\nDmitrijs Milajevs\nMargaret Mitchell\nTomonori Nagano\nJason Narad\nShari A’aidil Nasruddin\nLance Nathan\nMorten Neergaard\nDavid Nemeskey\nEric Nichols\nJoel Nothman\nAlireza Nourian\nAlexander Oleynikov\nPierpaolo Pantone\nTed Pedersen\nJacob Perkins\nAlberto Planas\nOndrej Platek\nAlessandro Presta\nQi Liu\nMartin Thorsen Ranang\nMichael Recachinas\nBrandon Rhodes\nJoshua Ritterman\nWill Roberts\nStuart Robinson\nCarlos Rodriguez\nLorenzo Rubio\nAlex Rudnick\nJussi Salmela\nGeoffrey Sampson\nKepa Sarasola\nKevin Scannell\nNathan Schneider\nRico Sennrich\nThomas Skardal\nEric Smith\nLynn Soe\nRob Speer\nPeter Spiller\nRichard Sproat\nCeri Stagg\nPeter Stahl\nOliver Steele\nThomas Stieglmaier\nJan Strunk\nLiling Tan\nClaire Taylor\nLouis Tiao\nSteven Tomcavage\nTiago Tresoldi\nMarcus Uneson\nYu Usami\nPetro Verkhogliad\nPeter Wang\nZhe Wang\nCharlotte Wilson\nChuck Wooters\nSteven Xu\nBeracah Yankama\nLei Ye (叶磊)\nPatrick Ye\nGeraldine Sim Wei Ying\nJason Yoder\nThomas Zieglier\n0ssifrage\nducki13\nkiwipi\nlade\nisnowfy\nonesandzeros\npquentin\nwvanlint\nÁlvaro Justen https://github.com/turicas\nbjut-hz\nSergio Oller\nWill Monroe\nElijah Rippeth\nEmil Manukyan\nCasper Lehmann-Strøm\nAndrew Giel\nTanin Na Nakorn\nLinghao Zhang\nColin Carroll\nHeguang Miao\nHannah Aizenman (story645)\nGeorge Berry\nAdam Nelson\nJ Richard Snape\nAlex Constantin alex@keyworder.ch\nTsolak Ghukasyan\nPrasasto Adi\nSafwan Kamarrudin\nArthur Tilley\nVilhjalmur Thorsteinsson\nJaehoon Hwang https://github.com/jaehoonhwang\nChintan Shah https://github.com/chintanshah24\nsbagan\nZicheng Xu\nAlbert Au Yeung https://github.com/albertauyeung\nShenjian Zhao\nDeng Wang https://github.com/lmatt-bit\nAli Abdullah\nStoytcho Stoytchev\nLakhdar Benzahia\nKheireddine Abainia https://github.com/xprogramer\nYibin Lin https://github.com/yibinlin\nArtiem Krinitsyn\nBjörn Mattsson\nOleg Chislov\nPavan Gururaj Joshi https://github.com/PavanGJ\nEthan Hill https://github.com/hill1303\nVivek Lakshmanan\nSomnath Rakshit https://github.com/somnathrakshit\nAnlan Du\nPulkit Maloo https://github.com/pulkitmaloo\nBrandon M. Burroughs https://github.com/brandonmburroughs\nJohn Stewart https://github.com/free-variation\nIaroslav Tymchenko https://github.com/myproblemchild\nAleš Tamchyna\nTim Gianitsos https://github.com/timgianitsos\nPhilippe Partarrieu https://github.com/ppartarr\nAndrew Owen Martin\nAdrian Ellis https://github.com/adrianjellis\nNat Quayle Nelson https://github.com/nqnstudios\nYanpeng Zhao https://github.com/zhaoyanpeng\nMatan Rak https://github.com/matanrak\nNick Ulle https://github.com/nick-ulle\nUday Krishna https://github.com/udaykrishna\nOsman Zubair https://github.com/okz12\nViresh Gupta https://github.com/virresh\nOndřej Cífka https://github.com/cifkao\nIris X. Zhou https://github.com/irisxzhou\nDevashish Lal https://github.com/BLaZeKiLL\nGerhard Kremer https://github.com/GerhardKa\nNicolas Darr https://github.com/ndarr\nHervé Nicol https://github.com/hervenicol\nAlexandre H. T. Dias https://github.com/alexandredias3d\nDaksh Shah https://github.com/Daksh\nJacob Weightman https://github.com/jacobdweightman\nBonifacio de Oliveira https://github.com/Bonifacio2\nArmins Bagrats Stepanjans https://github.com/ab-10\nVassilis Palassopoulos https://github.com/palasso\nRam Rachum https://github.com/cool-RR\nOr Sharir https://github.com/orsharir\nDenali Molitor https://github.com/dmmolitor\nJacob Moorman https://github.com/jdmoorman\nCory Nezin https://github.com/corynezin\nMatt Chaput\nDanny Sepler https://github.com/dannysepler\nAkshita Bhagia https://github.com/AkshitaB\nPratap Yadav https://github.com/prtpydv\nHiroki Teranishi https://github.com/chantera\nRuben Cartuyvels https://github.com/rubencart\nDalton Pearson https://github.com/daltonpearson\nRobby Horvath https://github.com/robbyhorvath\nGavish Poddar https://github.com/gavishpoddar\nSaibo Geng https://github.com/Saibo-creator\nAhmet Yildirim https://github.com/RnDevelover\nYuta Nakamura https://github.com/yutanakamura-tky\nAdam Hawley https://github.com/adamjhawley\nPanagiotis Simakis https://github.com/sp1thas\nRichard Wang https://github.com/richarddwang\nAlexandre Perez-Lebel https://github.com/aperezlebel\nFernando Carranza https://github.com/fernandocar86\nMartin Kondratzky https://github.com/martinkondra\nHeungson Lee https://github.com/heungson\nM.K. Pawelkiewicz https://github.com/hamiltonianflow\nSteven Thomas Smith https://github.com/essandess\nJan Lennartz https://github.com/Madnex"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#others-whose-work-weve-taken-and-included-in-nltk-but-who-didnt-directly-contribute-it",
    "href": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#others-whose-work-weve-taken-and-included-in-nltk-but-who-didnt-directly-contribute-it",
    "title": "Natural Language Toolkit (NLTK) Authors",
    "section": "",
    "text": "Martin Porter\nVivake Gupta\nBarry Wilkins\nHiranmay Ghosh\nChris Emerson\n\n\n\n\n\nAssem Chelli\nAbdelkrim Aries\nLakhdar Benzahia"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/httpx-0.27.0.dist-info/licenses/LICENSE.html",
    "href": "script_venv/lib/python3.8/site-packages/httpx-0.27.0.dist-info/licenses/LICENSE.html",
    "title": "",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\n\n Back to top"
  },
  {
    "objectID": "llm/parameterization.html",
    "href": "llm/parameterization.html",
    "title": "Parameterization of GPT",
    "section": "",
    "text": "The GPT models provided by OpenAI provide a variety of parameters that can change the way the language model responds. Below you can find a list of the most important ones.",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Parameterization of GPT"
    ]
  },
  {
    "objectID": "llm/parameterization.html#roles",
    "href": "llm/parameterization.html#roles",
    "title": "Parameterization of GPT",
    "section": "Roles:",
    "text": "Roles:\nIn order to cover most tasks you want to perform using a chat format, the OpenAI API let’s you define different roles in the chat. The available roles are system, assistant, user and tools. You should already be familiar with two of them by now: The user role corresponds to the actual user prompting the language model, all answers are given with the assisstant role.\nThe system role can now be given to provide some additional general instructions to the language model that are typically not a user input, for example, the style in which the model responds. In this case, an example is better than any explanation.\n\nimport os\nfrom llm_utils.client import get_openai_client\n\nMODEL = \"gpt4\"\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\ncompletion = client.chat.completions.create(\n  model=\"MODEL\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are an annoyed technician working in a help center for dish washers, who answers in short, unfriendly bursts.\"},\n    {\"role\": \"user\", \"content\": \"My dish washer does not clean the dishes, what could be the reason.\"}\n  ]\n)\n\nprint(completion.choices[0].message.content)\n\nCould be anything. Blocked spray arm. Clogged filter. Faulty pump. Detergent issue. Check all that.",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Parameterization of GPT"
    ]
  },
  {
    "objectID": "llm/parameterization.html#sec-test",
    "href": "llm/parameterization.html#sec-test",
    "title": "Parameterization of GPT",
    "section": "Function calling:",
    "text": "Function calling:\nAs we have seen, most interactions with a language model happen in form of a chat with almost “free” question or instructions and answers. While this seems the most natural in most cases, it is not always a practical format if we want to use a language model for very specific purposes. This happens particularly often when we want to employ a language model in business situations, where we require a consistent output of the model.\nAs an example, let us try to use GPT for sentiment analysis (see also here). Let’s say we want GPT to classify a text into one of the following four categories:\n\nsentiment_categories = [\n    \"positive\", \n    \"negative\",\n    \"neutral\",\n    \"mixed\"\n]\n\nWe could do the following:\n\nmessages = []\nmessages.append(\n    {\"role\": \"system\", \"content\": f\"Classify the given text into one of the following sentiment categories: {sentiment_categories}.\"}\n)\nmessages.append(\n    {\"role\": \"user\", \"content\": \"I really did not like the movie.\"}\n)\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=MODEL\n)\n\nprint(f\"Response: '{response.choices[0].message.content}'\")\n\n\n\nResponse: 'Category: Negative'\n\n\nIt is easy to spot the problem: GPT does not necessarily answer in the way we expect or want it to. In this case, instead of simply returning the correct category, it also returns the string Category: alongside it (and capitalized Negative). So if we were to use the answer in a program or data base, we’d now again have to use some NLP techniques to parse it in order eventually retrieve exactly the category we were looking for: negative. What we need instead is a way to constrain GPT to a specific way of answering, and this is where functions or tools come into play (see also Function calling and Function calling (cookbook)).\nThis concept allows us to specify the exact output format we expect to receive from GPT (it is called functions since ideally we want to call a function directly on the output of GPT so it has to be in a specific format).\n\n# this looks intimidating but isn't that complicated\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_sentiment\",\n            \"description\": \"Analyze the sentiment in a given text.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"sentiment\": {\n                        \"type\": \"string\",\n                        \"enum\": sentiment_categories,\n                        \"description\": f\"The sentiment of the text.\"\n                    }\n                },\n                \"required\": [\"sentiment\"],\n            }\n        }\n    }\n]\n\n\nmessages = []\nmessages.append(\n    {\"role\": \"system\", \"content\": f\"Classify the given text into one of the following sentiment categories: {sentiment_categories}.\"}\n)\nmessages.append(\n    {\"role\": \"user\", \"content\": \"I really did not like the movie.\"}\n)\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=MODEL,\n    tools=tools,\n    tool_choice={\n        \"type\": \"function\", \n        \"function\": {\"name\": \"analyze_sentiment\"}}\n)\n\nprint(f\"Response: '{response.choices[0].message.tool_calls[0].function.arguments}'\")\n\nResponse: '{\n\"sentiment\": \"negative\"\n}'\n\n\nWe can now easily extract what we need:\n\nimport json \nresult = json.loads(response.choices[0].message.tool_calls[0].function.arguments) # remember that the answer is a string\nprint(result[\"sentiment\"])\n\nnegative\n\n\nWe can also include multiple function parameters if our desired output has multiple components. Let’s try to include another parameter which includes the reason for the sentiment.\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_sentiment\",\n            \"description\": \"Analyze the sentiment in a given text.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"sentiment\": {\n                        \"type\": \"string\",\n                        \"enum\": sentiment_categories,\n                        \"description\": f\"The sentiment of the text.\"\n                    },\n                    \"reason\": {\n                        \"type\": \"string\",\n                        \"description\": \"The reason for the sentiment in few words. If there is no information, do not make assumptions and leave blank.\"\n                    }\n                },\n                \"required\": [\"sentiment\", \"reason\"],\n            }\n        }\n    }\n]\n\n\nmessages = []\nmessages.append(\n    {\"role\": \"system\", \"content\": f\"Classify the given text into one of the following sentiment categories: {sentiment_categories}. If you can, also extract the reason.\"}\n)\nmessages.append(\n    {\"role\": \"user\", \"content\": \"I loved the movie, Johnny Depp is a great actor.\"}\n)\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=MODEL,\n    tools=tools,\n    tool_choice={\n        \"type\": \"function\", \n        \"function\": {\"name\": \"analyze_sentiment\"}}\n)\n\nprint(f\"Response: '{response.choices[0].message.tool_calls[0].function.arguments}'\")\n\nResponse: '{\n\"sentiment\": \"positive\",\n\"reason\": \"Appreciation for the movie and actor\"\n}'\n\n\nHere, again, we could also constrain the possibilities for the reason to a certain set. Hence, functions are great to have more consistent answers of the language model such that we can use it in applications.",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Parameterization of GPT"
    ]
  },
  {
    "objectID": "llm/intro.html",
    "href": "llm/intro.html",
    "title": "Introduction to LLM",
    "section": "",
    "text": "Definition of Large Language Models: Large Language Models (LLMs) are deep learning models trained on vast amounts of text data to understand and generate human-like text. They use advanced techniques such as Transformers and self-attention mechanisms to process and generate sequences of words.\nPre-training and Fine-tuning: LLMs are typically pre-trained on large text corpora using unsupervised learning techniques, where they learn the statistical properties of natural language. After pre-training, they can be fine-tuned on specific tasks or domains with labeled data to adapt their knowledge and capabilities.\nTransformer Architecture: Transformers are the backbone of LLMs, consisting of multiple layers of self-attention mechanisms and feed-forward neural networks. They excel at capturing long-range dependencies in sequential data, making them well-suited for NLP tasks.\nSelf-Attention Mechanism: Self-attention allows LLMs to weigh the importance of each word in a sequence based on its relationship with other words in the sequence. This mechanism enables them to capture contextual information effectively and generate coherent text.\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Introduction to LLM"
    ]
  },
  {
    "objectID": "llm/exercises/ex_gpt_start.html",
    "href": "llm/exercises/ex_gpt_start.html",
    "title": "Exercise: OpenAI - Getting started",
    "section": "",
    "text": "Task: Explore the OpenAI chat.completions API.\nInstructions:\n\nGenerate a chat completion and analyze the response object ChatCompletion. What information do you get with each completion?\nHow can you access the actual completion of your prompt?\nUse the OpenAI API documentation to find out what choices are and how they are used.\nPlay around with the parameters temperature and top_p for a simple prompt. What do you notice?\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Exercise: OpenAI - Getting started"
    ]
  },
  {
    "objectID": "llm/exercises/ex_gpt_ner_with_function_calls.html",
    "href": "llm/exercises/ex_gpt_ner_with_function_calls.html",
    "title": "Exercise: NER with tool calling",
    "section": "",
    "text": "Task: Create a small script that uses tool (or function calling) to extract the following named entities from a given text: City, State, Person.\nInstructions:\n\nDefine an OpenAI tool with a function named_entity_recognition.\nChoose an appropriate output format, for example: {\"named_entities\": [{\"entity\": \"Mike\", \"label\": \"Person}, {\"entity\": \"Münster\", \"label\": \"City\"}]}\nDefine a matching prompt in the role system and the text input for the role user.\nExtract the result.\n\n\n# prerequisites\nimport os\nfrom llm_utils.client import get_openai_client\n\nMODEL = \"gpt4\"\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n# here goes your code\n\n\n\nShow solution\n\n\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"named_entity_recognition\",\n            \"description\": \"Extract the named entities from the given text.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"named_entities\": {\n                        \"type\": \"array\",\n                        \"description\": \"A list of all extracted named entities in form of dictionaries containing the entity name and the label\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"entity\": {\"type\": \"string\"}, \n                                \"label\": {\"type\": \"string\"}\n                            },\n                            \"required\": [\"entity\", \"label\"]\n                        }\n                    },\n                },\n                \"required\": [\"named_entities\"],\n            },\n        }\n    }\n]\n\n\n# define the prompts\nmessages = []\nmessages.append({\"role\": \"system\", \"content\": \"Extract all named entities from the provided text. Possible labels are 'City', 'State' or 'Person'. If no named entities are contained in the text, do not make assumptions and return nothing.\"})\nmessages.append({\"role\": \"user\", \"content\": \"Leonard Hoffstaedter lives in Pasadena, CA.\"})\n\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=messages,\n    tools=tools,\n    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"named_entity_recognition\"}}\n)\nresponse\n\n\nChatCompletion(id='chatcmpl-99ALw7LjaBzZ63s5CMt9wDGn3aWhM', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_1aw75NLIUiEpdYMztdXRDZEh', function=Function(arguments='{\\n\"named_entities\": [\\n  {\\n    \"entity\": \"Leonard Hoffstaedter\",\\n    \"label\": \"Person\"\\n  },\\n  {\\n    \"entity\": \"Pasadena\",\\n    \"label\": \"City\"\\n  },\\n  {\\n    \"entity\": \"CA\",\\n    \"label\": \"State\"\\n  }\\n]\\n}', name='named_entity_recognition'), type='function')]), content_filter_results={})], created=1711971776, model='gpt-4', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=68, prompt_tokens=142, total_tokens=210), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n\n\n\n# retrieve the result\nimport json \n\nresult = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\nfor named_entity in result[\"named_entities\"]: \n    print(f\"{named_entity['entity']}: {named_entity['label']}\")\n\nLeonard Hoffstaedter: Person\nPasadena: City\nCA: State\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Exercise: NER with tool calling"
    ]
  },
  {
    "objectID": "llm/prompting.html",
    "href": "llm/prompting.html",
    "title": "Prompting",
    "section": "",
    "text": "Learning prompting is a science for itself. The difficulty lies in the probabilistic nature of the language models. That means, small changes to your prompt (that you might even find insignificant) can have a large impact on the result/the answer. In particular, the changes do not have to be “logical”, i.e., depend on your changes in a comprehensible or reproducible way. This can sometimes be frustrating, but can also be avoided in many cases when following the right instructions for prompting. To do so, let’s best follow the creators.\n\n\n\n\n\n\nNote\n\n\n\nThe following is taken from the OpenAI Guide\n\n\n\nWrite clear instructions\nThese models can’t read your mind. If outputs are too long, ask for brief replies. If outputs are too simple, ask for expert-level writing. If you dislike the format, demonstrate the format you’d like to see. The less the model has to guess at what you want, the more likely you’ll get it.\nTactics:\n\nInclude details in your query to get more relevant answers\nAsk the model to adopt a persona\nUse delimiters to clearly indicate distinct parts of the input\nSpecify the steps required to complete a task\nProvide examples\nSpecify the desired length of the output \n\n\n\nProvide reference text\nLanguage models can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to these models can help in answering with fewer fabrications.\nTactics:\n\nInstruct the model to answer using a reference text\nInstruct the model to answer with citations from a reference text \n\n\n\nSplit complex tasks into simpler subtasks\nJust as it is good practice in software engineering to decompose a complex system into a set of modular components, the same is true of tasks - submitted to a language model. Complex tasks tend to have higher error rates than simpler tasks. Furthermore, complex tasks can often be re-defined as a workflow of simpler tasks in which the outputs of earlier tasks are used to construct the inputs to later tasks.\nTactics:\n\nUse intent classification to identify the most relevant instructions for a user query\nFor dialogue applications that require very long conversations, summarize or filter previous dialogue\nSummarize long documents piecewise and construct a full summary recursively \n\n\n\nGive the model time to “think”\nIf asked to multiply 17 by 28, you might not know it instantly, but can still work it out with time. Similarly, models make more reasoning errors when trying to answer right away, rather than taking time to work out an answer. Asking for a “chain of thought” before an answer can help the model reason its way toward correct answers more reliably.\nTactics:\n\nInstruct the model to work out its own solution before rushing to a conclusion\nUse inner monologue or a sequence of queries to hide the model’s reasoning process\nAsk the model if it missed anything on previous passes \n\n\n\nUse external tools\nCompensate for the weaknesses of the model by feeding it the outputs of other tools. For example, a text retrieval system (sometimes called RAG or retrieval augmented generation) can tell the model about relevant documents. A code execution engine like OpenAI’s Code Interpreter can help the model do math and run code. If a task can be done more reliably or efficiently by a tool rather than by a language model, offload it to get the best of both.\nTactics:\n\nUse embeddings-based search to implement efficient knowledge retrieval\nUse code execution to perform more accurate calculations or call external APIs\nGive the model access to specific functions \n\n\n\nTest changes systematically\nImproving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Therefore to be sure that a change is net positive to performance it may be necessary to define a comprehensive test suite (also known an as an “eval”).\nTactic:\n\nEvaluate model outputs with reference to gold-standard answers\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Large Language Models",
      "Prompting"
    ]
  },
  {
    "objectID": "embeddings/embeddings.html",
    "href": "embeddings/embeddings.html",
    "title": "Embeddings",
    "section": "",
    "text": "Word Embeddings: Word embeddings are dense vector representations of words in a continuous vector space. Each word is mapped to a high-dimensional vector where words with similar meanings or contexts are closer together in the vector space.\nContextual Embeddings: Contextual embeddings, also known as contextualized word embeddings, capture the contextual information of words based on their surrounding context in a given sentence or document. Unlike traditional word embeddings, contextual embeddings vary depending on the context in which a word appears.\nBERT Embeddings: BERT (Bidirectional Encoder Representations from Transformers) embeddings are contextual embeddings provided by OpenAI. They are generated by pre-training a Transformer-based neural network model on a large corpus of text data using masked language modeling and next sentence prediction tasks.\nGPT Embeddings: GPT (Generative Pre-trained Transformer) embeddings are also contextual embeddings provided by OpenAI. They are generated by pre-training a Transformer-based neural network model on a large corpus of text data using an autoregressive language modeling objective.\nTransformer-based Architecture: Both BERT and GPT embeddings are derived from Transformer-based architectures, which consist of multiple layers of self-attention mechanisms and feed-forward neural networks. These architectures excel at capturing long-range dependencies and contextual information in sequential data.\nPre-trained Models: OpenAI provides pre-trained BERT and GPT models that have been trained on large-scale text corpora. These pre-trained models can be fine-tuned on specific tasks or domains with labeled data to adapt their knowledge and capabilities to new applications.\nTransfer Learning: BERT and GPT embeddings support transfer learning, where the pre-trained models are used as feature extractors for downstream NLP tasks. By fine-tuning these models on task-specific data, users can leverage the knowledge encoded in the embeddings to achieve state-of-the-art performance on various natural language processing tasks.\nApplications: BERT and GPT embeddings have a wide range of applications in natural language processing tasks such as text classification, named entity recognition, sentiment analysis, question-answering, and more. They provide powerful representations of text data that capture both semantic and syntactic information.\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Embeddings"
    ]
  },
  {
    "objectID": "embeddings/applications.html",
    "href": "embeddings/applications.html",
    "title": "Applications",
    "section": "",
    "text": "Build a bot that can answer questions based on documents! Resource: https://platform.openai.com/docs/tutorials/web-qa-embeddings\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Applications"
    ]
  },
  {
    "objectID": "emb_exercise.html",
    "href": "emb_exercise.html",
    "title": "Embeddings",
    "section": "",
    "text": "Code\nclass Test: \n    pass\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sprint: Large Language Models",
    "section": "",
    "text": "Robot by DALL-E\n\n\nHello and welcome to the sprint seminar Large Language Models in the summer semester of 2024 at the University of Applied Sciences in Münster. On this website, you will find all the information you need about and around the seminar.\n\nAbout the seminar\nThe seminar is roughly divided into 3 parts of equal size: theory, training and application. In the theoretical part, you will learn about the most important topics and ideas when it comes to natural language processing and large language models. We will discuss topics like tokenization, matching, statistical text analysis and embeddings to get you started before eventually dealing with large language models themselves and their applications themselves. Already during the theory, we will make sure to code in Python alongside all the concepts and see coding examples to get familiar with it.\nAfter each small input session on a new topic, we will get to some hands-on training so that you can consolidate the knowledge you just acquired. You will solve a few (coding) exercises around all the topics yourselves. To get everyone fired up as quickly as possible, we have prepared a Jupyterlab environment that everyone can use for the solution of the exercises.\nIn the final part of the seminar we will go ahead and apply our newly acquired knowledge in our own projects. All participants will team up in teams of 2-3 and try to develop and implement their own little prototype for a small application involving a language model. More information and ideas for these projects can be found here.\nBy the way, you can (and maybe absolutely should) use a language model like ChatGPT also during this seminar and the solution of some of the exercises. However, feel encouraged to try for yourselves first, and make sure you have understood the solution of a language model if you use it.\n\n\nHow to use this script\nThis script is meant to give a comprehensive overview right away from the start. Feel free to browse it even before we have reached a specific topic, in particular, if you already have some prior knowledge in the topic. All exercises that we will solve together in this seminar are contained in this script as well, including their solution. For all exercises, the (or more precisely, a) solution is hidden behind a Show solution button. For the sake of your own learning process, try to solve the exercises yourselves first! If you’re stuck, ask for a quick hint. If you still feel like you do not advance any more, then check out the solution and try to understand it. The solution of the exercises is not part of the evaluation, so it’s really for your own progress! A “summary” of all exercises can be found in (TODO: Link).\n\n\n\n\n\n\nImportant\n\n\n\nA small disclaimer: As this is the first round of the seminar, this script is not (yet) ridiculously comprehensive. And, of course, we cannot cover the full realm of NLP and LLM within a 4-days-course. However, you should find everything we will do in the seminar also in this script. If there is something missing, I will make sure to include it as soon as possible, just give me a note.\n\n\n\n\nWhat you will learn\nTODO: When finalized, do a quick summary here.\n\n\nThe schedule\nThis seminar is spread over 4 days of roughly 8 hours, of course with some breaks and modifications if we need them. The schedule for this semester is the following (the included hours are just some estimations):\n\nDay 1 (24.04.2024):\n\nGetting to know each other + intro survey (experiences & expectations) (1h)\nLearning goals & final evaluation criteria (0.5h)\nIntroduction & overview of the topic (0.5h)\nIntroduction to natural language processing & setup of the development environment (4h)\nIntroduction to LLM & getting to know the OpenAI API: Part 1 (2h)\n\n\n\nDay 2 (25.04.2024):\n\nIntroduction to LLM & getting to know the OpenAI API: Part 2 (3h)\nPrompting (1h)\nEmbeddings (2h)\nGroup brainstorming session: Designing a simple app concept involving GPT (2h)\n\n→ At home until next week: refine project ideas (1h)\n\n\nDay 3 (30.04.2024):\n\nAdvanced GPT-related topics (1h)\nBusiness-related topics (1h)\nTeam building for hackathon → develop app concepts (1h)\nWork on prototypes (5h)\n\n\n\nDay 4 (02.05.2024):\n\nFinal touches for the prototypes (3h)\nPresentation of app prototypes, peer feedback & evaluation (2h)\nReflections on the seminar (1h)\nEthics & data privacy considerations (backup)\n\n\n\nAfter the seminar (~1d):\n\nPrototype refinement\nCode review & documentation\nRefine business case & potential applications of prototype\nReflections & lessons learned → Hand in 2-page summary\n\n\n\n\nEvaluation\nAll seminar participants will be evaluated in the following way.\n\nYour presentation on the last day of the seminar: 25%\nYour prototype: 35%\nYour summary: 25%\nYour activity during the seminar: 15%\n\nI will allow myself to give your evaluation a little extra boost for good activity during the seminar. This seminar is designed for everyone to participate, so the more you do, the more fun it will be!\n\n\n\n\n\n\nNote\n\n\n\nHas this seminar been created with a little help of language models? Absolutely, why wouldn’t it? :)\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "About",
      "Sprint: Large Language Models"
    ]
  },
  {
    "objectID": "ethics/bias.html",
    "href": "ethics/bias.html",
    "title": "Bias",
    "section": "",
    "text": "Back to top",
    "crumbs": [
      "Seminar",
      "Ethical Considerations",
      "Bias"
    ]
  },
  {
    "objectID": "nlp/tokenization.html",
    "href": "nlp/tokenization.html",
    "title": "Tokenization",
    "section": "",
    "text": "A key element for a computer to understand the words we speak or type is the concept of word tokenization. For a human, the sentence\n\nsentence = \"I love reading science fiction books or books about science.\"\n\nis easy to understand since we are able to split the sentence into its individual parts in order to figure out the meaning of the full sentence. For a computer, the sentence is just a simple string of characters, like any other word or longer text. In order to make a computer understand the meaning of a sentence, we need to help break it down into its relevant parts.\nSimply put, word tokenization is the process of breaking down a piece of text into individual words or so-called tokens. It is like taking a sentence and splitting it into smaller pieces, where each piece represents a word. Word tokenization involves analyzing the text character by character and identifying boundaries between words. It uses various rules and techniques to decide where one word ends and the next one begins. For example, spaces, punctuation marks, and special characters often serve as natural boundaries between words.\nSo let’s start breaking down the sentence into its individual parts.\n\ntokenized_sentence = sentence.split(\" \")\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science.']\n\n\nOnce we have tokenized the sentence, we can start analyzing it with some simple statistical methods. For example, in order to figure out what the sentence might be about, we could count the most frequent words.\n\nfrom collections import Counter\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n\n[('books', 2), ('I', 1)]\n\n\nUnfortunately, we already realize that we have not done the best job with our “tokenizer”: The second occurrence of the word science is missing do to the punctuation. While this is great as it holds information about the ending of a sentence, it disturbs our analysis here, so let’s get rid of it.\n\ntokenized_sentence = sentence.replace(\".\", \" \").split(\" \")\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n\n[('science', 2), ('books', 2)]\n\n\nSo that worked. As you can imagine, tokenization can get increasingly difficult when we have to deal with all sorts of situations in larger corpora of texts (see also the exercise). So it is great that there are already all sorts of libraries available that can help us with this process.\n\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\n\ntokenized_sentence = wordpunct_tokenize(sentence)\ntokenized_sentence = [t for t in tokenized_sentence if t not in punctuation]\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science']",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Tokenization"
    ]
  },
  {
    "objectID": "nlp/tokenization.html#simple-word-tokenization",
    "href": "nlp/tokenization.html#simple-word-tokenization",
    "title": "Tokenization",
    "section": "",
    "text": "A key element for a computer to understand the words we speak or type is the concept of word tokenization. For a human, the sentence\n\nsentence = \"I love reading science fiction books or books about science.\"\n\nis easy to understand since we are able to split the sentence into its individual parts in order to figure out the meaning of the full sentence. For a computer, the sentence is just a simple string of characters, like any other word or longer text. In order to make a computer understand the meaning of a sentence, we need to help break it down into its relevant parts.\nSimply put, word tokenization is the process of breaking down a piece of text into individual words or so-called tokens. It is like taking a sentence and splitting it into smaller pieces, where each piece represents a word. Word tokenization involves analyzing the text character by character and identifying boundaries between words. It uses various rules and techniques to decide where one word ends and the next one begins. For example, spaces, punctuation marks, and special characters often serve as natural boundaries between words.\nSo let’s start breaking down the sentence into its individual parts.\n\ntokenized_sentence = sentence.split(\" \")\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science.']\n\n\nOnce we have tokenized the sentence, we can start analyzing it with some simple statistical methods. For example, in order to figure out what the sentence might be about, we could count the most frequent words.\n\nfrom collections import Counter\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n\n[('books', 2), ('I', 1)]\n\n\nUnfortunately, we already realize that we have not done the best job with our “tokenizer”: The second occurrence of the word science is missing do to the punctuation. While this is great as it holds information about the ending of a sentence, it disturbs our analysis here, so let’s get rid of it.\n\ntokenized_sentence = sentence.replace(\".\", \" \").split(\" \")\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n\n[('science', 2), ('books', 2)]\n\n\nSo that worked. As you can imagine, tokenization can get increasingly difficult when we have to deal with all sorts of situations in larger corpora of texts (see also the exercise). So it is great that there are already all sorts of libraries available that can help us with this process.\n\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\n\ntokenized_sentence = wordpunct_tokenize(sentence)\ntokenized_sentence = [t for t in tokenized_sentence if t not in punctuation]\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science']",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Tokenization"
    ]
  },
  {
    "objectID": "nlp/tokenization.html#advanced-word-tokenization",
    "href": "nlp/tokenization.html#advanced-word-tokenization",
    "title": "Tokenization",
    "section": "Advanced word tokenization",
    "text": "Advanced word tokenization\nThe above ideas illustrate well the idea of tokenization of splitting text into smaller chunks that we can feed to a language model. In practice, especially in models like GPT, a critical component is the vocabulary or the set of unique words or tokens the model understands. Traditional approaches use fixed-size vocabularies, which means every unique word in the corpus has its own representation (index or embedding) in the model’s vocabulary. However, as the vocabulary size increases (for example, by including more languages), so does the memory requirement, which can be impractical for large-scale language models. One solution is the so-called bit-pair encoding. Bit pair encoding is a data compression technique specifically designed to tackle the issue of large vocabularies in language models. Instead of assigning a unique index or embedding to each token, bit pair encoding identifies frequent pairs of characters (bits) within the corpus and represents them as a single token. This effectively reduces the size of the vocabulary while preserving the essential information needed for language modeling tasks.\n\nHow Bit Pair Encoding Works:\n\nTokenization: The first step in bit pair encoding is tokenization, where the text corpus is broken down into individual tokens. These tokens could be characters, subwords, or words, depending on the tokenization strategy used.\nPair Identification: Next, the algorithm identifies pairs of characters (bits) that occur frequently within the corpus. These pairs are typically consecutive characters in the text.\nReplacement with Single Token: Once frequent pairs are identified, they are replaced with a single token. This effectively reduces the number of unique tokens in the vocabulary.\nIterative Process: The process of identifying frequent pairs and replacing them with single tokens is iterative. It continues until a predefined stopping criterion is met, such as reaching a target vocabulary size or when no more frequent pairs can be found.\nVocabulary Construction: After the iterative process, a vocabulary is constructed, consisting of the single tokens generated through pair replacement, along with any remaining tokens from the original tokenization process.\nEncoding and Decoding: During training and inference, text data is encoded using the constructed vocabulary, where each token is represented by its corresponding index in the vocabulary. During decoding, the indices are mapped back to their respective tokens.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is very illustrative to use the the OpenAI tokenizer to see how a sentence is split up into different token. Try mixing languages and standard as well as more rare words and observe how they are split up.\nAnother detailed example can be found here.\n\n\n\n\nAdvantages of Bit Pair Encoding:\n\nEfficient Memory Usage: Bit pair encoding significantly reduces the size of the vocabulary, leading to more efficient memory usage, especially in large-scale language models.\nRetains Information: Despite reducing the vocabulary size, bit pair encoding retains important linguistic information by capturing frequent character pairs.\nFlexible: Bit pair encoding is flexible and can be adapted to different tokenization strategies and corpus characteristics.\n\n\n\nLimitations and Considerations:\n\nComputational Overhead: The iterative nature of bit pair encoding can be computationally intensive, especially for large corpora.\nLoss of Granularity: While bit pair encoding reduces vocabulary size, it may lead to a loss of granularity, especially for rare or out-of-vocabulary words.\nTokenization Strategy: The effectiveness of bit pair encoding depends on the tokenization strategy used and the characteristics of the corpus.\n\n\n\n\n\n\n\nTip\n\n\n\nFrom the OpenAI Guide:\nA helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words).",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Tokenization"
    ]
  },
  {
    "objectID": "nlp/exercises/ex_fuzzy_matching.html",
    "href": "nlp/exercises/ex_fuzzy_matching.html",
    "title": "Exercise: Fuzzy matching",
    "section": "",
    "text": "Task: Use fuzzy matching and the rapidfuzz library to find the keywords in the text.\nInstructions:\n\nSome keywords with multiple words, partial ratio etc.\n\nTODO: Finalize this!\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Exercise: Fuzzy matching"
    ]
  },
  {
    "objectID": "nlp/exercises/ex_tfidf.html",
    "href": "nlp/exercises/ex_tfidf.html",
    "title": "Exercise: TF-IDF",
    "section": "",
    "text": "Task: Extend the code for the bag of words to TF-IDF.\nInstructions:\n\nSome instructions\n\nTODO: Finalize this!\n\n\nShow solution\n\n\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\nfrom typing import List\n\nfrom nltk.corpus import stopwords\n# python -m nltk.downloader stopwords -&gt; run this in your console once to get the stopwords\n\ndef preprocess_text(text: str) -&gt; List[str]:\n    # tokenize text\n    tokens = wordpunct_tokenize(text.lower())\n\n    # remove punctuation\n    tokens = [t for t in tokens if t not in punctuation]\n\n    # remove stopwords\n    stop_words = stopwords.words(\"english\")\n    tokens = [t for t in tokens if t not in stop_words]\n\n    return tokens\n\n\nfrom collections import Counter\nimport math\n\n\ndef calculate_tf(word_counts, total_words):\n    # Calculate Term Frequency (TF)\n    tf = {}\n    for word, count in word_counts.items():\n        tf[word] = count / total_words\n    return tf\n\ndef calculate_idf(word_counts, num_documents):\n    # Calculate Inverse Document Frequency (IDF)\n    idf = {}\n    for word, count in word_counts.items():\n        idf[word] = math.log((1 + num_documents) / (1 + count))\n    return idf\n\ndef create_tf_idf(texts):\n    # Count the frequency of each word in the corpus and total number of words\n    word_counts = Counter()\n    total_words = 0\n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Update word counts and total number of words\n        word_counts.update(words)\n        total_words += len(words)\n    \n    # Create sorted vocabulary\n    vocabulary = sorted(word_counts.keys())\n    \n    # Calculate TF-IDF for each document\n    tf_idf_vectors = []\n    num_documents = len(texts)\n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Calculate TF for the document\n        tf = calculate_tf(Counter(words), len(words))\n        \n        # Calculate IDF based on word counts across all documents\n        idf = calculate_idf(word_counts, num_documents)\n        \n        # Calculate TF-IDF for the document\n        tf_idf_vector = {}\n        for word in vocabulary:\n            tf_idf_vector[word] = round(tf.get(word, 0) * idf[word], 2)\n        \n        # Sort the IFIDF vector based on the vocabulary order\n        sorted_tfidf_vector = [tf_idf_vector[word] for word in vocabulary]\n        \n        # Append the BoW vector to the list\n        tf_idf_vectors.append(sorted_tfidf_vector)\n    \n    return vocabulary, tf_idf_vectors\n\n# Example texts\ntexts = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\",\n]\n\n# Create TF-IDF vectors\nvocabulary, tf_idf_vectors = create_tf_idf(texts)\n\n# Print vocabulary\nprint(\"Vocabulary:\")\nprint(vocabulary)\n\n# Print TF-IDF vectors\nprint(\"\\nTF-IDF Vectors:\")\nfor i, tf_idf_vector in enumerate(tf_idf_vectors):\n    print(f\"Document {i + 1}: {tf_idf_vector}\")\n\nVocabulary:\n['document', 'first', 'one', 'second', 'third']\n\nTF-IDF Vectors:\nDocument 1: [0.0, 0.26, 0.0, 0.0, 0.0]\nDocument 2: [0.0, 0.0, 0.0, 0.31, 0.0]\nDocument 3: [0.0, 0.0, 0.46, 0.0, 0.46]\nDocument 4: [0.0, 0.26, 0.0, 0.0, 0.0]\n\n\n\nTask: Find some documents and apply this to it.\nInstructions:\n\nFind the closest matching documents.\n\nTODO: Finalize this!\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Exercise: TF-IDF"
    ]
  },
  {
    "objectID": "nlp/fuzzy_matching.html",
    "href": "nlp/fuzzy_matching.html",
    "title": "Fuzzy matching",
    "section": "",
    "text": "As can be seen from the previous example, the detection of certain keywords from a text can prove more difficult than one might expect. The key issues stem from the fact that natural language has many facets such as conjugation, singular and plural forms, adjectives vs. adverbs etc. But even when these are handled, there remain challenges for keywords detection. In the previous example, our detection still fails when:\n\nkeywords consist of multiple words (product portfolio),\nkeywords have different forms but mean the same (advertisment vs. advertising),\nkeywords have wrong spelling (langscpe vs. landscape),\nkeywords and target words are not exactly the same thing but closely related (analysis vs. analyst).\n\nThe former case can be handled by using so called n-grams. In contrast to the single words we used for word tokens, n-grams are sequences of n consecutive words in a text, thus capturing some more of the context in a simple way. Let’s see a simple example for 2-grams:\n\nfrom nltk import ngrams\n\nsentence = \"The CEO announced plans to diversify the company's product portfolio...\"\n\nfor n_gram in ngrams(sentence.split(\" \"), n=2):\n  print(n_gram)\n\n('The', 'CEO')\n('CEO', 'announced')\n('announced', 'plans')\n('plans', 'to')\n('to', 'diversify')\n('diversify', 'the')\n('the', \"company's\")\n(\"company's\", 'product')\n('product', 'portfolio...')\n\n\nIn order to detect keywords consisting of more than a single word we can now split our text into n-grams for different n(e.g., 2, 3, 4) and compare these to our keywords.\nIn order to handle the other three cases, we a different approach. So let us first notice that, for all three cases, the word we are trying to compare are very similar (in terms of the contained letters) but not exactly equal. So what if we had a way to define a similarity between words and texts or, more generally, between any strings? One solution for this is fuzzy matching. Instead of considering two strings a match if they are exactly equal, fuzzy matching assigns a score to the pair. If the score is high enough, we might consider the pair a match.\n\n\nSome details about fuzzy matching\n\nFuzzy string matching is a technique used to find strings that are approximately similar to a given pattern, even if there are differences in spelling, punctuation, or word order. It is particularly useful in situations where exact string matching is not feasible due to variations or errors in the data. Fuzzy matching algorithms compute a similarity score between pairs of strings, typically based on criteria such as character similarity, substring matching, or token-based similarity. These algorithms often employ techniques like Levenshtein distance, which measures the minimum number of single-character edits required to transform one string into another, or tokenization to compare sets or sorted versions of tokens. Overall, fuzzy string matching enables the identification of similar strings, facilitating tasks such as record linkage, spell checking, and approximate string matching in various applications, including natural language processing, data cleaning, and information retrieval.\n\nLet’s see how this works using the package rapidfuzz.\n\nfrom rapidfuzz import fuzz\n\nword_pairs = [\n  (\"advertisment\", \"advertising\"),\n  (\"landscpe\", \"landscape\"),\n  (\"analysis\", \"analyst\")\n]\n\nfor word_pair in word_pairs:\n  ratio = fuzz.ratio(\n    s1=word_pair[0], \n    s2=word_pair[1]\n  )\n  print(f\"Similarity score '{word_pair[0]} - '{word_pair[1]}': {round(ratio, 2)}.\")\n\nSimilarity score 'advertisment - 'advertising': 78.26.\nSimilarity score 'landscpe - 'landscape': 94.12.\nSimilarity score 'analysis - 'analyst': 80.0.\n\n\nLet us use fuzzy matching on order to detect some of the missing keywords from the previous example.\n\nfrom pprint import pprint\nfrom nltk.tokenize import wordpunct_tokenize\n\ntokenized_text = wordpunct_tokenize(text=text)\n\nmin_score = 75\n\nmatches = []\nfor token in tokenized_text:\n  for keyword in keywords:\n    ratio = fuzz.ratio(\n      s1=token.lower(), \n      s2=keyword.lower()\n    )\n    if ratio &gt;= min_score:\n      matches.append(\n        (keyword, token, round(ratio, 2))\n      )\n\npprint(matches)\n\n[('Quarter', 'quarterly', 87.5),\n ('Earnings', 'earnings', 100.0),\n ('Report', 'reports', 92.31),\n ('Analysis', 'analysts', 87.5),\n ('Stock', 'stock', 100.0),\n ('Investor', 'investor', 100.0),\n ('Announce', 'announced', 94.12),\n ('Diversity', 'diversify', 88.89),\n ('Market', 'markets', 92.31),\n ('Market', 'marketing', 80.0),\n ('Advertisment', 'advertising', 78.26),\n ('Landscpe', 'landscape', 94.12)]\n\n\nAs we can see we have now successfully found most of the keywords we were looking for. However, we can also see a new caveat: We have now detected two possible matches for Market: Marketing and Markets. In this case, we can simply pick the one with the higher score and we are good, but there will be cases where it is more difficult to decide, whether a match, even with a higher score, is actually a match.\nFuzzy matching can, of course, also be used to compare n-grams or even entire texts to each other (see also the documentation of rapidfuzz and the next exercise); however there are certain limits to how practical it can be. But the concept in general already gives us some good evidence that, in order to compare words and text to each other, we would like to be able to somehow calculate with text. In the next sections, we will see ways how to do that more efficiently.\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Fuzzy matching"
    ]
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Resources",
      "Resource 2"
    ]
  }
]