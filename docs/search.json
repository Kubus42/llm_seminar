[
  {
    "objectID": "test_viz.html",
    "href": "test_viz.html",
    "title": "",
    "section": "",
    "text": "# prerequisites\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.manifold import TSNE\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n# Define a list of words to visualize\nwords = [\"python\", \"javascript\", \"c++\", \"reptile\", \"snake\"]\n\n# Get embeddings for the words\nresponse = client.embeddings.create(\n    input=words,\n    model=MODEL\n)\n\nembeddings = [emb.embedding for emb in response.data]\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n    n_components=2, \n    random_state=42,\n    perplexity=4 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    plt.scatter(x, y, marker='o', color='red')\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()"
  },
  {
    "objectID": "nlp/history.html",
    "href": "nlp/history.html",
    "title": "A short history of NLP",
    "section": "",
    "text": "Tasks:\n\nPart-of-speech tagging\nNamed entity recognition\nMachine translation\nSentiment analysis\nText classification\nInformation extraction\nQuestion-answering\n\nChallenges:\n\nAmbiguity of language\nDifferent languages\nRule-based approaches cannot handle nuances and complexity of human language\nBias\nImportance of context\nWorld knowledge\nCommon sense reasoning",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "A short history of NLP"
    ]
  },
  {
    "objectID": "nlp/exercises/ex_tokenization.html",
    "href": "nlp/exercises/ex_tokenization.html",
    "title": "Exercise: Tokenization",
    "section": "",
    "text": "# write/extend tokenizer",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Exercise: Tokenization"
    ]
  },
  {
    "objectID": "nlp/statistical_text_analysis.html",
    "href": "nlp/statistical_text_analysis.html",
    "title": "Statistical text analysis",
    "section": "",
    "text": "Bag of Words -&gt; “Simple” form of embeddings\nExercise: Create a bag of words\nTerm frequency\nTF-IDF\nClustering? -&gt; Ref to embeddings later on\n\n\ntext = \"\"\nwith open(\"../assets/chapter1.txt\", \"r\") as file:  \n    for line in file:\n        text += line.strip()\n        \ntext = text.lower()\n\n\nfrom nltk.tokenize import wordpunct_tokenize\n\ntokens = wordpunct_tokenize(text)\n\nfrom string import punctuation\nno_punctuation = [t for t in tokens if t not in punctuation]\n\n\nfrom nltk.corpus import stopwords\n\n# python -m nltk.downloader stopwords -&gt; run this in your console once to get the stopwords\n\nstop_words = stopwords.words(\"english\")\n\nno_stopwords = [t for t in no_punctuation if t not in stop_words]\n\nfrom collections import Counter\n\nCounter(no_stopwords).most_common(15)\n\n[('one', 35),\n ('winston', 32),\n ('face', 28),\n ('even', 24),\n ('--', 24),\n ('big', 22),\n ('could', 19),\n ('party', 18),\n ('would', 18),\n ('moment', 18),\n ('like', 17),\n ('brother', 15),\n ('goldstein', 15),\n ('telescreen', 14),\n ('seemed', 14)]",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Statistical text analysis"
    ]
  },
  {
    "objectID": "nlp/tokenization.html",
    "href": "nlp/tokenization.html",
    "title": "Tokenization",
    "section": "",
    "text": "TODO: Some introductory sentence.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Tokenization"
    ]
  },
  {
    "objectID": "nlp/tokenization.html#simple-word-tokenization",
    "href": "nlp/tokenization.html#simple-word-tokenization",
    "title": "Tokenization",
    "section": "Simple word tokenization",
    "text": "Simple word tokenization\nA key element for a computer to understand the words we speak or type is the concept of word tokenization. For a human, the sentence\n\nsentence = \"I love reading science fiction books or books about science.\"\n\nis easy to understand since we are able to split the sentence into its individual parts in order to figure out the meaning of the full sentence. For a computer, the sentence is just a simple string of characters, like any other word or longer text. In order to make a computer understand the meaning of a sentence, we need to help break it down into its relevant parts.\nSimply put, word tokenization is the process of breaking down a piece of text into individual words or so-called tokens. It is like taking a sentence and splitting it into smaller pieces, where each piece represents a word. Word tokenization involves analyzing the text character by character and identifying boundaries between words. It uses various rules and techniques to decide where one word ends and the next one begins. For example, spaces, punctuation marks, and special characters often serve as natural boundaries between words.\nSo let’s start breaking down the sentence into its individual parts.\n\ntokenized_sentence = sentence.split(\" \")\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science.']\n\n\nOnce we have tokenized the sentence, we can start anaylzing it with some simple statistical methods. For example, in order to figure out what the sentence might be about, we could count the most frequent words.\n\nfrom collections import Counter\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n\n[('books', 2), ('I', 1)]\n\n\nUnfortunately, we already realize that we have not done the best job with our “tokenizer”: The second occurence of the word science is missing do to the punctuation. While this is great as it holds information about the ending of a sentence, it disturbs our analysis here, so let’s get rid of it.\n\ntokenized_sentence = sentence.replace(\".\", \" \").split(\" \")\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n\n[('science', 2), ('books', 2)]\n\n\nSo that worked. As you can imagine, tokenization can get increasingly difficult when we have to deal with all sorts of situations in larger corpora of texts (see also the exercise). So it is great that there are already all sorts of libraries available that can help us with this process.\n\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\n\ntokenized_sentence = wordpunct_tokenize(sentence)\ntokenized_sentence = [t for t in tokenized_sentence if t not in punctuation]\nprint(tokenized_sentence)\n\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science']",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Tokenization"
    ]
  },
  {
    "objectID": "nlp/tokenization.html#advanced-word-tokenization",
    "href": "nlp/tokenization.html#advanced-word-tokenization",
    "title": "Tokenization",
    "section": "Advanced word tokenization",
    "text": "Advanced word tokenization\nTODO: Write",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Tokenization"
    ]
  },
  {
    "objectID": "about/projects.html",
    "href": "about/projects.html",
    "title": "Projects",
    "section": "",
    "text": "AI Assistant: Chatbot for a specific topic and behavior with GPT (”AI Assistant”)\nDocument tagging / classification: with embeddings and GPT (in any flavor)\nClustering of text-based entities: Based on embeddings and clustering algorithms\nText-based RPG Game: Develop a text-based role-playing game where players interact with characters and navigate through a story generated by GPT. Players make choices that influence the direction of the narrative.\nSentiment Analysis Tool: Build an app that analyzes the sentiment of text inputs (e.g., social media posts, customer reviews) using GPT. Users can input text, and the app provides insights into the overall sentiment expressed in the text.\nText Summarization Tool: Create an application that summarizes long blocks of text into shorter, concise summaries. Users can input articles, essays, or documents, and the tool generates a summarized version.\nLanguage Translation Tool: Build a simple translation app that utilizes GPT to translate text between different languages. Users can input text in one language, and the app outputs the translated text in the desired language. Has to include some nice tweaks.\nStory Generation Game: Develop a storytelling game where users provide prompts or keywords, and GPT generates a short story based on those inputs. Users can then rate the generated stories for creativity and coherence.\nQuestion-Answering Chatbot: Build a chatbot that can answer questions posed by users on a specific topic (e.g., based on documents). Users input their questions, and the chatbot retrieves relevant information from a pre-trained GPT model or specific documents).\nPersonalized Recipe Generator: Develop an app that generates personalized recipes based on user preferences and dietary restrictions. Users input their preferred ingredients and dietary needs, and the app generates custom recipes using GPT.\nLyrics Generator: Create a lyrics generation tool that generates lyrics based on user input such as themes, music style, emotions, or keywords. Users can explore different poetic styles and themes generated by GPT.\nText Summarizer",
    "crumbs": [
      "Seminar",
      "About",
      "Projects"
    ]
  },
  {
    "objectID": "embeddings/exercises/ex_emb_similarity.html",
    "href": "embeddings/exercises/ex_emb_similarity.html",
    "title": "Exercise: Embedding similarity",
    "section": "",
    "text": "Task: Use the OpenAI embeddings API to compute the similarity between two given words or phrases.\nInstructions:\n\nChoose two words or phrases with similar or related meanings.\nUse the OpenAI embeddings API to obtain embeddings for both words or phrases.\nCalculate the cosine similarity between the embeddings to measure their similarity.\nPrint the similarity score and interpret the results.\n\nSolution\n\n\nCode\nimport numpy as np\n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -&gt; float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\n\n\n\nCode\nimport os\n\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n# create the embeddings\nword_1 = \"king\"\nword_2 = \"queen\"\n\nresponse_1 = client.embeddings.create(input=word_1, model=MODEL)\nembedding_1 = response_1.data[0].embedding\nresponse_2 = client.embeddings.create(input=word_2, model=MODEL)\nembedding_2 = response_2.data[0].embedding\n\n\n\n\nCode\n# calculate the distance \ndist_12 = cosine_similarity(embedding_1, embedding_2)\nprint(f\"Cosine similarity between {word_1} and {word_2}: {round(dist_12, 3)}.\")\n\n\nCosine similarity between king and queen: 0.915.\n\n\n\n\nCode\nword_3 = \"pawn\"\nembedding_3 = client.embeddings.create(input=word_3, model=MODEL).data[0].embedding\n\ndist_13 = cosine_similarity(embedding_1, embedding_3)\nprint(f\"Cosine similarity between {word_1} and {word_3}: {round(dist_13, 3)}.\")\n\n\nCosine similarity between king and pawn: 0.829.\n\n\nTask: Use the OpenAI embeddings API and simple embedding arithmetics to introduce more context to word similarities.\nInstructions:\n\nCreate embeddings for the following three words: python, snake, javascript using the OpenAI API.\nCalculate the cosine similarity between each pair.\nCreate another embedding for the word reptile and add it to python. You can use numpy for this.\nCalculate the cosine similarity between python and this sum. What do you notice?\n\nSolution\n\n\nCode\nwords = [\"python\", \"snake\", \"javascript\", \"reptile\"]\nresponse = client.embeddings.create(input=words, model=MODEL)\nembeddings = [emb.embedding for emb in response.data]\n\n\n\n\nCode\nprint(f\"Similarity between '{words[0]}' and '{words[1]}': {round(cosine_similarity(embeddings[0], embeddings[1]), 3)}.\")\nprint(f\"Similarity between '{words[0]}' and '{words[2]}': {round(cosine_similarity(embeddings[0], embeddings[2]), 3)}.\")\nprint(f\"Similarity between '{words[0]} + {words[3]}' and '{words[1]}': {round(cosine_similarity(np.array(embeddings[0]) + np.array(embeddings[3]), embeddings[1]), 3)}.\")\n\n\nSimilarity between 'python' and 'snake': 0.841.\nSimilarity between 'python' and 'javascript': 0.85.\nSimilarity between 'python + reptile' and 'snake': 0.894.",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Exercise: Embedding similarity"
    ]
  },
  {
    "objectID": "embeddings/embeddings.html",
    "href": "embeddings/embeddings.html",
    "title": "Embeddings",
    "section": "",
    "text": "Word Embeddings: Word embeddings are dense vector representations of words in a continuous vector space. Each word is mapped to a high-dimensional vector where words with similar meanings or contexts are closer together in the vector space.\nContextual Embeddings: Contextual embeddings, also known as contextualized word embeddings, capture the contextual information of words based on their surrounding context in a given sentence or document. Unlike traditional word embeddings, contextual embeddings vary depending on the context in which a word appears.\nBERT Embeddings: BERT (Bidirectional Encoder Representations from Transformers) embeddings are contextual embeddings provided by OpenAI. They are generated by pre-training a Transformer-based neural network model on a large corpus of text data using masked language modeling and next sentence prediction tasks.\nGPT Embeddings: GPT (Generative Pre-trained Transformer) embeddings are also contextual embeddings provided by OpenAI. They are generated by pre-training a Transformer-based neural network model on a large corpus of text data using an autoregressive language modeling objective.\nTransformer-based Architecture: Both BERT and GPT embeddings are derived from Transformer-based architectures, which consist of multiple layers of self-attention mechanisms and feed-forward neural networks. These architectures excel at capturing long-range dependencies and contextual information in sequential data.\nPre-trained Models: OpenAI provides pre-trained BERT and GPT models that have been trained on large-scale text corpora. These pre-trained models can be fine-tuned on specific tasks or domains with labeled data to adapt their knowledge and capabilities to new applications.\nTransfer Learning: BERT and GPT embeddings support transfer learning, where the pre-trained models are used as feature extractors for downstream NLP tasks. By fine-tuning these models on task-specific data, users can leverage the knowledge encoded in the embeddings to achieve state-of-the-art performance on various natural language processing tasks.\nApplications: BERT and GPT embeddings have a wide range of applications in natural language processing tasks such as text classification, named entity recognition, sentiment analysis, question-answering, and more. They provide powerful representations of text data that capture both semantic and syntactic information.",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Embeddings"
    ]
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/idna-3.6.dist-info/LICENSE.html",
    "href": "script_venv/lib/python3.8/site-packages/idna-3.6.dist-info/LICENSE.html",
    "title": "",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2023, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "href": "script_venv/lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html",
    "title": "UAT for NbAgg backend.",
    "section": "",
    "text": "from imp import reload\nThe first line simply reloads matplotlib, uses the nbagg backend and then reloads the backend, just to ensure we have the latest modification to the backend code. Note: The underlying JavaScript will not be updated by this process, so a refresh of the browser after clearing the output and saving is necessary to clear everything fully.\nimport matplotlib\nreload(matplotlib)\n\nmatplotlib.use('nbagg')\n\nimport matplotlib.backends.backend_nbagg\nreload(matplotlib.backends.backend_nbagg)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "href": "script_venv/lib/python3.8/site-packages/matplotlib/backends/web_backend/nbagg_uat.html#uat-13---animation",
    "title": "UAT for NbAgg backend.",
    "section": "UAT 13 - Animation",
    "text": "UAT 13 - Animation\nThe following should generate an animated line:\n\nimport matplotlib.animation as animation\nimport numpy as np\n\nfig, ax = plt.subplots()\n\nx = np.arange(0, 2*np.pi, 0.01)        # x-array\nline, = ax.plot(x, np.sin(x))\n\ndef animate(i):\n    line.set_ydata(np.sin(x+i/10.0))  # update the data\n    return line,\n\n#Init only required for blitting to give a clean slate.\ndef init():\n    line.set_ydata(np.ma.array(x, mask=True))\n    return line,\n\nani = animation.FuncAnimation(fig, animate, np.arange(1, 200), init_func=init,\n                              interval=100., blit=True)\nplt.show()\n\n\nUAT 14 - Keyboard shortcuts in IPython after close of figure\nAfter closing the previous figure (with the close button above the figure) the IPython keyboard shortcuts should still function.\n\n\nUAT 15 - Figure face colours\nThe nbagg honours all colours apart from that of the figure.patch. The two plots below should produce a figure with a red background. There should be no yellow figure.\n\nimport matplotlib\nmatplotlib.rcParams.update({'figure.facecolor': 'red',\n                            'savefig.facecolor': 'yellow'})\nplt.figure()\nplt.plot([3, 2, 1])\n\nplt.show()\n\n\n\nUAT 16 - Events\nPressing any keyboard key or mouse button (or scrolling) should cycle the line while the figure has focus. The figure should have focus by default when it is created and re-gain it by clicking on the canvas. Clicking anywhere outside of the figure should release focus, but moving the mouse out of the figure should not release focus.\n\nimport itertools\nfig, ax = plt.subplots()\nx = np.linspace(0,10,10000)\ny = np.sin(x)\nln, = ax.plot(x,y)\nevt = []\ncolors = iter(itertools.cycle(['r', 'g', 'b', 'k', 'c']))\ndef on_event(event):\n    if event.name.startswith('key'):\n        fig.suptitle('%s: %s' % (event.name, event.key))\n    elif event.name == 'scroll_event':\n        fig.suptitle('%s: %s' % (event.name, event.step))\n    else:\n        fig.suptitle('%s: %s' % (event.name, event.button))\n    evt.append(event)\n    ln.set_color(next(colors))\n    fig.canvas.draw()\n    fig.canvas.draw_idle()\n\nfig.canvas.mpl_connect('button_press_event', on_event)\nfig.canvas.mpl_connect('button_release_event', on_event)\nfig.canvas.mpl_connect('scroll_event', on_event)\nfig.canvas.mpl_connect('key_press_event', on_event)\nfig.canvas.mpl_connect('key_release_event', on_event)\n\nplt.show()\n\n\n\nUAT 17 - Timers\nSingle-shot timers follow a completely different code path in the nbagg backend than regular timers (such as those used in the animation example above.) The next set of tests ensures that both “regular” and “single-shot” timers work properly.\nThe following should show a simple clock that updates twice a second:\n\nimport time\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\n\ndef update(text):\n    text.set(text=time.ctime())\n    text.axes.figure.canvas.draw()\n    \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\ntimer.start()\nplt.show()\n\nHowever, the following should only update once and then stop:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center') \ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\n\nplt.show()\n\nAnd the next two examples should never show any visible text at all:\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\nfig, ax = plt.subplots()\ntext = ax.text(0.5, 0.5, '', ha='center')\ntimer = fig.canvas.new_timer(500, [(update, [text], {})])\n\ntimer.single_shot = True\ntimer.start()\ntimer.stop()\n\nplt.show()\n\n\n\nUAT 18 - stopping figure when removed from DOM\nWhen the div that contains from the figure is removed from the DOM the figure should shut down it’s comm, and if the python-side figure has no more active comms, it should destroy the figure. Repeatedly running the cell below should always have the same figure number\n\nfig, ax = plt.subplots()\nax.plot(range(5))\nplt.show()\n\nRunning the cell below will re-show the figure. After this, re-running the cell above should result in a new figure number.\n\nfig.canvas.manager.reshow()\n\n\n\nUAT 19 - Blitting\nClicking on the figure should plot a green horizontal line moving up the axes.\n\nimport itertools\n\ncnt = itertools.count()\nbg = None\n\ndef onclick_handle(event):\n    \"\"\"Should draw elevating green line on each mouse click\"\"\"\n    global bg\n    if bg is None:\n        bg = ax.figure.canvas.copy_from_bbox(ax.bbox) \n    ax.figure.canvas.restore_region(bg)\n\n    cur_y = (next(cnt) % 10) * 0.1\n    ln.set_ydata([cur_y, cur_y])\n    ax.draw_artist(ln)\n    ax.figure.canvas.blit(ax.bbox)\n\nfig, ax = plt.subplots()\nax.plot([0, 1], [0, 1], 'r')\nln, = ax.plot([0, 1], [0, 0], 'g', animated=True)\nplt.show()\nax.figure.canvas.draw()\n\nax.figure.canvas.mpl_connect('button_press_event', onclick_handle)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/pyzmq-25.1.2.dist-info/AUTHORS.html",
    "href": "script_venv/lib/python3.8/site-packages/pyzmq-25.1.2.dist-info/AUTHORS.html",
    "title": "",
    "section": "",
    "text": "This project was started and continues to be led by Brian E. Granger (ellisonbg AT gmail DOT com). Min Ragan-Kelley (benjaminrk AT gmail DOT com) is the primary developer of pyzmq at this time.\nThe following people have contributed to the project:\n\nAlexander Else (alexander DOT else AT team DOT telstra DOT com)\nAlexander Pyhalov (apyhalov AT gmail DOT com)\nAlexandr Emelin (frvzmb AT gmail DOT com)\nAmr Ali (amr AT ledgerx DOT com)\nAndre Caron (andre DOT l DOT caron AT gmail DOT com)\nAndrea Crotti (andrea DOT crotti DOT 0 AT gmail DOT com)\nAndrew Gwozdziewycz (git AT apgwoz DOT com)\nBaptiste Lepilleur (baptiste DOT lepilleur AT gmail DOT com)\nBrandyn A. White (bwhite AT dappervision DOT com)\nBrian E. Granger (ellisonbg AT gmail DOT com)\nBrian Hoffman (hoffman_brian AT bah DOT com)\nCarlos A. Rocha (carlos DOT rocha AT gmail DOT com)\nChris Laws (clawsicus AT gmail DOT com)\nChristian Wyglendowski (christian AT bu DOT mp)\nChristoph Gohlke (cgohlke AT uci DOT edu)\nCurtis (curtis AT tinbrain DOT net)\nCyril Holweck (cyril DOT holweck AT free DOT fr)\nDan Colish (dcolish AT gmail DOT com)\nDaniel Lundin (dln AT eintr DOT org)\nDaniel Truemper (truemped AT googlemail DOT com)\nDouglas Creager (douglas DOT creager AT redjack DOT com)\nEduardo Stalinho (eduardooc DOT 86 AT gmail DOT com)\nEren Güven (erenguven0 AT gmail DOT com)\nErick Tryzelaar (erick DOT tryzelaar AT gmail DOT com)\nErik Tollerud (erik DOT tollerud AT gmail DOT com)\nFELD Boris (lothiraldan AT gmail DOT com)\nFantix King (fantix DOT king AT gmail DOT com)\nFelipe Cruz (felipecruz AT loogica DOT net)\nFernando Perez (Fernando DOT Perez AT berkeley DOT edu)\nFrank Wiles (frank AT revsys DOT com)\nFélix-Antoine Fortin (felix DOT antoine DOT fortin AT gmail DOT com)\nGavrie Philipson (gavriep AT il DOT ibm DOT com)\nGodefroid Chapelle (gotcha AT bubblenet DOT be)\nGreg Banks (gbanks AT mybasis DOT com)\nGreg Ward (greg AT gerg DOT ca)\nGuido Goldstein (github AT a-nugget DOT de)\nIan Lee (IanLee1521 AT gmail DOT com)\nIonuț Arțăriși (ionut AT artarisi DOT eu)\nIvo Danihelka (ivo AT danihelka DOT net)\nIyed (iyed DOT bennour AT gmail DOT com)\nJim Garrison (jim AT garrison DOT cc)\nJohn Gallagher (johnkgallagher AT gmail DOT com)\nJulian Taylor (jtaylor DOT debian AT googlemail DOT com)\nJustin Bronder (jsbronder AT gmail DOT com)\nJustin Riley (justin DOT t DOT riley AT gmail DOT com)\nMarc Abramowitz (marc AT marc-abramowitz DOT com)\nMatthew Aburn (mattja6 AT gmail DOT com)\nMichel Pelletier (pelletier DOT michel AT gmail DOT com)\nMichel Zou (xantares09 AT hotmail DOT com)\nMin Ragan-Kelley (benjaminrk AT gmail DOT com)\nNell Hardcastle (nell AT dev-nell DOT com)\nNicholas Pilkington (nicholas DOT pilkington AT gmail DOT com)\nNicholas Piël (nicholas AT nichol DOT as)\nNick Pellegrino (npellegrino AT mozilla DOT com)\nNicolas Delaby (nicolas DOT delaby AT ezeep DOT com)\nOndrej Certik (ondrej AT certik DOT cz)\nPaul Colomiets (paul AT colomiets DOT name)\nPawel Jasinski (pawel DOT jasinski AT gmail DOT com)\nPhus Lu (phus DOT lu AT gmail DOT com)\nRobert Buchholz (rbu AT goodpoint DOT de)\nRobert Jordens (jordens AT gmail DOT com)\nRyan Cox (ryan DOT a DOT cox AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nScott Maxwell (scott AT codecobblers DOT com)\nScott Sadler (github AT mashi DOT org)\nSimon Knight (simon DOT knight AT gmail DOT com)\nStefan Friesel (sf AT cloudcontrol DOT de)\nStefan van der Walt (stefan AT sun DOT ac DOT za)\nStephen Diehl (stephen DOT m DOT diehl AT gmail DOT com)\nSylvain Corlay (scorlay AT bloomberg DOT net)\nThomas Kluyver (takowl AT gmail DOT com)\nThomas Spura (tomspur AT fedoraproject DOT org)\nTigger Bear (Tigger AT Tiggers-Mac-mini DOT local)\nTorsten Landschoff (torsten DOT landschoff AT dynamore DOT de)\nVadim Markovtsev (v DOT markovtsev AT samsung DOT com)\nYannick Hold (yannickhold AT gmail DOT com)\nZbigniew Jędrzejewski-Szmek (zbyszek AT in DOT waw DOT pl)\nhugo shi (hugoshi AT bleb2 DOT (none))\njdgleeson (jdgleeson AT mac DOT com)\nkyledj (kyle AT bucebuce DOT com)\nspez (steve AT hipmunk DOT com)\nstu (stuart DOT axon AT jpcreative DOT co DOT uk)\nxantares (xantares AT fujitsu-l64 DOT (none))\n\nas reported by:\ngit log --all --format='- %aN (%aE)' | sort -u | sed 's/@/ AT /1' | sed -e 's/\\.\\([^ ]\\)/ DOT \\1/g'\nwith some adjustments.\n\n\n\nBrandon Craig-Rhodes (brandon AT rhodesmill DOT org)\nEugene Chernyshov (chernyshov DOT eugene AT gmail DOT com)\nCraig Austin (craig DOT austin AT gmail DOT com)\n\n\n\n\n\nTravis Cline (travis DOT cline AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nZachary Voase (z AT zacharyvoase DOT com)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/pyzmq-25.1.2.dist-info/AUTHORS.html#authors",
    "href": "script_venv/lib/python3.8/site-packages/pyzmq-25.1.2.dist-info/AUTHORS.html#authors",
    "title": "",
    "section": "",
    "text": "This project was started and continues to be led by Brian E. Granger (ellisonbg AT gmail DOT com). Min Ragan-Kelley (benjaminrk AT gmail DOT com) is the primary developer of pyzmq at this time.\nThe following people have contributed to the project:\n\nAlexander Else (alexander DOT else AT team DOT telstra DOT com)\nAlexander Pyhalov (apyhalov AT gmail DOT com)\nAlexandr Emelin (frvzmb AT gmail DOT com)\nAmr Ali (amr AT ledgerx DOT com)\nAndre Caron (andre DOT l DOT caron AT gmail DOT com)\nAndrea Crotti (andrea DOT crotti DOT 0 AT gmail DOT com)\nAndrew Gwozdziewycz (git AT apgwoz DOT com)\nBaptiste Lepilleur (baptiste DOT lepilleur AT gmail DOT com)\nBrandyn A. White (bwhite AT dappervision DOT com)\nBrian E. Granger (ellisonbg AT gmail DOT com)\nBrian Hoffman (hoffman_brian AT bah DOT com)\nCarlos A. Rocha (carlos DOT rocha AT gmail DOT com)\nChris Laws (clawsicus AT gmail DOT com)\nChristian Wyglendowski (christian AT bu DOT mp)\nChristoph Gohlke (cgohlke AT uci DOT edu)\nCurtis (curtis AT tinbrain DOT net)\nCyril Holweck (cyril DOT holweck AT free DOT fr)\nDan Colish (dcolish AT gmail DOT com)\nDaniel Lundin (dln AT eintr DOT org)\nDaniel Truemper (truemped AT googlemail DOT com)\nDouglas Creager (douglas DOT creager AT redjack DOT com)\nEduardo Stalinho (eduardooc DOT 86 AT gmail DOT com)\nEren Güven (erenguven0 AT gmail DOT com)\nErick Tryzelaar (erick DOT tryzelaar AT gmail DOT com)\nErik Tollerud (erik DOT tollerud AT gmail DOT com)\nFELD Boris (lothiraldan AT gmail DOT com)\nFantix King (fantix DOT king AT gmail DOT com)\nFelipe Cruz (felipecruz AT loogica DOT net)\nFernando Perez (Fernando DOT Perez AT berkeley DOT edu)\nFrank Wiles (frank AT revsys DOT com)\nFélix-Antoine Fortin (felix DOT antoine DOT fortin AT gmail DOT com)\nGavrie Philipson (gavriep AT il DOT ibm DOT com)\nGodefroid Chapelle (gotcha AT bubblenet DOT be)\nGreg Banks (gbanks AT mybasis DOT com)\nGreg Ward (greg AT gerg DOT ca)\nGuido Goldstein (github AT a-nugget DOT de)\nIan Lee (IanLee1521 AT gmail DOT com)\nIonuț Arțăriși (ionut AT artarisi DOT eu)\nIvo Danihelka (ivo AT danihelka DOT net)\nIyed (iyed DOT bennour AT gmail DOT com)\nJim Garrison (jim AT garrison DOT cc)\nJohn Gallagher (johnkgallagher AT gmail DOT com)\nJulian Taylor (jtaylor DOT debian AT googlemail DOT com)\nJustin Bronder (jsbronder AT gmail DOT com)\nJustin Riley (justin DOT t DOT riley AT gmail DOT com)\nMarc Abramowitz (marc AT marc-abramowitz DOT com)\nMatthew Aburn (mattja6 AT gmail DOT com)\nMichel Pelletier (pelletier DOT michel AT gmail DOT com)\nMichel Zou (xantares09 AT hotmail DOT com)\nMin Ragan-Kelley (benjaminrk AT gmail DOT com)\nNell Hardcastle (nell AT dev-nell DOT com)\nNicholas Pilkington (nicholas DOT pilkington AT gmail DOT com)\nNicholas Piël (nicholas AT nichol DOT as)\nNick Pellegrino (npellegrino AT mozilla DOT com)\nNicolas Delaby (nicolas DOT delaby AT ezeep DOT com)\nOndrej Certik (ondrej AT certik DOT cz)\nPaul Colomiets (paul AT colomiets DOT name)\nPawel Jasinski (pawel DOT jasinski AT gmail DOT com)\nPhus Lu (phus DOT lu AT gmail DOT com)\nRobert Buchholz (rbu AT goodpoint DOT de)\nRobert Jordens (jordens AT gmail DOT com)\nRyan Cox (ryan DOT a DOT cox AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nScott Maxwell (scott AT codecobblers DOT com)\nScott Sadler (github AT mashi DOT org)\nSimon Knight (simon DOT knight AT gmail DOT com)\nStefan Friesel (sf AT cloudcontrol DOT de)\nStefan van der Walt (stefan AT sun DOT ac DOT za)\nStephen Diehl (stephen DOT m DOT diehl AT gmail DOT com)\nSylvain Corlay (scorlay AT bloomberg DOT net)\nThomas Kluyver (takowl AT gmail DOT com)\nThomas Spura (tomspur AT fedoraproject DOT org)\nTigger Bear (Tigger AT Tiggers-Mac-mini DOT local)\nTorsten Landschoff (torsten DOT landschoff AT dynamore DOT de)\nVadim Markovtsev (v DOT markovtsev AT samsung DOT com)\nYannick Hold (yannickhold AT gmail DOT com)\nZbigniew Jędrzejewski-Szmek (zbyszek AT in DOT waw DOT pl)\nhugo shi (hugoshi AT bleb2 DOT (none))\njdgleeson (jdgleeson AT mac DOT com)\nkyledj (kyle AT bucebuce DOT com)\nspez (steve AT hipmunk DOT com)\nstu (stuart DOT axon AT jpcreative DOT co DOT uk)\nxantares (xantares AT fujitsu-l64 DOT (none))\n\nas reported by:\ngit log --all --format='- %aN (%aE)' | sort -u | sed 's/@/ AT /1' | sed -e 's/\\.\\([^ ]\\)/ DOT \\1/g'\nwith some adjustments.\n\n\n\nBrandon Craig-Rhodes (brandon AT rhodesmill DOT org)\nEugene Chernyshov (chernyshov DOT eugene AT gmail DOT com)\nCraig Austin (craig DOT austin AT gmail DOT com)\n\n\n\n\n\nTravis Cline (travis DOT cline AT gmail DOT com)\nRyan Kelly (ryan AT rfk DOT id DOT au)\nZachary Voase (z AT zacharyvoase DOT com)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/httpcore-1.0.4.dist-info/licenses/LICENSE.html",
    "href": "script_venv/lib/python3.8/site-packages/httpcore-1.0.4.dist-info/licenses/LICENSE.html",
    "title": "",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html",
    "href": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html",
    "title": "Authors",
    "section": "",
    "text": "pyqode.qt: Colin Duquesnoy (@ColinDuquesnoy)\nspyderlib.qt: Pierre Raybaut (@PierreRaybaut)\nqt-helpers: Thomas Robitaille (@astrofrog)\n\n\n\n\n\nDaniel Althviz (@dalthviz)\nCarlos Cordoba (@ccordoba12)\nC.A.M. Gerlach (@CAM-Gerlach)\nSpyder Development Team (Spyder-IDE)\n\n\n\n\n\nThe QtPy Contributors"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#original-authors",
    "href": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#original-authors",
    "title": "Authors",
    "section": "",
    "text": "pyqode.qt: Colin Duquesnoy (@ColinDuquesnoy)\nspyderlib.qt: Pierre Raybaut (@PierreRaybaut)\nqt-helpers: Thomas Robitaille (@astrofrog)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#current-maintainers",
    "href": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#current-maintainers",
    "title": "Authors",
    "section": "",
    "text": "Daniel Althviz (@dalthviz)\nCarlos Cordoba (@ccordoba12)\nC.A.M. Gerlach (@CAM-Gerlach)\nSpyder Development Team (Spyder-IDE)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#contributors",
    "href": "script_venv/lib/python3.8/site-packages/QtPy-2.4.1.dist-info/AUTHORS.html#contributors",
    "title": "Authors",
    "section": "",
    "text": "The QtPy Contributors"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/wasabi/tests/test-data/wasabi-test-notebook.html",
    "href": "script_venv/lib/python3.8/site-packages/wasabi/tests/test-data/wasabi-test-notebook.html",
    "title": "",
    "section": "",
    "text": "import sys\nimport wasabi\n\nwasabi.msg.warn(\"This is a test. This is only a test.\")\nif sys.version_info &gt;= (3, 7):\n    assert wasabi.util.supports_ansi()\n\nprint(sys.stdout)"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/soupsieve-2.5.dist-info/licenses/LICENSE.html",
    "href": "script_venv/lib/python3.8/site-packages/soupsieve-2.5.dist-info/licenses/LICENSE.html",
    "title": "",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2023 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html",
    "href": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html",
    "title": "Natural Language Toolkit (NLTK) Authors",
    "section": "",
    "text": "Steven Bird stevenbird1@gmail.com\nEdward Loper edloper@gmail.com\nEwan Klein ewan@inf.ed.ac.uk\n\n\n\n\n\nTom Aarsen\nRami Al-Rfou’\nMark Amery\nGreg Aumann\nIvan Barria\nIngolf Becker\nYonatan Becker\nPaul Bedaride\nSteven Bethard\nRobert Berwick\nDan Blanchard\nNathan Bodenstab\nAlexander Böhm\nFrancis Bond\nPaul Bone\nJordan Boyd-Graber\nDaniel Blanchard\nPhil Blunsom\nLars Buitinck\nCristian Capdevila\nSteve Cassidy\nChen-Fu Chiang\nDmitry Chichkov\nJinyoung Choi\nAndrew Clausen\nLucas Champollion\nGraham Christensen\nTrevor Cohn\nDavid Coles\nTom Conroy https://github.com/tconroy\nClaude Coulombe\nLucas Cooper\nRobin Cooper\nChris Crowner\nJames Curran\nArthur Darcet\nDariel Dato-on\nSelina Dennis\nLeon Derczynski\nAlexis Dimitriadis\nNikhil Dinesh\nLiang Dong\nDavid Doukhan\nRebecca Dridan\nPablo Duboue\nLong Duong\nChristian Federmann\nCampion Fellin\nMichelle Fullwood\nDan Garrette\nMaciej Gawinecki\nJean Mark Gawron\nSumukh Ghodke\nYoav Goldberg\nMichael Wayne Goodman\nDougal Graham\nBrent Gray\nSimon Greenhill\nClark Grubb\nEduardo Pereira Habkost\nMasato Hagiwara\nLauri Hallila\nMichael Hansen\nYurie Hara\nWill Hardy\nTyler Hartley\nPeter Hawkins\nSaimadhav Heblikar\nFredrik Hedman\nHelder\nMichael Heilman\nOfer Helman\nChristopher Hench\nBruce Hill\nAmy Holland\nKristy Hollingshead\nMarcus Huderle\nBaden Hughes\nNancy Ide\nRebecca Ingram\nEdward Ivanovic\nThomas Jakobsen\nNick Johnson\nEric Kafe\nPiotr Kasprzyk\nAngelos Katharopoulos\nSudharshan Kaushik\nChris Koenig\nMikhail Korobov\nDenis Krusko\nIlia Kurenkov\nStefano Lattarini\nPierre-François Laquerre\nStefano Lattarini\nHaejoong Lee\nJackson Lee\nMax Leonov\nChris Liechti\nHyuckin David Lim\nTom Lippincott\nPeter Ljunglöf\nAlex Louden\nJoseph Lynch\nNitin Madnani\nFelipe Madrigal\nBjørn Mæland\nDean Malmgren\nChristopher Maloof\nRob Malouf\nIker Manterola\nCarl de Marcken\nMitch Marcus\nTorsten Marek\nRobert Marshall\nMarius Mather\nDuncan McGreggor\nDavid McClosky\nXinfan Meng\nDmitrijs Milajevs\nMargaret Mitchell\nTomonori Nagano\nJason Narad\nShari A’aidil Nasruddin\nLance Nathan\nMorten Neergaard\nDavid Nemeskey\nEric Nichols\nJoel Nothman\nAlireza Nourian\nAlexander Oleynikov\nPierpaolo Pantone\nTed Pedersen\nJacob Perkins\nAlberto Planas\nOndrej Platek\nAlessandro Presta\nQi Liu\nMartin Thorsen Ranang\nMichael Recachinas\nBrandon Rhodes\nJoshua Ritterman\nWill Roberts\nStuart Robinson\nCarlos Rodriguez\nLorenzo Rubio\nAlex Rudnick\nJussi Salmela\nGeoffrey Sampson\nKepa Sarasola\nKevin Scannell\nNathan Schneider\nRico Sennrich\nThomas Skardal\nEric Smith\nLynn Soe\nRob Speer\nPeter Spiller\nRichard Sproat\nCeri Stagg\nPeter Stahl\nOliver Steele\nThomas Stieglmaier\nJan Strunk\nLiling Tan\nClaire Taylor\nLouis Tiao\nSteven Tomcavage\nTiago Tresoldi\nMarcus Uneson\nYu Usami\nPetro Verkhogliad\nPeter Wang\nZhe Wang\nCharlotte Wilson\nChuck Wooters\nSteven Xu\nBeracah Yankama\nLei Ye (叶磊)\nPatrick Ye\nGeraldine Sim Wei Ying\nJason Yoder\nThomas Zieglier\n0ssifrage\nducki13\nkiwipi\nlade\nisnowfy\nonesandzeros\npquentin\nwvanlint\nÁlvaro Justen https://github.com/turicas\nbjut-hz\nSergio Oller\nWill Monroe\nElijah Rippeth\nEmil Manukyan\nCasper Lehmann-Strøm\nAndrew Giel\nTanin Na Nakorn\nLinghao Zhang\nColin Carroll\nHeguang Miao\nHannah Aizenman (story645)\nGeorge Berry\nAdam Nelson\nJ Richard Snape\nAlex Constantin alex@keyworder.ch\nTsolak Ghukasyan\nPrasasto Adi\nSafwan Kamarrudin\nArthur Tilley\nVilhjalmur Thorsteinsson\nJaehoon Hwang https://github.com/jaehoonhwang\nChintan Shah https://github.com/chintanshah24\nsbagan\nZicheng Xu\nAlbert Au Yeung https://github.com/albertauyeung\nShenjian Zhao\nDeng Wang https://github.com/lmatt-bit\nAli Abdullah\nStoytcho Stoytchev\nLakhdar Benzahia\nKheireddine Abainia https://github.com/xprogramer\nYibin Lin https://github.com/yibinlin\nArtiem Krinitsyn\nBjörn Mattsson\nOleg Chislov\nPavan Gururaj Joshi https://github.com/PavanGJ\nEthan Hill https://github.com/hill1303\nVivek Lakshmanan\nSomnath Rakshit https://github.com/somnathrakshit\nAnlan Du\nPulkit Maloo https://github.com/pulkitmaloo\nBrandon M. Burroughs https://github.com/brandonmburroughs\nJohn Stewart https://github.com/free-variation\nIaroslav Tymchenko https://github.com/myproblemchild\nAleš Tamchyna\nTim Gianitsos https://github.com/timgianitsos\nPhilippe Partarrieu https://github.com/ppartarr\nAndrew Owen Martin\nAdrian Ellis https://github.com/adrianjellis\nNat Quayle Nelson https://github.com/nqnstudios\nYanpeng Zhao https://github.com/zhaoyanpeng\nMatan Rak https://github.com/matanrak\nNick Ulle https://github.com/nick-ulle\nUday Krishna https://github.com/udaykrishna\nOsman Zubair https://github.com/okz12\nViresh Gupta https://github.com/virresh\nOndřej Cífka https://github.com/cifkao\nIris X. Zhou https://github.com/irisxzhou\nDevashish Lal https://github.com/BLaZeKiLL\nGerhard Kremer https://github.com/GerhardKa\nNicolas Darr https://github.com/ndarr\nHervé Nicol https://github.com/hervenicol\nAlexandre H. T. Dias https://github.com/alexandredias3d\nDaksh Shah https://github.com/Daksh\nJacob Weightman https://github.com/jacobdweightman\nBonifacio de Oliveira https://github.com/Bonifacio2\nArmins Bagrats Stepanjans https://github.com/ab-10\nVassilis Palassopoulos https://github.com/palasso\nRam Rachum https://github.com/cool-RR\nOr Sharir https://github.com/orsharir\nDenali Molitor https://github.com/dmmolitor\nJacob Moorman https://github.com/jdmoorman\nCory Nezin https://github.com/corynezin\nMatt Chaput\nDanny Sepler https://github.com/dannysepler\nAkshita Bhagia https://github.com/AkshitaB\nPratap Yadav https://github.com/prtpydv\nHiroki Teranishi https://github.com/chantera\nRuben Cartuyvels https://github.com/rubencart\nDalton Pearson https://github.com/daltonpearson\nRobby Horvath https://github.com/robbyhorvath\nGavish Poddar https://github.com/gavishpoddar\nSaibo Geng https://github.com/Saibo-creator\nAhmet Yildirim https://github.com/RnDevelover\nYuta Nakamura https://github.com/yutanakamura-tky\nAdam Hawley https://github.com/adamjhawley\nPanagiotis Simakis https://github.com/sp1thas\nRichard Wang https://github.com/richarddwang\nAlexandre Perez-Lebel https://github.com/aperezlebel\nFernando Carranza https://github.com/fernandocar86\nMartin Kondratzky https://github.com/martinkondra\nHeungson Lee https://github.com/heungson\nM.K. Pawelkiewicz https://github.com/hamiltonianflow\nSteven Thomas Smith https://github.com/essandess\nJan Lennartz https://github.com/Madnex\n\n\n\n\n\n\n\nMartin Porter\nVivake Gupta\nBarry Wilkins\nHiranmay Ghosh\nChris Emerson\n\n\n\n\n\nAssem Chelli\nAbdelkrim Aries\nLakhdar Benzahia"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#original-authors",
    "href": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#original-authors",
    "title": "Natural Language Toolkit (NLTK) Authors",
    "section": "",
    "text": "Steven Bird stevenbird1@gmail.com\nEdward Loper edloper@gmail.com\nEwan Klein ewan@inf.ed.ac.uk"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#contributors",
    "href": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#contributors",
    "title": "Natural Language Toolkit (NLTK) Authors",
    "section": "",
    "text": "Tom Aarsen\nRami Al-Rfou’\nMark Amery\nGreg Aumann\nIvan Barria\nIngolf Becker\nYonatan Becker\nPaul Bedaride\nSteven Bethard\nRobert Berwick\nDan Blanchard\nNathan Bodenstab\nAlexander Böhm\nFrancis Bond\nPaul Bone\nJordan Boyd-Graber\nDaniel Blanchard\nPhil Blunsom\nLars Buitinck\nCristian Capdevila\nSteve Cassidy\nChen-Fu Chiang\nDmitry Chichkov\nJinyoung Choi\nAndrew Clausen\nLucas Champollion\nGraham Christensen\nTrevor Cohn\nDavid Coles\nTom Conroy https://github.com/tconroy\nClaude Coulombe\nLucas Cooper\nRobin Cooper\nChris Crowner\nJames Curran\nArthur Darcet\nDariel Dato-on\nSelina Dennis\nLeon Derczynski\nAlexis Dimitriadis\nNikhil Dinesh\nLiang Dong\nDavid Doukhan\nRebecca Dridan\nPablo Duboue\nLong Duong\nChristian Federmann\nCampion Fellin\nMichelle Fullwood\nDan Garrette\nMaciej Gawinecki\nJean Mark Gawron\nSumukh Ghodke\nYoav Goldberg\nMichael Wayne Goodman\nDougal Graham\nBrent Gray\nSimon Greenhill\nClark Grubb\nEduardo Pereira Habkost\nMasato Hagiwara\nLauri Hallila\nMichael Hansen\nYurie Hara\nWill Hardy\nTyler Hartley\nPeter Hawkins\nSaimadhav Heblikar\nFredrik Hedman\nHelder\nMichael Heilman\nOfer Helman\nChristopher Hench\nBruce Hill\nAmy Holland\nKristy Hollingshead\nMarcus Huderle\nBaden Hughes\nNancy Ide\nRebecca Ingram\nEdward Ivanovic\nThomas Jakobsen\nNick Johnson\nEric Kafe\nPiotr Kasprzyk\nAngelos Katharopoulos\nSudharshan Kaushik\nChris Koenig\nMikhail Korobov\nDenis Krusko\nIlia Kurenkov\nStefano Lattarini\nPierre-François Laquerre\nStefano Lattarini\nHaejoong Lee\nJackson Lee\nMax Leonov\nChris Liechti\nHyuckin David Lim\nTom Lippincott\nPeter Ljunglöf\nAlex Louden\nJoseph Lynch\nNitin Madnani\nFelipe Madrigal\nBjørn Mæland\nDean Malmgren\nChristopher Maloof\nRob Malouf\nIker Manterola\nCarl de Marcken\nMitch Marcus\nTorsten Marek\nRobert Marshall\nMarius Mather\nDuncan McGreggor\nDavid McClosky\nXinfan Meng\nDmitrijs Milajevs\nMargaret Mitchell\nTomonori Nagano\nJason Narad\nShari A’aidil Nasruddin\nLance Nathan\nMorten Neergaard\nDavid Nemeskey\nEric Nichols\nJoel Nothman\nAlireza Nourian\nAlexander Oleynikov\nPierpaolo Pantone\nTed Pedersen\nJacob Perkins\nAlberto Planas\nOndrej Platek\nAlessandro Presta\nQi Liu\nMartin Thorsen Ranang\nMichael Recachinas\nBrandon Rhodes\nJoshua Ritterman\nWill Roberts\nStuart Robinson\nCarlos Rodriguez\nLorenzo Rubio\nAlex Rudnick\nJussi Salmela\nGeoffrey Sampson\nKepa Sarasola\nKevin Scannell\nNathan Schneider\nRico Sennrich\nThomas Skardal\nEric Smith\nLynn Soe\nRob Speer\nPeter Spiller\nRichard Sproat\nCeri Stagg\nPeter Stahl\nOliver Steele\nThomas Stieglmaier\nJan Strunk\nLiling Tan\nClaire Taylor\nLouis Tiao\nSteven Tomcavage\nTiago Tresoldi\nMarcus Uneson\nYu Usami\nPetro Verkhogliad\nPeter Wang\nZhe Wang\nCharlotte Wilson\nChuck Wooters\nSteven Xu\nBeracah Yankama\nLei Ye (叶磊)\nPatrick Ye\nGeraldine Sim Wei Ying\nJason Yoder\nThomas Zieglier\n0ssifrage\nducki13\nkiwipi\nlade\nisnowfy\nonesandzeros\npquentin\nwvanlint\nÁlvaro Justen https://github.com/turicas\nbjut-hz\nSergio Oller\nWill Monroe\nElijah Rippeth\nEmil Manukyan\nCasper Lehmann-Strøm\nAndrew Giel\nTanin Na Nakorn\nLinghao Zhang\nColin Carroll\nHeguang Miao\nHannah Aizenman (story645)\nGeorge Berry\nAdam Nelson\nJ Richard Snape\nAlex Constantin alex@keyworder.ch\nTsolak Ghukasyan\nPrasasto Adi\nSafwan Kamarrudin\nArthur Tilley\nVilhjalmur Thorsteinsson\nJaehoon Hwang https://github.com/jaehoonhwang\nChintan Shah https://github.com/chintanshah24\nsbagan\nZicheng Xu\nAlbert Au Yeung https://github.com/albertauyeung\nShenjian Zhao\nDeng Wang https://github.com/lmatt-bit\nAli Abdullah\nStoytcho Stoytchev\nLakhdar Benzahia\nKheireddine Abainia https://github.com/xprogramer\nYibin Lin https://github.com/yibinlin\nArtiem Krinitsyn\nBjörn Mattsson\nOleg Chislov\nPavan Gururaj Joshi https://github.com/PavanGJ\nEthan Hill https://github.com/hill1303\nVivek Lakshmanan\nSomnath Rakshit https://github.com/somnathrakshit\nAnlan Du\nPulkit Maloo https://github.com/pulkitmaloo\nBrandon M. Burroughs https://github.com/brandonmburroughs\nJohn Stewart https://github.com/free-variation\nIaroslav Tymchenko https://github.com/myproblemchild\nAleš Tamchyna\nTim Gianitsos https://github.com/timgianitsos\nPhilippe Partarrieu https://github.com/ppartarr\nAndrew Owen Martin\nAdrian Ellis https://github.com/adrianjellis\nNat Quayle Nelson https://github.com/nqnstudios\nYanpeng Zhao https://github.com/zhaoyanpeng\nMatan Rak https://github.com/matanrak\nNick Ulle https://github.com/nick-ulle\nUday Krishna https://github.com/udaykrishna\nOsman Zubair https://github.com/okz12\nViresh Gupta https://github.com/virresh\nOndřej Cífka https://github.com/cifkao\nIris X. Zhou https://github.com/irisxzhou\nDevashish Lal https://github.com/BLaZeKiLL\nGerhard Kremer https://github.com/GerhardKa\nNicolas Darr https://github.com/ndarr\nHervé Nicol https://github.com/hervenicol\nAlexandre H. T. Dias https://github.com/alexandredias3d\nDaksh Shah https://github.com/Daksh\nJacob Weightman https://github.com/jacobdweightman\nBonifacio de Oliveira https://github.com/Bonifacio2\nArmins Bagrats Stepanjans https://github.com/ab-10\nVassilis Palassopoulos https://github.com/palasso\nRam Rachum https://github.com/cool-RR\nOr Sharir https://github.com/orsharir\nDenali Molitor https://github.com/dmmolitor\nJacob Moorman https://github.com/jdmoorman\nCory Nezin https://github.com/corynezin\nMatt Chaput\nDanny Sepler https://github.com/dannysepler\nAkshita Bhagia https://github.com/AkshitaB\nPratap Yadav https://github.com/prtpydv\nHiroki Teranishi https://github.com/chantera\nRuben Cartuyvels https://github.com/rubencart\nDalton Pearson https://github.com/daltonpearson\nRobby Horvath https://github.com/robbyhorvath\nGavish Poddar https://github.com/gavishpoddar\nSaibo Geng https://github.com/Saibo-creator\nAhmet Yildirim https://github.com/RnDevelover\nYuta Nakamura https://github.com/yutanakamura-tky\nAdam Hawley https://github.com/adamjhawley\nPanagiotis Simakis https://github.com/sp1thas\nRichard Wang https://github.com/richarddwang\nAlexandre Perez-Lebel https://github.com/aperezlebel\nFernando Carranza https://github.com/fernandocar86\nMartin Kondratzky https://github.com/martinkondra\nHeungson Lee https://github.com/heungson\nM.K. Pawelkiewicz https://github.com/hamiltonianflow\nSteven Thomas Smith https://github.com/essandess\nJan Lennartz https://github.com/Madnex"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#others-whose-work-weve-taken-and-included-in-nltk-but-who-didnt-directly-contribute-it",
    "href": "script_venv/lib/python3.8/site-packages/nltk-3.8.1.dist-info/AUTHORS.html#others-whose-work-weve-taken-and-included-in-nltk-but-who-didnt-directly-contribute-it",
    "title": "Natural Language Toolkit (NLTK) Authors",
    "section": "",
    "text": "Martin Porter\nVivake Gupta\nBarry Wilkins\nHiranmay Ghosh\nChris Emerson\n\n\n\n\n\nAssem Chelli\nAbdelkrim Aries\nLakhdar Benzahia"
  },
  {
    "objectID": "script_venv/lib/python3.8/site-packages/httpx-0.27.0.dist-info/licenses/LICENSE.html",
    "href": "script_venv/lib/python3.8/site-packages/httpx-0.27.0.dist-info/licenses/LICENSE.html",
    "title": "",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "embeddings/clustering.html",
    "href": "embeddings/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "# prerequisites\n\nimport os\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value\n\n# get the OpenAI client\nclient = get_openai_client(\n  model=MODEL,\n  config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n\n# Define a list of words to cluster\nwords = [\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"banana\", \"grapes\", \"cat\", \"dog\", \"happy\", \"sad\"]\n\n# Get embeddings for the words\nresponse = client.embeddings.create(\n  input=words,\n  model=MODEL\n)\n\nembeddings = [emb.embedding for emb in response.data]\n\n\n# do the clustering\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nn_clusters = 5\n\n# define the model\nkmeans = KMeans(\n  n_clusters=n_clusters,\n  n_init=\"auto\",\n  random_state=2 # do this to get the same output\n)\n\n# fit the model to the data\nkmeans.fit(np.array(embeddings))\n\n# get the cluster labels\ncluster_labels = kmeans.labels_\n\n::: {#cell-tsne-visualization of clustering .cell execution_count=5}\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n  n_components=2, \n  random_state=42,\n  perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Define a color map for clusters\ncolors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    cluster_label = cluster_labels[i]\n    color = colors[cluster_label]\n    plt.scatter(x, y, marker='o', color=color)\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\n\n\nt-SNE visualization of clustering word embeddings\n\n\n\n:::",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Clustering"
    ]
  },
  {
    "objectID": "embeddings/visualization.html",
    "href": "embeddings/visualization.html",
    "title": "Visualization",
    "section": "",
    "text": "# prerequisites\n\nimport os\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n\n# Define a list of words to visualize\nwords = [\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"banana\", \"grapes\", \"cat\", \"dog\", \"happy\", \"sad\"]\n\n# Get embeddings for the words\nresponse = client.embeddings.create(\n    input=words,\n    model=MODEL\n)\n\nembeddings = [emb.embedding for emb in response.data]\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n    n_components=2, \n    random_state=42,\n    perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    plt.scatter(x, y, marker='o', color='red')\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\n\n\n\nt-SNE visualization of word embeddings",
    "crumbs": [
      "Seminar",
      "Embeddings",
      "Visualization"
    ]
  },
  {
    "objectID": "emb_exercise.html",
    "href": "emb_exercise.html",
    "title": "Embeddings",
    "section": "",
    "text": "Code\nclass Test: \n    pass"
  },
  {
    "objectID": "nlp/exercises/ex_fuzzy_matching.html",
    "href": "nlp/exercises/ex_fuzzy_matching.html",
    "title": "Exercise: Fuzzy matching",
    "section": "",
    "text": "Task: Use fuzzy matching and the rapidfuzz library to find the keywords in the text.\nInstructions:\n\nSome keywords with multiple words, partial ratio etc.\n\nTODO: Finalize this!",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Exercise: Fuzzy matching"
    ]
  },
  {
    "objectID": "nlp/exercises/ex_word_matching.html",
    "href": "nlp/exercises/ex_word_matching.html",
    "title": "Exercise: Word matching",
    "section": "",
    "text": "Task: For each element of the following list of keywords, determine whether it is contained in the text.\nInstructions:\n\nTransform the text to lower case and use your (or a) tokenizer to split the text into word tokens.\nFirst, use a simple comparison of strings to check whether the keywords match any token. When does this approach fail?\nLemmatize the tokens from your text in order to handle some more matching cases. When does this approach still fail? Hint: Use the different options for pos in order to handle different types of words such as nouns, verbs etc.\n\n\ntext = \"The company's latest quarterly earnings reports exceeded analysts' expectations, driving up the stock price. However, concerns about future growth prospects weighed on investor sentiment. The CEO announced plans to diversify the company's product portfolio and expand into new markets, aiming to sustain long-term profitability. The marketing team launched a new advertising campaign to promote the company's flagship product, targeting key demographics. Despite challenges in the competitive landscape, the company remains committed to innovation and customer satisfaction.\"\n\n\nkeywords = [\n    \"Announce\", \n    \"Aim\",\n    \"Earnings\",\n    \"Quarter\",\n    \"Report\",\n    \"Investor\",\n    \"Analysis\",\n    \"Market\",\n    \"Diversity\",\n    \"Product portfolio\",\n    \"Advertisment\",\n    \"Stock\",\n    \"Landscpe\" # yes, this is here on purpose\n]\n\nSolution:\n\nfrom pprint import pprint\nfrom nltk.tokenize import wordpunct_tokenize\n\ntext_token = wordpunct_tokenize(text=text.lower())\ndetected_words = [\n    (keyword, keyword.lower() in text_token) for keyword in keywords\n]\npprint(detected_words)\nprint(f\"\\nDetected {sum([x[1] for x in detected_words])}/{len(keywords)} words.\")\n\n[('Announce', False),\n ('Aim', False),\n ('Earnings', True),\n ('Quarter', False),\n ('Report', False),\n ('Investor', True),\n ('Analysis', False),\n ('Market', False),\n ('Diversity', False),\n ('Product portfolio', False),\n ('Advertisment', False),\n ('Stock', True),\n ('Landscpe', False)]\n\nDetected 3/13 words.\n\n\n\nfrom nltk.stem import WordNetLemmatizer\n\nwnl = WordNetLemmatizer()\n\nlemmatized_text_token = [\n    wnl.lemmatize(w) for w in text_token\n]\ndetected_words = [\n    (keyword, keyword.lower() in lemmatized_text_token) for keyword in keywords\n]\npprint(detected_words)\nprint(f\"\\nDetected {sum([x[1] for x in detected_words])}/{len(keywords)} words.\")\n\n[('Announce', False),\n ('Aim', False),\n ('Earnings', True),\n ('Quarter', False),\n ('Report', True),\n ('Investor', True),\n ('Analysis', False),\n ('Market', True),\n ('Diversity', False),\n ('Product portfolio', False),\n ('Advertisment', False),\n ('Stock', True),\n ('Landscpe', False)]\n\nDetected 5/13 words.\n\n\n\nfully_lemmatized_text_token = []\n\nfor token in text_token:\n    lemmatized_token = token\n    for pos in [\"n\", \"v\", \"a\"]:\n        lemmatized_token = wnl.lemmatize(token, pos=pos)\n        \n        fully_lemmatized_text_token.append(lemmatized_token)\n\ndetected_words = [\n    (keyword, keyword.lower() in fully_lemmatized_text_token) for keyword in keywords\n]\npprint(detected_words)    \nprint(f\"\\nDetected {sum([x[1] for x in detected_words])}/{len(keywords)} words.\")  \n        \n\n[('Announce', True),\n ('Aim', True),\n ('Earnings', True),\n ('Quarter', False),\n ('Report', True),\n ('Investor', True),\n ('Analysis', False),\n ('Market', True),\n ('Diversity', False),\n ('Product portfolio', False),\n ('Advertisment', False),\n ('Stock', True),\n ('Landscpe', False)]\n\nDetected 7/13 words.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Exercise: Word matching"
    ]
  },
  {
    "objectID": "nlp/fuzzy_matching.html",
    "href": "nlp/fuzzy_matching.html",
    "title": "Fuzzy matching",
    "section": "",
    "text": "As can be seen from the previous example, the detection of certain keywords from a text can prove more difficult than one might expect. The key issues stem from the fact that natural language has many facets such as conjugation, singular and plural forms, adjectives vs. adverbs etc. But even when these are handled, there remain challenges for keywords detection. In the previous example, our detection still fails when:\n\nkeywords consist of multiple words (product portfolio),\nkeywords have different forms but mean the same (advertisment vs. advertising),\nkeywords have wrong spelling (langscpe vs. landscape),\nkeywords and target words are not exactly the same thing but closely related (analysis vs. analyst).\n\nThe former case can be handled by using so called n-grams. In contrast to the single words we used for word tokens, n-grams are sequences of n consecutive words in a text, thus capturing some more of the context in a simple way. Let’s see a simple example for 2-grams:\n\nfrom nltk import ngrams\n\nsentence = \"The CEO announced plans to diversify the company's product portfolio...\"\n\nfor n_gram in ngrams(sentence.split(\" \"), n=2):\n  print(n_gram)\n\n('The', 'CEO')\n('CEO', 'announced')\n('announced', 'plans')\n('plans', 'to')\n('to', 'diversify')\n('diversify', 'the')\n('the', \"company's\")\n(\"company's\", 'product')\n('product', 'portfolio...')\n\n\nIn order to detect keywords consisting of more than a single word we can now split our text into n-grams for different n(e.g., 2, 3, 4) and compare these to our keywords.\nIn order to handle the other three cases, we a different approach. So let us first notice that, for all three cases, the word we are trying to compare are very similar (in terms of the contained letters) but not exactly equal. So what if we had a way to define a similarity between words and texts or, more generally, between any strings? One solution for this is fuzzy matching. Instead of considering two strings a match if they are exactly equal, fuzzy matching assigns a score to the pair. If the score is high enough, we might consider the pair a match.\n\n\nSome details about fuzzy matching\n\nFuzzy string matching is a technique used to find strings that are approximately similar to a given pattern, even if there are differences in spelling, punctuation, or word order. It is particularly useful in situations where exact string matching is not feasible due to variations or errors in the data. Fuzzy matching algorithms compute a similarity score between pairs of strings, typically based on criteria such as character similarity, substring matching, or token-based similarity. These algorithms often employ techniques like Levenshtein distance, which measures the minimum number of single-character edits required to transform one string into another, or tokenization to compare sets or sorted versions of tokens. Overall, fuzzy string matching enables the identification of similar strings, facilitating tasks such as record linkage, spell checking, and approximate string matching in various applications, including natural language processing, data cleaning, and information retrieval.\n\nLet’s see how this works using the package rapidfuzz.\n\nfrom rapidfuzz import fuzz\n\nword_pairs = [\n  (\"advertisment\", \"advertising\"),\n  (\"landscpe\", \"landscape\"),\n  (\"analysis\", \"analyst\")\n]\n\nfor word_pair in word_pairs:\n  ratio = fuzz.ratio(\n    s1=word_pair[0], \n    s2=word_pair[1]\n  )\n  print(f\"Similarity score '{word_pair[0]} - '{word_pair[1]}': {round(ratio, 2)}.\")\n\nSimilarity score 'advertisment - 'advertising': 78.26.\nSimilarity score 'landscpe - 'landscape': 94.12.\nSimilarity score 'analysis - 'analyst': 80.0.\n\n\nLet us use fuzzy matching on order to detect some of the missing keywords from the previous example.\n\nfrom pprint import pprint\nfrom nltk.tokenize import wordpunct_tokenize\n\ntokenized_text = wordpunct_tokenize(text=text)\n\nmin_score = 75\n\nmatches = []\nfor token in tokenized_text:\n  for keyword in keywords:\n    ratio = fuzz.ratio(\n      s1=token.lower(), \n      s2=keyword.lower()\n    )\n    if ratio &gt;= min_score:\n      matches.append(\n        (keyword, token, round(ratio, 2))\n      )\n\npprint(matches)\n\n[('Quarter', 'quarterly', 87.5),\n ('Earnings', 'earnings', 100.0),\n ('Report', 'reports', 92.31),\n ('Analysis', 'analysts', 87.5),\n ('Stock', 'stock', 100.0),\n ('Investor', 'investor', 100.0),\n ('Announce', 'announced', 94.12),\n ('Diversity', 'diversify', 88.89),\n ('Market', 'markets', 92.31),\n ('Market', 'marketing', 80.0),\n ('Advertisment', 'advertising', 78.26),\n ('Landscpe', 'landscape', 94.12)]\n\n\nAs we can see we have now successfully found most of the keywords we were looking for. However, we can also see a new caveat: We have now detected two possible matches for Market: Marketing and Markets. In this case, we can simply pick the one with the higher score and we are good, but there will be cases where it is more difficult to decide, whether a match, even with a higher score, is actually a match.\nFuzzy matching can, of course, also be used to compare n-grams or even entire texts to each other (see also the documentation of rapidfuzz and the next exercise); however there are certain limits to how practical it can be. But the concept in general already gives us some good evidence that, in order to compare words and text to each other, we would like to be able to somehow calculate with text. In the next sections, we will see ways how to do that more efficiently.",
    "crumbs": [
      "Seminar",
      "Natural Language Processing",
      "Fuzzy matching"
    ]
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: A line plot on a polar axis",
    "crumbs": [
      "Resources",
      "Resource 2"
    ]
  }
]