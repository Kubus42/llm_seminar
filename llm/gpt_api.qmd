---
title: "The OpenAI API"
format:
  html:
    code-fold: false
jupyter: python3
---

::: {.callout-note}
Resource: [OpenAI API docs](https://platform.openai.com/docs/introduction){.external}
:::


Let's finally get started working with GPT. 
In this seminar, we will use the OpenAI API to work with, but there are many alternatives out there. 
We have collected a few in the [resources](../resources/apis.qmd).


### Authentication

Getting started with the OpenAI Chat Completions API requires signing up for an account on the OpenAI platform. 
Once you've registered, you'll gain access to an API key, which serves as a unique identifier for your application to authenticate requests to the API. 
This key is essential for ensuring secure communication between your application and OpenAI's servers. 
Without proper authentication, your requests will be rejected.
You can create your own account, but for the seminar we will provide the client with the credential within the University's [Jupyterlab](https://jupyter.fh-muenster.de/){.external}.

```{python}

#| echo: false
#| output: false

import os
from dotenv import load_dotenv

load_dotenv(
    dotenv_path=os.environ.get("CONFIG_PATH"),
    override=True
)
```

```{python}
# setting up the client in Python
import os
from openai import AzureOpenAI

MODEL = "gpt-4o"

client = AzureOpenAI(
    api_key=os.environ.get("AZURE_OPENAI_API_KEY"),
    azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT"),
    azure_deployment=MODEL
)

```


### Requesting Completions

Most interaction with GPT and other models consist in generating completions for prompts, i.e., providing some text with instructions and letting the language model **complete** the text one token after the other as seen [here](../llm/intro.qmd).

To request completions from the OpenAI API, we use Python to send HTTP requests to the designated API endpoint. 
These requests are structured to include various parameters that guide the generation of text completions. 
The most fundamental parameter is the prompt text, which sets the context for the completion. 
Additionally, you can specify the desired model configuration, such as the engine to use (e.g., "gpt-4"), as well as any constraints or preferences for the generated completions, such as the maximum number of tokens or the temperature for controlling creativity ([here](../llm/parameterization.qmd)).

```{python}
# creating a completion
chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": "How old is the earth?",
        }
    ],
    model=MODEL # choose the model
)
```


### Processing

Once the OpenAI API receives your request, it proceeds to process the provided prompt using the specified model. 
This process involves analyzing the context provided by the prompt and leveraging the model's pre-trained knowledge to generate text completions. 
The model employs advanced natural language processing techniques to ensure that the generated completions are coherent and contextually relevant. 
By drawing from its extensive training data and understanding of human language, the model aims to produce responses that closely align with human-like communication.

### Response

After processing your request, the OpenAI API returns a response containing the generated text completions. 
Depending on the specifics of your request, you may receive multiple completions, each accompanied by additional information such as the amount of token processed in the request, the reason why the model stopped the answer etc. 
This response provides valuable insights into the quality and relevance of the completions, allowing you to tailor your application's behavior accordingly.
Let's check it out briefly, before you explore the response object more in-depth in your next exercise.

```{python}
# check out the type of the response

print(f"Response object type: {type(chat_completion)}") # a ChatCompletion object

# print the message we want
print(f"\nResponse message: {chat_completion.choices[0].message.content}")

# check the tokens used 
print(f"\nTotal tokens used: {chat_completion.usage.total_tokens}")

```

### Error Handling

While interacting with the OpenAI API (or any API for that matter), it's crucial to implement some robust error handling mechanisms to manage any potential issues that may arise. 
The kind of classic errors include providing invalid parameters, experiencing authentication failures due to an incorrect API key, or encountering rate limiting restrictions. 
But for language models in particular, there are plenty more problems that can arise simply involving the answer we get from the model. 
Some examples are requests involving explicit language or content or restricted content etc. which are typically blocked by the API.
Other times it might simply happen that a model does not respond in a way you expected, for example, just repeating your input instead of responding properly, or not responding in the format you requested. 
Whenever we are using language model for applications, we need to be aware of this and implement the right measures to handle these situations. 
