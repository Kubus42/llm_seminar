{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Exercise: TF-IDF\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**Task:** Extend the code for the bag of words to TF-IDF. \n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Some instructions\n",
    "\n",
    "TODO: Finalize this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from string import punctuation\n",
    "from typing import List\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# python -m nltk.downloader stopwords -> run this in your console once to get the stopwords\n",
    "\n",
    "def preprocess_text(text: str) -> List[str]:\n",
    "    # tokenize text\n",
    "    tokens = wordpunct_tokenize(text.lower())\n",
    "\n",
    "    # remove punctuation\n",
    "    tokens = [t for t in tokens if t not in punctuation]\n",
    "\n",
    "    # remove stopwords\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['document', 'first', 'one', 'second', 'third']\n",
      "\n",
      "TF-IDF Vectors:\n",
      "Document 1: [0.0, 0.26, 0.0, 0.0, 0.0]\n",
      "Document 2: [0.0, 0.0, 0.0, 0.31, 0.0]\n",
      "Document 3: [0.0, 0.0, 0.46, 0.0, 0.46]\n",
      "Document 4: [0.0, 0.26, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "def calculate_tf(word_counts, total_words):\n",
    "    # Calculate Term Frequency (TF)\n",
    "    tf = {}\n",
    "    for word, count in word_counts.items():\n",
    "        tf[word] = count / total_words\n",
    "    return tf\n",
    "\n",
    "def calculate_idf(word_counts, num_documents):\n",
    "    # Calculate Inverse Document Frequency (IDF)\n",
    "    idf = {}\n",
    "    for word, count in word_counts.items():\n",
    "        idf[word] = math.log((1 + num_documents) / (1 + count))\n",
    "    return idf\n",
    "\n",
    "def create_tf_idf(texts):\n",
    "    # Count the frequency of each word in the corpus and total number of words\n",
    "    word_counts = Counter()\n",
    "    total_words = 0\n",
    "    for text in texts:\n",
    "        # Preprocess the text\n",
    "        words = preprocess_text(text)\n",
    "        \n",
    "        # Update word counts and total number of words\n",
    "        word_counts.update(words)\n",
    "        total_words += len(words)\n",
    "    \n",
    "    # Create sorted vocabulary\n",
    "    vocabulary = sorted(word_counts.keys())\n",
    "    \n",
    "    # Calculate TF-IDF for each document\n",
    "    tf_idf_vectors = []\n",
    "    num_documents = len(texts)\n",
    "    for text in texts:\n",
    "        # Preprocess the text\n",
    "        words = preprocess_text(text)\n",
    "        \n",
    "        # Calculate TF for the document\n",
    "        tf = calculate_tf(Counter(words), len(words))\n",
    "        \n",
    "        # Calculate IDF based on word counts across all documents\n",
    "        idf = calculate_idf(word_counts, num_documents)\n",
    "        \n",
    "        # Calculate TF-IDF for the document\n",
    "        tf_idf_vector = {}\n",
    "        for word in vocabulary:\n",
    "            tf_idf_vector[word] = round(tf.get(word, 0) * idf[word], 2)\n",
    "        \n",
    "        # Sort the IFIDF vector based on the vocabulary order\n",
    "        sorted_tfidf_vector = [tf_idf_vector[word] for word in vocabulary]\n",
    "        \n",
    "        # Append the BoW vector to the list\n",
    "        tf_idf_vectors.append(sorted_tfidf_vector)\n",
    "    \n",
    "    return vocabulary, tf_idf_vectors\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "vocabulary, tf_idf_vectors = create_tf_idf(texts)\n",
    "\n",
    "# Print vocabulary\n",
    "print(\"Vocabulary:\")\n",
    "print(vocabulary)\n",
    "\n",
    "# Print TF-IDF vectors\n",
    "print(\"\\nTF-IDF Vectors:\")\n",
    "for i, tf_idf_vector in enumerate(tf_idf_vectors):\n",
    "    print(f\"Document {i + 1}: {tf_idf_vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Find some documents and apply this to it.  \n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Find the closest matching documents.\n",
    "\n",
    "TODO: Finalize this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "script_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
