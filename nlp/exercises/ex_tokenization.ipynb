{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Exercise: Sentence tokenization\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "    code-overflow: wrap\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Write a sentence tokenizer that takes the given paragraph and tokenizes it into *sentences*. Then, count the number of sentences and display the result.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Start with just a simple punctuation (`.`) as the delimiter for sentences.\n",
    "- Check out the regex library `re` and the function `re.split` to include also other delimiters. Try out the following regex `r'[.:;!?]\\s*'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"The distant planet, its surface shrouded in mystery and intrigue! With its swirling clouds and alien landscapes, the planet: a tantalizing enigma to explorers and scientists alike? Oh, the wonders it conceals: ancient ruins and extraterrestrial life forms, waiting to be discovered! As the spacecraft descended through the atmosphere, anticipation filled the hearts of the crew. Little did they know, their journey was about to unveil secrets beyond their wildest imagination.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Show code</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "def tokenize_sentences_at_dot(paragraph: str) -> List[str]:\n",
    "    sentence_tokens = paragraph.split(\".\")\n",
    "    sentence_tokens = [s.strip() for s in sentence_tokens if s.strip() != \"\"] # remove white space after .\n",
    "    return sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paragraph contains 2 sentences.\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = tokenize_sentences_at_dot(paragraph=paragraph)\n",
    "print(f\"The paragraph contains {len(tokenized_sentence)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "def tokenize_sentences_at_punctuation(paragraph: str) -> List[str]:\n",
    "    sentence_tokens = re.split(r'[.:;!?]\\s*', paragraph)\n",
    "    sentence_tokens = [s.strip() for s in sentence_tokens if s.strip() != \"\"] # remove white space after .\n",
    "    \n",
    "    return sentence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The paragraph contains 7 sentences.\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = tokenize_sentences_at_punctuation(paragraph=paragraph)\n",
    "print(f\"The paragraph contains {len(tokenized_sentence)} sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distant planet, its surface shrouded in mystery and intrigue\n",
      "With its swirling clouds and alien landscapes, the planet\n",
      "a tantalizing enigma to explorers and scientists alike\n",
      "Oh, the wonders it conceals\n",
      "ancient ruins and extraterrestrial life forms, waiting to be discovered\n",
      "As the spacecraft descended through the atmosphere, anticipation filled the hearts of the crew\n",
      "Little did they know, their journey was about to unveil secrets beyond their wildest imagination\n"
     ]
    }
   ],
   "source": [
    "for sentence in tokenized_sentence:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "script_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
