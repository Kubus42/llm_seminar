---
title: "Tokenization"
format:
  html:
    code-fold: false
jupyter: python3
---

TODO: Some introductory sentence.

## Simple word tokenization
A key element for a computer to understand the words we speak or type is the concept of word tokenization. 
For a human, the sentence 

```{python}
sentence = "I love reading science fiction books or books about science."
```

is easy to understand since we are able to split the sentence into its individual parts in order to figure out the meaning of the full sentence.
For a computer, the sentence is just a simple string of characters, like any other word or longer text.
In order to make a computer understand the meaning of a sentence, we need to help break it down into its relevant parts.

Simply put, word tokenization is the process of breaking down a piece of text into individual words or so-called tokens. 
It is like taking a sentence and splitting it into smaller pieces, where each piece represents a word.
Word tokenization involves analyzing the text character by character and identifying boundaries between words. 
It uses various rules and techniques to decide where one word ends and the next one begins. 
For example, spaces, punctuation marks, and special characters often serve as natural boundaries between words.

So let's start breaking down the sentence into its individual parts.

```{python}
tokenized_sentence = sentence.split(" ")
print(tokenized_sentence)
```

Once we have tokenized the sentence, we can start analyzing it with some simple statistical methods. 
For example, in order to figure out what the sentence might be about, we could count the most frequent words. 

```{python}

from collections import Counter

token_counter = Counter(tokenized_sentence)
print(token_counter.most_common(2))
```

Unfortunately, we already realize that we have not done the best job with our "tokenizer": The second occurrence of the word `science` is missing do to the punctuation. 
While this is great as it holds information about the ending of a sentence, it disturbs our analysis here, so let's get rid of it. 

```{python}
tokenized_sentence = sentence.replace(".", " ").split(" ")

token_counter = Counter(tokenized_sentence)
print(token_counter.most_common(2))
```

So that worked.
As you can imagine, tokenization can get increasingly difficult when we have to deal with all sorts of situations in larger corpora of texts (see also the exercise). 
So it is great that there are already all sorts of libraries available that can help us with this process. 



```{python}
from nltk.tokenize import wordpunct_tokenize
from string import punctuation

tokenized_sentence = wordpunct_tokenize(sentence)
tokenized_sentence = [t for t in tokenized_sentence if t not in punctuation]
print(tokenized_sentence)
```


## Advanced word tokenization

The above ideas illustrate well the idea of tokenization of splitting text into smaller chunks that we can feed to a language model.
In practice, especially in models like GPT, a critical component is the vocabulary or the set of unique words or tokens the model understands.
Traditional approaches use fixed-size vocabularies, which means every unique word in the corpus has its own representation (index or embedding) in the model's vocabulary. 
However, as the vocabulary size increases (for example, by including more languages), so does the memory requirement, which can be impractical for large-scale language models. 
One solution is the so-called bit-pair encoding.
Bit pair encoding is a data compression technique specifically designed to tackle the issue of large vocabularies in language models. 
Instead of assigning a unique index or embedding to each token, bit pair encoding identifies frequent pairs of characters (bits) within the corpus and represents them as a single token. 
This effectively reduces the size of the vocabulary while preserving the essential information needed for language modeling tasks.


### How Bit Pair Encoding Works:

1. **Tokenization**: The first step in bit pair encoding is tokenization, where the text corpus is broken down into individual tokens. These tokens could be characters, subwords, or words, depending on the tokenization strategy used.

2. **Pair Identification**: Next, the algorithm identifies pairs of characters (bits) that occur frequently within the corpus. These pairs are typically consecutive characters in the text.

3. **Replacement with Single Token**: Once frequent pairs are identified, they are replaced with a single token. This effectively reduces the number of unique tokens in the vocabulary.

4. **Iterative Process**: The process of identifying frequent pairs and replacing them with single tokens is iterative. It continues until a predefined stopping criterion is met, such as reaching a target vocabulary size or when no more frequent pairs can be found.

5. **Vocabulary Construction**: After the iterative process, a vocabulary is constructed, consisting of the single tokens generated through pair replacement, along with any remaining tokens from the original tokenization process.

6. **Encoding and Decoding**: During training and inference, text data is encoded using the constructed vocabulary, where each token is represented by its corresponding index in the vocabulary. During decoding, the indices are mapped back to their respective tokens.


::: {.callout-tip}
It is very illustrative to use the the OpenAI [tokenizer](https://platform.openai.com/tokenizer){.external} to see how a sentence is split up into different token.
Try mixing languages and standard as well as more rare words and observe how they are split up.

Another detailed example can be found [here](https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/){.external}.
:::



### Advantages of Bit Pair Encoding:

1. **Efficient Memory Usage**: Bit pair encoding significantly reduces the size of the vocabulary, leading to more efficient memory usage, especially in large-scale language models.

2. **Retains Information**: Despite reducing the vocabulary size, bit pair encoding retains important linguistic information by capturing frequent character pairs.

3. **Flexible**: Bit pair encoding is flexible and can be adapted to different tokenization strategies and corpus characteristics.


### Limitations and Considerations:

1. **Computational Overhead**: The iterative nature of bit pair encoding can be computationally intensive, especially for large corpora.

2. **Loss of Granularity**: While bit pair encoding reduces vocabulary size, it may lead to a loss of granularity, especially for rare or out-of-vocabulary words.

3. **Tokenization Strategy**: The effectiveness of bit pair encoding depends on the tokenization strategy used and the characteristics of the corpus.



::: {.callout-tip}
__From the [OpenAI Guide](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them){.external}__:

A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly Â¾ of a word (so 100 tokens ~= 75 words).
:::




