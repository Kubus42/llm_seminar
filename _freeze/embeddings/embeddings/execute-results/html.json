{
  "hash": "b2f495031ac31941da6da9b1c9444e02",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Embeddings\nformat:\n  html:\n    code-fold: true\n---\n\n## What are embeddings? \nEmbeddings play a crucial role in representing words as dense vectors in a continuous vector space. \n\n\n\nWhile the bag of words model has been a simple and widely-used approach for representing text, it has certain limitations, including a fixed vocabulary and the inability to capture nuanced semantic relationships between words.\n\nEmbeddings address these shortcomings by leveraging the power of contextual representations. \nInstead of representing each word in the vocabulary as a one-hot encoded vector, where each word is represented by a binary vector with a dimension equal to the vocabulary size, embeddings generate dense vector representations for words that encode rich semantic information.\n\nUnlike the bag of words model, embeddings are context-aware, meaning they capture the meaning of words based on their surrounding context in the text. \nThis contextual understanding allows embeddings to capture subtle semantic relationships between words, such as synonymy, antonymy, and semantic similarity.\n\nMoreover, embeddings offer a more compact representation of words compared to the sparse vectors used in the bag of words model. By compressing the information into dense vectors of fixed dimensionality, embeddings reduce the dimensionality of the input space, making it more manageable for downstream tasks and allowing for more efficient computation.\n\n\n\n\n#### Word2Vec\n\nWord2Vec is a popular technique for generating word embeddings based on distributed representations of words in a continuous vector space. \nThe key idea behind Word2Vec is to train a neural network model to predict the surrounding words (context) of a given target word in a large corpus of text. \nThis process can be done using either the continuous bag of words (CBOW) or skip-gram architectures. \nIn the CBOW model, the input is the context words, and the output is the target word, while in the skip-gram model, the input is the target word, and the output is the context words. \nBy training the model on a large corpus of text, Word2Vec learns to encode semantic relationships between words in the form of dense vector representations (embeddings). \nThese embeddings capture syntactic and semantic similarities between words, allowing for tasks like word similarity calculation, analogy completion, and even downstream NLP tasks like sentiment analysis and machine translation. \nWord2Vec embeddings are widely used in NLP applications due to their simplicity, efficiency, and effectiveness in capturing word semantics.\n\n\n\n\n\n\n- **Word Embeddings**: Word embeddings are dense vector representations of words in a continuous vector space. Each word is mapped to a high-dimensional vector where words with similar meanings or contexts are closer together in the vector space.\n- **Contextual Embeddings**: Contextual embeddings, also known as contextualized word embeddings, capture the contextual information of words based on their surrounding context in a given sentence or document. Unlike traditional word embeddings, contextual embeddings vary depending on the context in which a word appears.\n- **BERT Embeddings**: BERT (Bidirectional Encoder Representations from Transformers) embeddings are contextual embeddings provided by OpenAI. They are generated by pre-training a Transformer-based neural network model on a large corpus of text data using masked language modeling and next sentence prediction tasks.\n- **GPT Embeddings**: GPT (Generative Pre-trained Transformer) embeddings are also contextual embeddings provided by OpenAI. They are generated by pre-training a Transformer-based neural network model on a large corpus of text data using an autoregressive language modeling objective.\n\n\n## Matching with embeddings\n\n",
    "supporting": [
      "embeddings_files"
    ],
    "filters": [],
    "includes": {}
  }
}