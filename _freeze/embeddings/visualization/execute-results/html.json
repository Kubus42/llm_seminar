{
  "hash": "9c3e901eef8d4d4523fa16efc5de41e0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Visualization & clustering of embeddings\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\n---\n\n\n\n\n\n### Visualization of embeddings\nLet's go a little bit further down the road of similarities between words or texts in general. \nWouldn't it be great if we could somehow visualize a text as a point in space and see how other texts relate to it? \nEmbeddings allow us to do that as well, with one caveat: Embeddings have too many dimensions to visualize (usually a few thousand).\nLuckily, there are tools such as principal component analysis or T-SNE available to reduce the dimension of vectors (for example, to two dimensions), while preserving most of the relations between them.\nLet's see how this works.\n\n::: {#0b54296d .cell execution_count=1}\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\nTrue\n```\n:::\n:::\n\n\n::: {#b5b056a3 .cell execution_count=2}\n``` {.python .cell-code}\n# prerequisites\nimport os\nfrom openai import OpenAI\n\nMODEL = \"text-embedding-3-small\" # choose the embedding model\n\n# get the OpenAI client\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\")    \n)\n```\n:::\n\n\n::: {#0dbcb25f .cell execution_count=3}\n``` {.python .cell-code}\n# Define a list of words to visualize\nwords = [\"king\", \"queen\", \"man\", \"woman\", \"apple\", \"banana\", \"grapes\", \"cat\", \"dog\", \"happy\", \"sad\"]\n\n# Get embeddings for the words\nresponse = client.embeddings.create(\n    input=words,\n    model=MODEL\n)\n\nembeddings = [emb.embedding for emb in response.data]\n```\n:::\n\n\n::: {#cell-tsne-visualization .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n    n_components=2, \n    random_state=42,\n    perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    plt.scatter(x, y, marker='o', color='red')\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![t-SNE visualization of word embeddings](visualization_files/figure-html/tsne-visualization-output-1.png){#tsne-visualization}\n:::\n:::\n\n\n## Clustering of embeddings\nThe great thing is, we can already see that there are some clusters forming. \nWe can again use models like KMeans to find them explicitly. \nIn this case, we obviously have five clusters, so let's try to identify them.\n\n::: {#62f2b993 .cell execution_count=5}\n``` {.python .cell-code}\n# do the clustering\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nn_clusters = 5\n\n# define the model\nkmeans = KMeans(\n  n_clusters=n_clusters,\n  n_init=\"auto\",\n  random_state=2 # do this to get the same output\n)\n\n# fit the model to the data\nkmeans.fit(np.array(embeddings))\n\n# get the cluster labels\ncluster_labels = kmeans.labels_\n```\n:::\n\n\n::: {#cell-tsne-clusters .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n  n_components=2, \n  random_state=42,\n  perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Define a color map for clusters\ncolors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(10, 8))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    cluster_label = cluster_labels[i]\n    color = colors[cluster_label]\n    plt.scatter(x, y, marker='o', color=color)\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![t-SNE visualization of word embedding clusters](visualization_files/figure-html/tsne-clusters-output-1.png){#tsne-clusters}\n:::\n:::\n\n\nThat, again, is great news. \nEmbeddings allow us to find clusters in texts, based on the semantics included. \nThis helps in many applications where documents need to be analyzed without having been seen by a human.\nMaybe you can use it in your project?\n\n",
    "supporting": [
      "visualization_files"
    ],
    "filters": [],
    "includes": {}
  }
}