{
  "hash": "b2e6d98ed4c34c193f24df3e7aa76795",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Embeddings\"\nformat: \n    revealjs:\n        theme: default\n        chalkboard: true\n        footer: \"Sprint: LLM, 2024\"\n        logo: ../../assets/logo.svg\n---\n\n## Revisiting what we know \n\nEmbeddings ... \n\n- transform text into numerical vectors\n- are used in neural network architectures\n- Key benefit: Capture **semantic** similarities and relationships between words\n\n&nbsp;\n\n- Already seen: Bag of Words \n- Issue: These embeddings do not compress!\n\n\n## What are embeddings?\n\n- Represent words and text as **dense**, numerical vectors\n- Capture rich semantic information\n- Context-aware, based on surrounding text\n- Capture subtle semantic relationships\n- Compact representation compared to simple techniques such as bag of words\n\n\n## Approaches to generate embeddings:\n- Word2Vec, GloVe, FastText\n  - Train neural network to predict surrounding words\n  - CBOW or skip-gram architectures\n  - Learns semantic relationships in continuous vector space\n\n- Transformer architectures like GPT\n- Word embeddings provided by OpenAI\n\n\n## What does it look like? \n\nTrain a model to: \n\n- predict the target word based on the (surrounding) context words, **or** \n- predict the context words given a target word\n\n\n```{mermaid}\nflowchart LR\n  A[\"Input Layer (One Hot)\"] \n  A --> B[\"Embedding Layer\"]\n  B --> C[\"Sum/Average Layer\"]\n  C --> D[\"Output Layer\"]\n```\n\n\n::: {.fragment}\n### Use of the model\nThrow away the parts after the embedding layer!\n\n```{mermaid}\nflowchart LR\n  A[\"Input Layer (One Hot)\"] \n  A --> B[\"Embedding Layer\"]\n```\n\n:::\n\n\n# Matching with embeddings\n\n## Task: Find the matching document for a prompt\n\n::: {#f2e2f7c7 .cell execution_count=1}\n``` {.python .cell-code}\ntexts = [\n  \"This is the first document.\",\n  \"This document is the second document.\",\n  \"And this is the third one.\"\n]\n\nprompt = \"Is this the first document?\"\n```\n:::\n\n\n## Get the OpenAI client\n\n::: {#45714e17 .cell execution_count=2}\n``` {.python .cell-code}\n# prerequisites\n\nimport os\nfrom llm_utils.client import get_openai_client, OpenAIModels\n\nMODEL = OpenAIModels.EMBED.value # choose the embedding model\n\n# get the OpenAI client\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n```\n:::\n\n\n## Get the embeddings\n\n::: {#8c247e09 .cell execution_count=3}\n``` {.python .cell-code}\n# get the embeddings\nresponse = client.embeddings.create(\n    input=texts,\n    model=MODEL\n)\n\ntext_embeddings = [emb.embedding for emb in response.data]\n\nresponse = client.embeddings.create(\n    input=[prompt],\n    model=MODEL\n)\n\nprompt_embedding = response.data[0].embedding\n```\n:::\n\n\n## Compute the similarity \n\n::: {#0dbffcf7 .cell output-location='fragment' execution_count=4}\n``` {.python .cell-code}\nimport numpy as np \n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -> float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\n\nfor text, text_embedding in zip(texts, text_embeddings):\n    similarity = cosine_similarity(text_embedding, prompt_embedding)\n    print(f\"{text}: {round(similarity, 2)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThis is the first document.: 0.95\nThis document is the second document.: 0.88\nAnd this is the third one.: 0.8\n```\n:::\n:::\n\n\n# Visualization and clustering\n\n## Define some words to visualize\n\n::: {#02a80e22 .cell execution_count=5}\n``` {.python .cell-code}\n# Define a list of words to visualize\nwords = [\n    \"king\", \"queen\", \"man\", \"woman\", \"apple\", \"banana\", \n    \"grapes\", \"cat\", \"dog\", \"happy\", \"sad\"\n]\n\n# Get embeddings for the words\nresponse = client.embeddings.create(\n    input=words,\n    model=MODEL\n)\n\nembeddings = [emb.embedding for emb in response.data]\n```\n:::\n\n\n## Apply T-SNE to the embedding vectors \n\n::: {#9785ce16 .cell output-location='slide' execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n    n_components=2, \n    random_state=42,\n    perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(9, 7))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    plt.scatter(x, y, marker='o', color='red')\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](embeddings_files/figure-revealjs/cell-7-output-1.png){width=718 height=555 fig-align='center'}\n:::\n:::\n\n\n## Cluster the embeddings \n\n::: {#77ac9d72 .cell execution_count=7}\n``` {.python .cell-code}\n# do the clus#| tering\nimport numpy as np\nfrom sklearn.cluster import KMeans\n\nn_clusters = 5\n\n# define the model\nkmeans = KMeans(\n  n_clusters=n_clusters,\n  n_init=\"auto\",\n  random_state=2 # do this to get the same output\n)\n\n# fit the model to the data\nkmeans.fit(np.array(embeddings))\n\n# get the cluster labels\ncluster_labels = kmeans.labels_\n```\n:::\n\n\n## Visualize with T-SNE \n\n::: {#80a31df2 .cell output-location='slide' execution_count=8}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nfrom sklearn.manifold import TSNE\n\n# Apply t-SNE dimensionality reduction\ntsne = TSNE(\n  n_components=2, \n  random_state=42,\n  perplexity=5 # see documentation to set this correctly\n)\nembeddings_2d = tsne.fit_transform(np.array(embeddings))\n\n# Define a color map for clusters\ncolors = plt.cm.viridis(np.linspace(0, 1, n_clusters))\n\n# Plot the embeddings in a two-dimensional scatter plot\nplt.figure(figsize=(9, 7))\nfor i, word in enumerate(words):\n    x, y = embeddings_2d[i]\n    cluster_label = cluster_labels[i]\n    color = colors[cluster_label]\n    plt.scatter(x, y, marker='o', color=color)\n    plt.text(x, y, word, fontsize=9)\n\nplt.xlabel(\"t-SNE dimension 1\")\nplt.ylabel(\"t-SNE dimension 2\")\nplt.grid(True)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](embeddings_files/figure-revealjs/cell-9-output-1.png){width=718 height=555 fig-align='center'}\n:::\n:::\n\n\n",
    "supporting": [
      "embeddings_files"
    ],
    "filters": [],
    "includes": {}
  }
}