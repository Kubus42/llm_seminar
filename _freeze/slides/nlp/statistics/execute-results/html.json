{
  "hash": "021f557265545c6b1696f810878d6bc4",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Simple statistical text analysis\"\nformat: \n    revealjs:\n        theme: default\n        chalkboard: true\n        footer: \"Seminar: LLM, SoSe 2025\"\n        logo: ../../assets/logo.svg\n        code-block-height: 450px\n---\n\n\n## Term frequency: Token counting\n\n::: {#18921eaf .cell output-location='slide' execution_count=1}\n``` {.python .cell-code}\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\nfrom collections import Counter\nfrom typing import List\n\nfrom nltk.corpus import stopwords\n# python -m nltk.downloader stopwords -> run this in your console once to get the stopwords\n\n\n# load a text from file\ntext = \"\"\nwith open(\"../../assets/chapter1.txt\", \"r\") as file:  \n    for line in file:\n        text += line.strip()\n\n\ndef preprocess_text(text: str) -> List[str]:\n    # tokenize text\n    tokens = wordpunct_tokenize(text.lower())\n\n    # remove punctuation\n    tokens = [t for t in tokens if t not in punctuation]\n\n    # remove stopwords\n    stop_words = stopwords.words(\"english\")\n    tokens = [t for t in tokens if t not in stop_words]\n\n    return tokens\n\n# count the most frequent words\ntokens = preprocess_text(text=text)\n\nfor t in Counter(tokens).most_common(15):\n    print(f\"{t[0]}: {t[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\none: 35\nwinston: 32\nface: 28\neven: 24\n--: 24\nbig: 22\ncould: 19\nparty: 18\nwould: 18\nmoment: 18\nlike: 17\nbrother: 15\ngoldstein: 15\ntelescreen: 14\nseemed: 14\n```\n:::\n:::\n\n\n## Bag of Words: Creating a vocabulary\n\n::: {#e0728b35 .cell output-location='slide' execution_count=2}\n``` {.python .cell-code}\nfrom collections import Counter\n\n\ndef create_bag_of_words(texts):\n    # Count the frequency of each word in the corpus\n    word_counts = Counter()\n    \n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Update word counts\n        word_counts.update(words)\n    \n    # Create vocabulary by sorting the words based on their frequency\n    vocabulary = [word for word, _ in sorted(word_counts.items())]\n    \n    # Create BoW vectors for each document\n    bow_vectors = []\n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Create a Counter object to count word frequencies\n        bow_vector = Counter(words)\n        \n        # Fill in missing words with zero counts\n        for word in vocabulary:\n            if word not in bow_vector:\n                bow_vector[word] = 0\n\n        # Sort the BoW vector based on the vocabulary order\n        sorted_bow_vector = [bow_vector[word] for word in vocabulary]\n        \n        # Append the BoW vector to the list\n        bow_vectors.append(sorted_bow_vector)\n    \n    return vocabulary, bow_vectors\n\n# Example texts\ntexts = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\",\n]\n\n# Create Bag of Words\nvocabulary, bow_vectors = create_bag_of_words(texts)\n\n# Print vocabulary\nprint(\"Vocabulary:\")\nprint(vocabulary)\n\n# Print BoW vectors\nprint(\"\\nBag of Words Vectors:\")\nfor i, bow_vector in enumerate(bow_vectors):\n    print(f\"Document {i + 1}: {bow_vector}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVocabulary:\n['document', 'first', 'one', 'second', 'third']\n\nBag of Words Vectors:\nDocument 1: [1, 1, 0, 0, 0]\nDocument 2: [2, 0, 0, 1, 0]\nDocument 3: [0, 0, 1, 0, 1]\nDocument 4: [1, 1, 0, 0, 0]\n```\n:::\n:::\n\n\n## Statistical similarity of texts\n\n::: {#5d38c361 .cell output-location='fragment' execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -> float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\nquery = bow_vectors[3]\n\nsimilarities = []\nfor i, bv in enumerate(bow_vectors):\n    similarity = cosine_similarity(\n            vec1=query, \n            vec2=bv\n        )\n    similarities.append(\n        (texts[i], round(similarity, 2))\n    )\n\nsimilarities\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n[('This is the first document.', np.float64(1.0)),\n ('This document is the second document.', np.float64(0.63)),\n ('And this is the third one.', np.float64(0.0)),\n ('Is this the first document?', np.float64(1.0))]\n```\n:::\n:::\n\n\n# Limitations of Term Frequency and Bag of Words\n\n## Vocabulary\n- Grows with corpus size\n- Leads to computational inefficiencies and increased memory requirements\n- Fixed vocabulary after training, handling out-of-vocabulary words difficult\n\n\n## Context & structure\n- Treats each word independently\n- Fails to capture sequential and syntactical relationships\n- Limits understanding of sarcasm, irony, and metaphors\n- Lack of structural awareness (sentences with similar word distributions but differing meanings or intents)\n\n\n# Idea: Clustering of Bag of Word vectors\n\n## General idea\n- Clustering a common technique in text analysis\n- Groups similar documents based on word distributions\n- Each document represented as high-dimensional vector (e.g., BoW)\n- Dimensions correspond to unique words\n- Values reflect frequency of words in document\n\n\n## Approach\n- Clustering algorithms applied to partition documents\n- Similarity measured using distance metrics (**cosine similarity**, Euclidean distance)\n- Advantages:\n  - Simplicity and scalability\n  - Manageable computational complexity\n- Limitations:\n  - Reliance on word frequency alone\n  - Curse of dimensionality with large vocabularies\n\n## Code example\n\n::: {#6de51848 .cell output-location='slide' execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Example texts representing different topics\ntexts = [\n    \"apple orange banana\",\n    \"apple orange mango\",\n    \"banana apple kiwi\",\n    \"strawberry raspberry blueberry\",\n    \"strawberry raspberry blackberry\"\n]\n\n# create Bag of Words using CountVectorizer\nvectorizer = CountVectorizer()\nbow_matrix = vectorizer.fit_transform(texts)\n\nprint(\"Bag of Word vectors:\")\nprint(bow_matrix.toarray())\n\n# perform K-means clustering\nnum_clusters = 2\nkmeans = KMeans(n_clusters=num_clusters, n_init=\"auto\")\ncluster_labels = kmeans.fit_predict(bow_matrix)\n\nprint(\"\\nCluster labels:\")\nfor i, label in enumerate(cluster_labels):\n    print(f\"Document {i + 1} belongs to Cluster {label + 1}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBag of Word vectors:\n[[1 1 0 0 0 0 1 0 0]\n [1 0 0 0 0 1 1 0 0]\n [1 1 0 0 1 0 0 0 0]\n [0 0 0 1 0 0 0 1 1]\n [0 0 1 0 0 0 0 1 1]]\n\nCluster labels:\nDocument 1 belongs to Cluster 1\nDocument 2 belongs to Cluster 1\nDocument 3 belongs to Cluster 1\nDocument 4 belongs to Cluster 2\nDocument 5 belongs to Cluster 2\n```\n:::\n:::\n\n\n--- \n\n::: {#1e74f647 .cell output-location='fragment' execution_count=5}\n``` {.python .cell-code}\n# Get the vocabulary\nvocabulary = vectorizer.get_feature_names_out()\n\n# print the vocabulary\nprint(\"Vocabulary:\")\nprint(vocabulary)\n\n# print the Bag of Words matrix with corresponding words\nprint(\"\\nBag of Words matrix with corresponding words:\")\nbow_matrix_array = bow_matrix.toarray()\nfor i, document_vector in enumerate(bow_matrix_array):\n    words_in_document = [(word, frequency) for word, frequency in zip(vocabulary, document_vector) if frequency > 0]\n    print(f\"Document {i + 1}: {words_in_document}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVocabulary:\n['apple' 'banana' 'blackberry' 'blueberry' 'kiwi' 'mango' 'orange'\n 'raspberry' 'strawberry']\n\nBag of Words matrix with corresponding words:\nDocument 1: [('apple', np.int64(1)), ('banana', np.int64(1)), ('orange', np.int64(1))]\nDocument 2: [('apple', np.int64(1)), ('mango', np.int64(1)), ('orange', np.int64(1))]\nDocument 3: [('apple', np.int64(1)), ('banana', np.int64(1)), ('kiwi', np.int64(1))]\nDocument 4: [('blueberry', np.int64(1)), ('raspberry', np.int64(1)), ('strawberry', np.int64(1))]\nDocument 5: [('blackberry', np.int64(1)), ('raspberry', np.int64(1)), ('strawberry', np.int64(1))]\n```\n:::\n:::\n\n\n",
    "supporting": [
      "statistics_files"
    ],
    "filters": [],
    "includes": {}
  }
}