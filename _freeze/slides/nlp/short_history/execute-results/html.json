{
  "hash": "7957ca5400926367c7b22a77de65bf5f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"A Short History of Natural Language Processing\"\nformat: \n    revealjs:\n        theme: default\n        chalkboard: true\n        footer: \"Sprint: LLM, 2024\"\n        logo: ../../assets/logo.svg\n---\n\n## Early Days: Rule-Based Approaches (1960s-1980s)\n- Rely heavily on rule-based approaches\n- Significant efforts in tasks like part-of-speech tagging, named entity recognition, and machine translation\n- Struggled with ambiguity and complexity of natural language\n\n\n## Rise of Statistical Methods (1990s-2000s)\n- Emergence of statistical methods\n- Techniques like Hidden Markov Models and Conditional Random Fields gained prominence\n- Improved performance in tasks such as text classification, sentiment analysis, and information extraction\n\n\n## Machine Learning Revolution (2010s)\n- Rise of machine learning, particularly deep learning\n- Exploration of neural network architectures tailored for NLP tasks\n- Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) gained traction\n\n\n## Large Language Models: Transformers (2010s-Present)\n- Rise of large language models, epitomized by the Transformer architecture\n- Powered by self-attention mechanisms\n- Achieved unprecedented performance across a wide range of NLP tasks\n\n\n# Challenges in NLP\n::: {.incremental}\n- Ambiguity of language\n- Different languages\n- Bias\n- Importance of context\n- World knowledge\n- Common sense reasoning\n- \"Incomparability\" of language\n:::\n\n::: {.notes}\n- Poses challenges in accurately interpreting meaning\n- NLP systems struggle with languages other than English\n- NLP models can perpetuate biases present in the training data\n- Understanding context is paramount for NLP tasks\n- NLP systems lack comprehensive world knowledge\n:::\n\n\n# Classic NLP tasks & applications\n\n\n## Part-of-Speech Tagging (POS)\n- Labeling each word with its grammatical category\n- Crucial for language understanding, information retrieval, and machine translation\n\n> The sun sets behind the mountains, casting a golden glow across the sky.\n\n--- \n\n::: {#7bc88ed6 .cell output-location='slide' execution_count=1}\n``` {.python .cell-code .code-overflow-wrap}\nimport spacy\n\n# Load the English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example text\ntext = \"The sun sets behind the mountains, casting a golden glow across the sky.\"\n\n# Process the text with spaCy\ndoc = nlp(text)\n\n# Find the maximum length of token text and POS tag\nmax_token_length = max(len(token.text) for token in doc)\nmax_pos_length = max(len(token.pos_) for token in doc)\n\n# Print each token along with its part-of-speech tag\nfor token in doc:\n    print(f\"Token: {token.text.ljust(max_token_length)} | POS Tag: {token.pos_.ljust(max_pos_length)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken: The       | POS Tag: DET  \nToken: sun       | POS Tag: NOUN \nToken: sets      | POS Tag: VERB \nToken: behind    | POS Tag: ADP  \nToken: the       | POS Tag: DET  \nToken: mountains | POS Tag: NOUN \nToken: ,         | POS Tag: PUNCT\nToken: casting   | POS Tag: VERB \nToken: a         | POS Tag: DET  \nToken: golden    | POS Tag: ADJ  \nToken: glow      | POS Tag: NOUN \nToken: across    | POS Tag: ADP  \nToken: the       | POS Tag: DET  \nToken: sky       | POS Tag: NOUN \nToken: .         | POS Tag: PUNCT\n```\n:::\n:::\n\n\n## Named-Entity Recognition (NER)\n- Identifying and classifying named entities in text\n- Essential for information retrieval, document summarization, and question-answering systems\n\n> Apple is considering buying a U.K. based startup called LanguageHero located in London for $1 billion.\n\n---\n\n::: {#f72d424c .cell output-location='fragment' execution_count=2}\n``` {.python .cell-code .code-overflow-wrap}\nimport spacy\n\n# Load the English language model\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Example text\ntext = \"Apple is considering buying a U.K. based startup called LanguageHero located in London for $1 billion.\"\n\n# Process the text with spaCy\ndoc = nlp(text)\n\n# Print each token along with its Named Entity label\nfor ent in doc.ents:\n    print(f\"Entity: {ent.text.ljust(20)} | Label: {ent.label_}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEntity: Apple                | Label: ORG\nEntity: U.K.                 | Label: GPE\nEntity: LanguageHero         | Label: PRODUCT\nEntity: London               | Label: GPE\nEntity: $1 billion           | Label: MONEY\n```\n:::\n:::\n\n\n## Sentiment Analysis\n- Analyzing text to determine sentiment (e.g., positive, negative, neutral)\n- Used for gauging customer satisfaction, monitoring social media sentiment, etc.\n\n> I love TextBlob! It's an amazing library for natural language processing.\n\n--- \n\n::: {#79131118 .cell output-location='fragment' execution_count=3}\n``` {.python .cell-code .code-overflow-wrap}\n# python -m textblob.download_corpora\nfrom textblob import TextBlob\n\n# Example text\ntext = \"I love TextBlob! It's an amazing library for natural language processing.\"\n\n# Perform sentiment analysis with TextBlob\nblob = TextBlob(text)\nsentiment_score = blob.sentiment.polarity\n\n# Determine sentiment label based on sentiment score\nif sentiment_score > 0:\n    sentiment_label = \"Positive\"\nelif sentiment_score < 0:\n    sentiment_label = \"Negative\"\nelse:\n    sentiment_label = \"Neutral\"\n\n# Print sentiment analysis results\nprint(f\"Text: {text}\")\nprint(f\"Sentiment Score: {sentiment_score:.2f}\")\nprint(f\"Sentiment Label: {sentiment_label}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nText: I love TextBlob! It's an amazing library for natural language processing.\nSentiment Score: 0.44\nSentiment Label: Positive\n```\n:::\n:::\n\n\n## Text Classification\n- Categorizing text documents into predefined classes\n- Widely used in email spam detection, sentiment analysis, and content categorization\n\n--- \n\n::: {#ba7f27eb .cell output-location='fragment' execution_count=4}\n``` {.python .cell-code .code-overflow-wrap}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import LabelEncoder\n\n# Example labeled dataset\ntexts = [\n    \"I love this product!\",\n    \"This product is terrible.\",\n    \"Great service, highly recommended.\",\n    \"I had a bad experience with this company.\",\n]\nlabels = [\n    \"Positive\",\n    \"Negative\",\n    \"Positive\",\n    \"Negative\",\n]\n\n# Create a TF-IDF vectorizer\nvectorizer = TfidfVectorizer()\n\n# Encode labels as integers\nlabel_encoder = LabelEncoder()\nencoded_labels = label_encoder.fit_transform(labels)\n\n# Create a pipeline with TF-IDF vectorizer and SVM classifier\nclassifier = make_pipeline(vectorizer, SVC(kernel='linear'))\n\n# Train the classifier\nclassifier.fit(texts, encoded_labels)\n\n# Example test text\ntest_text = \"I love what this product can do.\"\n\n# Predict the label for the test text\npredicted_label = classifier.predict([test_text])[0]\n\n# Decode the predicted label back to original label\npredicted_label_text = label_encoder.inverse_transform([predicted_label])[0]\n\n# Print the predicted label\nprint(f\"Text: {test_text}\")\nprint(f\"Predicted Label: {predicted_label_text}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nText: I love what this product can do.\nPredicted Label: Positive\n```\n:::\n:::\n\n\n## Information Extraction\n- Extracting structured information from unstructured text data\n- Crucial for knowledge base construction, data integration, and business intelligence\n\n## Question-Answering\n- Generating accurate answers to user queries in natural language\n- Essential for information retrieval, virtual assistants, and educational applications\n\n## Machine Translation\n- Automatically translating text from one language to another\n- Facilitates communication across language barriers\n\n",
    "supporting": [
      "short_history_files"
    ],
    "filters": [],
    "includes": {}
  }
}