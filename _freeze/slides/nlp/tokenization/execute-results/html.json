{
  "hash": "64ed4fbca0185f70d4ba2dcbf86b9de1",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tokenization\"\nformat: \n    revealjs:\n        theme: default\n        chalkboard: true\n        footer: \"Sprint: LLM, 2024\"\n        logo: ../../assets/logo.svg\n---\n\n## Tokenization\n\n::: {#d7c0bb2b .cell execution_count=1}\n``` {.python .cell-code}\nsentence = \"I love reading science fiction books or books about science.\"\n```\n:::\n\n\n&nbsp;\n\n::: {.fragment}\n::: {.callout-note title=\"Definition\"}\nTokenization is the process of breaking down a text into smaller units called tokens. \n:::\n:::\n\n&nbsp;\n\n::: {.fragment}\n\n::: {#60cf144b .cell execution_count=2}\n``` {.python .cell-code}\ntokenized_sentence = sentence.split(\" \")\nprint(tokenized_sentence)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science.']\n```\n:::\n:::\n\n\n:::\n\n\n## Counting token\n\n::: {#5ff918ca .cell execution_count=3}\n``` {.python .cell-code}\nfrom collections import Counter\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('books', 2), ('I', 1), ('love', 1)]\n```\n:::\n:::\n\n\n&nbsp;\n\n::: {.fragment}\n\n::: {#5eadfd75 .cell execution_count=4}\n``` {.python .cell-code}\ntokenized_sentence = sentence.replace(\".\", \" \").split(\" \")\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('science', 2), ('books', 2)]\n```\n:::\n:::\n\n\n:::\n\n\n## NLTK tokenization\n\n::: {#2ccffcff .cell execution_count=5}\n``` {.python .cell-code}\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\n\ntokenized_sentence = wordpunct_tokenize(sentence)\ntokenized_sentence = [t for t in tokenized_sentence if t not in punctuation]\nprint(tokenized_sentence)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science']\n```\n:::\n:::\n\n\n## Lemmatization\n\n- Reduce words to their base or canonical form\n- Represents the dictionary form of a word (lemma)\n- Standardizes words for better text analysis accuracy\n- Example: `meeting` --> `meet` (verb)\n\n--- \n- Helps in tasks such as text classification, information retrieval, and sentiment analysis\n- Considers context and linguistic rules\n- Retains semantic meaning of words\n- Has to involve part-of-speech tagging (see example below)\n- Determines correct lemma based on word's role in sentence\n\n```{mermaid}\nflowchart LR\n    A(meeting)\n    A --> B(\"meet (verb)\")\n    A --> C(\"meeting (noun)\")\n```\n\n\n## Bit Pair Encoding: Why?\n\n- Tokenization: Breaking text into smaller chunks (tokens)\n- Traditional vocabularies: Fixed-size, memory-intensive\n- Bit pair encoding: Compression technique for large vocabularies\n\n## Bit Pair Encoding: How?\n- Pair Identification: Identifies frequent pairs of characters\n- Replacement with Single Token: Replaces pairs with single token\n- Iterative Process: Continues until stopping criterion met\n- Vocabulary Construction: Construct vocabulary with single tokens\n- Encoding and Decoding: Text encoded and decoded using constructed vocabulary\n\n## Bit Pair Encoding: Pros and Cons\n- Efficient Memory Usage\n- Retains Information\n- Flexibility\n- Computational Overhead\n- Loss of Granularity\n\n::: {.notes}\n- Reduces vocabulary size, efficient memory usage\n- Captures frequent character pairs, retains linguistic information\n- Adaptable to different tokenization strategies and corpus characteristics\n- Iterative nature can be computationally intensive\n- May lead to loss of granularity, especially for rare words\n- Effectiveness depends on tokenization strategy and corpus characteristics\n:::\n\n",
    "supporting": [
      "tokenization_files"
    ],
    "filters": [],
    "includes": {}
  }
}