{
  "hash": "9a70e3f6e00dc86ade8766ee71927078",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tokenization\"\nformat: \n    revealjs:\n        theme: default\n        chalkboard: true\n        footer: \"Seminar: LLM, SoSe 2025\"\n        logo: ../../assets/logo.svg\n        fig-align: center\n---\n\n\n## Tokenization\n\n::: {#f7dcf62c .cell execution_count=1}\n``` {.python .cell-code}\nsentence = \"I love reading science fiction books or books about science.\"\n```\n:::\n\n\n&nbsp;\n\n::: {.fragment}\n::: {.callout-note title=\"Definition\"}\nTokenization is the process of breaking down a text into smaller units called tokens. \n:::\n:::\n\n&nbsp;\n\n::: {.fragment}\n\n::: {#507a65c5 .cell execution_count=2}\n``` {.python .cell-code}\ntokenized_sentence = sentence.split(\" \")\nprint(tokenized_sentence)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science.']\n```\n:::\n:::\n\n\n:::\n\n\n## Counting token\n\n::: {#d7b79cff .cell execution_count=3}\n``` {.python .cell-code}\nfrom collections import Counter\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(3))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('books', 2), ('I', 1), ('love', 1)]\n```\n:::\n:::\n\n\n&nbsp;\n\n::: {.fragment}\n\n::: {#bcd7e28e .cell execution_count=4}\n``` {.python .cell-code}\ntokenized_sentence = sentence.replace(\".\", \" \").split(\" \")\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('science', 2), ('books', 2)]\n```\n:::\n:::\n\n\n:::\n\n\n## NLTK tokenization\n\n::: {#86ee8305 .cell execution_count=5}\n``` {.python .cell-code}\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\n\ntokenized_sentence = wordpunct_tokenize(sentence)\ntokenized_sentence = [t for t in tokenized_sentence if t not in punctuation]\nprint(tokenized_sentence)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science']\n```\n:::\n:::\n\n\n## Lemmatization\n\n- Reduce words to their base or canonical form\n- Represents the dictionary form of a word (lemma)\n- Standardizes words for better text analysis accuracy\n- Example: `meeting` --> `meet` (verb)\n\n\n---\n\n- Helps in tasks such as text classification, information retrieval, and sentiment analysis\n- Considers context and linguistic rules\n- Retains semantic meaning of words\n- Has to involve part-of-speech tagging (see example below)\n- Determines correct lemma based on word's role in sentence\n\n\n```{mermaid}\nflowchart LR\n    A(meeting)\n    A --> B(\"meet (verb)\")\n    A --> C(\"meeting (noun)\")\n```\n\n\n\n## Lemmatization with WordNet: Nouns\n\n::: {#21154af1 .cell output-location='fragment' execution_count=6}\n``` {.python .cell-code}\nfrom nltk.stem import WordNetLemmatizer\n\nsentence = \"The three brothers went over three big bridges\"\n\nwnl = WordNetLemmatizer()\n\nlemmatized_sentence_token = [\n    wnl.lemmatize(w, pos=\"n\") for w in sentence.split(\" \")\n]\n\nprint(lemmatized_sentence_token)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['The', 'three', 'brother', 'went', 'over', 'three', 'big', 'bridge']\n```\n:::\n:::\n\n\n## Lemmatization with WordNet: Verbs\n\n::: {#260d1a23 .cell output-location='fragment' execution_count=7}\n``` {.python .cell-code}\nlemmatized_sentence_token = [\n    wnl.lemmatize(w, pos=\"v\") for w in sentence.split(\" \")\n]\n\nprint(lemmatized_sentence_token)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['The', 'three', 'brothers', 'go', 'over', 'three', 'big', 'bridge']\n```\n:::\n:::\n\n\n## Lemmatization with WordNet and POS-tagging\n\n::: {#c6d94b59 .cell output-location='fragment' execution_count=8}\n``` {.python .cell-code}\npos_dict = {\n  \"brothers\": \"n\", \n  \"went\": \"v\",\n  \"big\": \"a\",\n  \"bridges\": \"n\"\n}\n\nlemmatized_sentence_token = []\nfor token in sentence.split(\" \"):\n    if token in pos_dict:\n        lemma = wnl.lemmatize(token, pos=pos_dict[token])\n    else: \n        lemma = token # leave as it is\n\n    lemmatized_sentence_token.append(lemma)\n\nprint(lemmatized_sentence_token)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['The', 'three', 'brother', 'go', 'over', 'three', 'big', 'bridge']\n```\n:::\n:::\n\n\n# Bit Pair Encoding\n\n## Bit Pair Encoding: Why?\n\n- Tokenization: Breaking text into smaller chunks (tokens)\n- Traditional vocabularies: Fixed-size, memory-intensive\n- Bit pair encoding: Compression technique for large vocabularies\n\n## Bit Pair Encoding: How?\n- Pair Identification: Identifies frequent pairs of characters\n- Replacement with Single Token: Replaces pairs with single token\n- Iterative Process: Continues until stopping criterion met\n- Vocabulary Construction: Construct vocabulary with single tokens\n- Encoding and Decoding: Text encoded and decoded using constructed vocabulary\n\n# [OpenAI Tokenizer](https://platform.openai.com/tokenizer){.external}\n\n## Bit Pair Encoding: Pros and Cons\n- Efficient Memory Usage\n- Retains Information\n- Flexibility\n- Computational Overhead\n- Loss of Granularity\n\n::: {.notes}\n- Reduces vocabulary size, efficient memory usage\n- Captures frequent character pairs, retains linguistic information\n- Adaptable to different tokenization strategies and corpus characteristics\n- Iterative nature can be computationally intensive\n- May lead to loss of granularity, especially for rare words\n- Effectiveness depends on tokenization strategy and corpus characteristics\n:::\n\n",
    "supporting": [
      "tokenization_files"
    ],
    "filters": [],
    "includes": {}
  }
}