{
  "hash": "03b59c694094dcd8b7c7dfbe0ef5281b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Large Language Models: An 'intuitive' introduction\"\nformat: \n    revealjs:\n        theme: default\n        chalkboard: true\n        footer: \"Seminar: LLM, SoSe 2025\"\n        logo: ../../assets/logo.svg\n        fig-align: center\n---\n\n\n\n## Background\n\n- Rise of machine learning and deep learning in the 2000s\n \n::: {.incremental}\n- **Idea:** Use neural networks for NLP tasks\n- Challenge: How do we feed text into a neural network? \n:::\n\n::: {.fragment}\n\nAnswer: Text embeddings! \n\n:::\n\n\n## Example from Bag of Words\n\n```{mermaid}\nflowchart LR\n  A(A cat does cat things) --> B{\" \"}\n  B --> C(A)\n  B --> D(cat)\n  B --> E(does)\n  B --> F(cat)\n  B --> G(things)\n  D --> H(cat)\n  E --> I(do)\n  F --> J(cat)\n  G --> K(thing)\n\n  H --> L(cat: 2)\n  J --> L\n  I --> M(do: 1)\n  K --> N(thing: 1)\n\n  L --> O(2)\n  M --> P(1)\n  N --> Q(1)\n\n```\n\n\n\n## How could a neural network look like? \n\n\n```{mermaid}\nflowchart LR\n  A(Input Text) --> B(Tokenization)\n  B --> C(Token processing)\n  C --> D(Embedding Layer)\n  D --> E(Hidden Layers)\n  E --> F(Output Layer)\n```\n\n\n- The hidden layers and output layers depend on the application\n- The rest of the layers can be pre-trained (later)\n\n\n## Example: Text classification\n\n\n```{mermaid}\nflowchart LR\n  A(Input Text) --> B(Tokenization)\n  B --> C(Token processing)\n  C --> D(Embedding Layer)\n  D --> E(Hidden Layers)\n  E --> F(Output Layer)\n```\n\n\n- Classifying news articles into categories (sports, politics, ...)\n\n::: {.incremental}\n  \n  - Training data: Dataset with corresponding category or label\n  - Data processing: Tokenization, stop words, lower casing etc.\n  - Training: \n    - Measure the difference between predicted and true labels and adjust network weights\n    - Example: BoW embeddings - frequency of words affects label likelihood.\n:::\n\n# Sequence Generation and Language Modeling\n\n## Sequence generation \n- **Idea:** Models are trained to generate sequences of data (mostly: text) based on input/context.\n- Sequences have to resemble the training data.\n- Application: Text generation, music composition, image captioning\n- Requires **understanding language structure** for meaningful output!\n\n\n## Language modeling \n\n&nbsp;\n\n**Idea:** Train a model to predict the probability distribution of words or tokens in a sequence given the preceding context!\n\n&nbsp;\n\n\n```{mermaid}\nflowchart LR\n  A(The) --> B(bird)\n  B --> C(flew)\n  C --> D(over)\n  D --> E(the)\n  E --> F{?}\n  F --> G(\"p(rooftops)=0.31\")\n  F --> H(\"p(trees)=0.14\")\n  F --> J(\"p(guitar)=0.001\")\n```\n\n\n\n## Training Process\n\n- Expose the model to large text datasets (great: we have the internet!)\n- Teach the model statistical properties of language (Which token comes next?)\n- Capture syntactic structures, semantic relationships, and contextual nuances\n- Training happens in an unsupervised fashion (we require no labels!)\n  \n\n## Challenges\n- Handling vast and diverse nature of human language.\n- Complex patterns, variations, and ambiguities.\n- Out-of-vocabulary words, long-range dependencies, domain-specific knowledge.\n- Requires robust architectures and sophisticated algorithms.\n\n**BUT:** They did it and it works!\n\n\n# GPT: Generative Pre-trained Transformer\n\n## What is GPT? \n\n- current state-of-the-art language model\n- introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017 (Google)\n- GPT belongs to the family of transformer-based models\n- key advantage over previous approaches: \n  - self-attention \n  - scalability\n\n## What is a transformer? \n\n- Traditional approach: \n  - information flow constrained by fixed-length context windows or recurrent connections\n  - One token at a time (in RNNs)\n\n---\n\n- New approach: \n  - Each word in a sentence can attend to all other words simultaneously (self-attention)\n  - Dynamically weigh the importance of each word in the context of the entire sequence\n  - Semantically related words receive higher attention weights \n  - Irrelevant or less informative words receive lower weights\n  - Processing all sequences in parallel\n\n---\n\n## A deeper dive: What an LLM wants to do! \n\n\n```{mermaid}\nflowchart LR\n  A(Token 1)\n  B(Token 2) \n  C(Token 3)\n  D(...)\n  E(Token k)  \n\n  F(The core of the LLM)\n\n  A --> F\n  B --> F\n  C --> F\n  D --> F\n  E --> F\n\n  AA(Prob. dist. 1st output token)\n  BB(Prob. dist. 2nd output token)\n  CC(...)\n  DD(Prob. dist. nth output token)\n\n  F --> AA\n  F --> BB\n  F --> CC\n  F --> DD\n\n\n\n```\n\n\n---\n\n## How can we generate a probability distribution? \n\nIdea: Transform a vector $z$ of numbers into a probability distribution! \n\n$$\n\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n$$\n\n::: {#e9b4c4a6 .cell execution_count=2}\n\n::: {.cell-output .cell-output-stdout}\n```\nSoftmax input z: [2.0, 1.0, 0.1, -1.0, 2.0]\nSoftmax output: [0.39, 0.14, 0.06, 0.02, 0.39]\n```\n:::\n:::\n\n\n&nbsp;\n\n:::{.fragment}\nGreat, so let's try to generate vectors with our model!\n:::\n\n\n## A deeper dive: What an LLM looks like! \n\n&nbsp;\n\n\n```{mermaid}\nflowchart LR\n  A(Input Text) --> B(Tokenization) --> C(Token Embedding) -.-> X:::hidden \n  \n  Y:::hidden -.-> D(Attention) --> E(Multilayer Perceptron) --> F(Attention) --> G(Multilayer Perceptron) --> H(...) -.-> Z:::hidden\n  \n  XX:::hidden -.-> I(Unembedding) --> J(Probabilities) --> K(Token Output)\n```\n\n\n&nbsp;\n\nGeneral idea: Transform input vectors (embeddings) over and over such that the result encodes all the context!\n\n---\n\n## Token embeddings \n\n::: {#e8bdf5ed .cell execution_count=3}\n``` {.python .cell-code}\ninput_text = \"A blue guitar is\"\n```\n:::\n\n\n&nbsp;\n\n::: {.columns}\n\n::: {.column width=\"25%\"}\nTokenization: \n:::\n\n::: {.column width=\"75%\"}\n\n```{mermaid}\nflowchart LR\n  A(A) --> B(blue) --> C(guitar) --> D(is) --> E(?)\n```\n\n:::\n:::\n\n&nbsp;\n\n::: {.columns}\n\n::: {.column width=\"25%\"}\nEmbedding: \n:::\n\n::: {.column width=\"75%\"}\n$$W_E =\n\\begin{bmatrix}\n0.25 & -0.73 & 0.58 & 0.44 \\\\\n-0.53 & 0.57 & -0.61 & 0.70 \\\\\n0.11 & 0.33 & 0.22 & -0.49 \\\\\n-0.80 & -0.08 & -0.73 & 0.29 \\\\\n\\end{bmatrix}\n$$\n\n:::\n:::\n\n\n---\n\n## Token embeddings \n\n- One embedding $E_k$ per token (lookup table, but it's being **learned** during training!)\n- No connection (\"context\") between token embeddings at first\n- But: Directional information contained in embeddings which encodes \"meaning\"\n\n---\n\n## Embedding visualization\n\n\n\n::: {#040daf8c .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](intro_files/figure-revealjs/cell-6-output-1.png){fig-align='center'}\n:::\n:::\n\n\n---\n\n## Unembedding, probabilities and token output\n\nWhat happens at the end of the LLM?\n\n&nbsp;\n\n\n```{mermaid}\nflowchart LR\n  A(A) --> AA(\"$$E_1$$\")\n  B(blue) --> BB(\"$$E_2$$\")\n  C(guitar) --> CC(\"$$E_3$$\")\n  D(is) --> DD(\"$$E_4$$\") \n\n  E(The core of the LLM)\n\n  AA --> E\n  BB --> E\n  CC --> E\n  DD --> E\n\n\n\n  F(Unembedding matrix)\n\n  E --> F\n\n  AAA(\"$$U_1$$\")\n  BBB(\"$$U_2$$\")\n  CCC(\"$$U_3$$\")\n\n  F --> AAA\n  F --> BBB\n  F --> CCC\n\n  G(Softmax) --> GG(Prob. dist. 1st output token)\n  H(Softmax) --> HH(Prob. dist. 2nd output token)\n  I(Softmax) --> II(Prob. dist 3rd output token)\n  \n  AAA --> G\n  BBB --> H\n  CCC --> I\n\n  GG --> X(\"p(great)=0.31\")\n  GG --> Y(\"p(aweful)=0.14\")\n  GG --> Z(\"p(train)=0.001\")\n\n```\n\n\n\n---\n\n\n## Understanding the core: Attention \n- Problem: Embedding vectors so far do not interact with each other, so we cannot process \"context\".\n- Solution: Let embedding vectors $E_k$ share information! \n\n\n```{mermaid}\nflowchart LR \n  A(\"$$E_1^{(1)}$$\")\n  B(\"$$E_2^{(1)}$$\")\n  C(\"$$E_3^{(1)}$$\")\n  D(\"$$E_k^{(1)}$$\")\n\n  E(\"Transformation ('Attention')\")\n\n  A --> E\n  B --> E\n  C --> E\n  D --> E\n\n  F(\"$$E_1^{(2)}$$\")\n  G(\"$$E_2^{(2)}$$\")\n  H(\"$$E_3^{(2)}$$\")\n  I(\"$$E_k^{(2)}$$\")\n\n  E --> F\n  E --> G\n  E --> H\n  E --> I\n\n```\n\n\n-- \n\n## Understanding the core: Attention \n\n$$ \n\\text{Attention}(Q, K, V) = \\text{softmax}(K^T Q)V \n$$\n\n\n- $Q$ is the \"query\", asking a question\n- $K$ is the \"key\", answering that question\n- $\\text{softmax}$ turns the result into a weight (\"How important is that question and its answer?\")\n- Then we multiply with the value matrix $V$ that transforms the initial embedding vectors, that now encode shared information!\n\n---\n\n## Understanding the core: Attention \n\n$$ \n\\text{Attention}(Q, K, V) = \\text{softmax}(K^T Q)V \n$$\n\n\n$$K_i = E_i * W_K$$\n$$Q_i = E_i * W_Q$$\n\n$$\n\\text{softmax}(\n\\begin{bmatrix}\nK_1 \\cdot Q_1 & K_1 \\cdot Q_2 & K_1 \\cdot Q_3 \\\\\nK_2 \\cdot Q_1 & K_2 \\cdot Q_2 & K_2 \\cdot Q_3 \\\\\nK_3 \\cdot Q_1 & K_3 \\cdot Q_2 & K_3 \\cdot Q_3\n\\end{bmatrix}) V\n$$\n\n\n\n---\n\n## Multilayer Perceptron \n\n- A \"standard\" neural network which works in the same way that other work. \n- Can be thought of as \"more parameters to tune in the network\"!\n- Won't go into details here.\n\n---\n\n## Put it all together\n\n&nbsp;\n\n\n```{mermaid}\nflowchart LR\n  A(A) --> AA(\"$$E_1$$\")\n  B(blue) --> BB(\"$$E_2$$\")\n  C(guitar) --> CC(\"$$E_3$$\")\n  D(is) --> DD(\"$$E_4$$\") \n\n  E(Attention and MP)\n\n  AA --> E\n  BB --> E\n  CC --> E\n  DD --> E\n\n  REP(Repeat 10k times)\n\n  F(Unembedding matrix)\n\n  E --> REP\n  REP --> F\n\n  AAA(\"$$U_1$$\")\n  BBB(\"$$U_2$$\")\n  CCC(\"$$U_3$$\")\n\n  F --> AAA\n  F --> BBB\n  F --> CCC\n\n  G(Softmax) --> GG(Prob. dist. 1st output token)\n  H(Softmax) --> HH(Prob. dist. 2nd output token)\n  I(Softmax) --> II(Prob. dist 3rd output token)\n  \n  AAA --> G\n  BBB --> H\n  CCC --> I\n\n  GG --> X(\"p(great)=0.31\")\n  GG --> Y(\"p(aweful)=0.14\")\n  GG --> Z(\"p(train)=0.001\")\n\n```\n\n\nWe have created a machine that can generate the most likely next token!\n\n--- \n\n## There are some details that need consideration\n\n- Details of the Multilayer Perceptron layers\n- Training of an LLM involves a second step (reinforcement learning)\n- Sampling, temperature scaling, $\\text{top_}{p}$ (How can an LLM be creative?)\n- How does the memory of an LLM work?\n- Many, many details we skipped! \n\n\n:::{.fragment}\n**But for now, let's get started using one!**\n:::\n\n",
    "supporting": [
      "intro_files"
    ],
    "filters": [],
    "includes": {}
  }
}