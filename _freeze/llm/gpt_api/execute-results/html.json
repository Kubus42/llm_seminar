{
  "hash": "9699c6de3e7ef87cd4298dc99e46caa3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"The OpenAI API\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\n---\n\n\n\n\n::: {.callout-note}\nResource: [OpenAI API docs](https://platform.openai.com/docs/introduction){.external}\n:::\n\n\nLet's finally get started working with GPT. \nIn this seminar, we will use the OpenAI API to work with, but there are many alternatives out there. \nWe have collected a few in the [resources](../resources/apis.qmd).\n\n\n### Authentication\n\nGetting started with the OpenAI Chat Completions API requires signing up for an account on the OpenAI platform. \nOnce you've registered, you'll gain access to an API key, which serves as a unique identifier for your application to authenticate requests to the API. \nThis key is essential for ensuring secure communication between your application and OpenAI's servers. \nWithout proper authentication, your requests will be rejected.\nYou can create your own account, but for the seminar we will provide the client with the credential within the University's [Jupyterlab](https://jupyter.fh-muenster.de/){.external}.\n\n::: {#9f22d549 .cell execution_count=1}\n``` {.python .cell-code}\n# setting up the client in Python\n\nimport os\nfrom openai import OpenAI\n\nclient = OpenAI(\n    api_key=os.environ.get(\"OPENAI_API_KEY\")\n)\n```\n:::\n\n\n### Requesting Completions\n\nMost interaction with GPT and other models consist in generating completions for prompts, i.e., providing some text with instructions and letting the language model **complete** the text one token after the other as seen [here](../llm/intro.qmd).\n\nTo request completions from the OpenAI API, we use Python to send HTTP requests to the designated API endpoint. \nThese requests are structured to include various parameters that guide the generation of text completions. \nThe most fundamental parameter is the prompt text, which sets the context for the completion. \nAdditionally, you can specify the desired model configuration, such as the engine to use (e.g., \"gpt-4\"), as well as any constraints or preferences for the generated completions, such as the maximum number of tokens or the temperature for controlling creativity ([here](../llm/parameterization.qmd)).\n\n::: {#284368a4 .cell execution_count=2}\n``` {.python .cell-code}\n# creating a completion\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"How old is the earth?\",\n        }\n    ],\n    model=\"gpt-3.5-turbo\" # choose the model\n)\n```\n:::\n\n\n### Processing\n\nOnce the OpenAI API receives your request, it proceeds to process the provided prompt using the specified model. \nThis process involves analyzing the context provided by the prompt and leveraging the model's pre-trained knowledge to generate text completions. \nThe model employs advanced natural language processing techniques to ensure that the generated completions are coherent and contextually relevant. \nBy drawing from its extensive training data and understanding of human language, the model aims to produce responses that closely align with human-like communication.\n\n### Response\n\nAfter processing your request, the OpenAI API returns a response containing the generated text completions. \nDepending on the specifics of your request, you may receive multiple completions, each accompanied by additional information such as the amount of token processed in the request, the reason why the model stopped the answer etc. \nThis response provides valuable insights into the quality and relevance of the completions, allowing you to tailor your application's behavior accordingly.\nLet's check it out briefly, before you explore the response object more in-depth in your next exercise.\n\n::: {#8930d3ad .cell execution_count=3}\n``` {.python .cell-code}\n# check out the type of the response\n\nprint(f\"Response object type: {type(chat_completion)}\") # a ChatCompletion object\n\n# print the message we want\nprint(f\"\\nResponse message: {chat_completion.choices[0].message.content}\")\n\n# check the tokens used \nprint(f\"\\nTotal tokens used: {chat_completion.usage.total_tokens}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse object type: <class 'openai.types.chat.chat_completion.ChatCompletion'>\n\nResponse message: The Earth is estimated to be around 4.5 billion years old.\n\nTotal tokens used: 28\n```\n:::\n:::\n\n\n### Error Handling\n\nWhile interacting with the OpenAI API (or any API for that matter), it's crucial to implement some robust error handling mechanisms to manage any potential issues that may arise. \nThe kind of classic errors include providing invalid parameters, experiencing authentication failures due to an incorrect API key, or encountering rate limiting restrictions. \nBut for language models in particular, there are plenty more problems that can arise simply involving the answer we get from the model. \nSome examples are requests involving explicit language or content or restricted content etc. which are typically blocked by the API.\nOther times it might simply happen that a model does not respond in a way you expected, for example, just repeating your input instead of responding properly, or not responding in the format you requested. \nWhenever we are using language model for applications, we need to be aware of this and implement the right measures to handle these situations. \n\n",
    "supporting": [
      "gpt_api_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}