{
  "hash": "a20b274126bd9f16f0ae718708e16f43",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Parameterization of GPT\nformat:\n  html:\n    code-fold: false\n    code-wrap: true\n---\n\nThe GPT models provided by OpenAI provide a variety of parameters that can change the way the language model responds. \nBelow you can find a list of the most important ones.\n\n- **Temperature**: Temperature (`temperaure`) is a parameter that controls the randomness of the generated text. Lower temperatures result in more deterministic outputs, where the model tends to choose the most likely tokens at each step. Higher temperatures introduce more randomness, allowing the model to explore less likely tokens and produce more creative outputs. It's often used to balance between generating safe, conservative responses and more novel, imaginative ones.\n\n- **Max Tokens**: Max Tokens (`max_tokens`) limits the maximum length of the generated text by specifying the maximum number of tokens (words or subwords) allowed in the output. This parameter helps to control the length of the response and prevent the model from generating overly long or verbose outputs, which may not be suitable for certain applications or contexts.\n\n- **Top P (Nucleus Sampling)**: Top P (`top_p`), also known as nucleus sampling, dynamically selects a subset of the most likely tokens based on their cumulative probability until the cumulative probability exceeds a certain threshold (specified by the parameter). This approach ensures diversity in the generated text while still prioritizing tokens with higher probabilities. It's particularly useful for generating diverse and contextually relevant responses.\n\n- **Frequency Penalty**: Frequency Penalty (`frequency_penalty`) penalizes tokens based on their frequency in the generated text. Tokens that appear more frequently are assigned higher penalties, discouraging the model from repeatedly generating common or redundant tokens. This helps to promote diversity in the generated text and prevent the model from producing overly repetitive outputs.\n\n- **Presence Penalty**: Presence Penalty (`presence_penalty`) penalizes tokens that are already present in the input prompt. By discouraging the model from simply echoing or replicating the input text, this parameter encourages the generation of responses that go beyond the provided context. It's useful for generating more creative and novel outputs that are not directly predictable from the input.\n\n- **Stop Sequence**: Stop Sequence (`stop`) specifies a sequence of tokens that, if generated by the model, signals it to stop generating further text. This parameter is commonly used to indicate the desired ending or conclusion of the generated text. It helps to control the length of the response and ensure that the model generates text that aligns with specific requirements or constraints.\n\n\n## Roles: \n\nIn order to cover most tasks you want to perform using a chat format, the OpenAI API let's you define different `roles` in the chat. \nThe available roles are `system`, `assistant`, `user` and `tools`. \nYou should already be familiar with two of them by now: \nThe `user` role corresponds to the actual user prompting the language model, all answers are given with the `assisstant` role.\n\nThe `system` role can now be given to provide some additional general instructions to the language model that are typically not a user input, for example, the style in which the model responds. \nIn this case, an example is better than any explanation.\n\n::: {#875ab431 .cell execution_count=1}\n``` {.python .cell-code}\nimport os\nfrom llm_utils.client import get_openai_client\n\nMODEL = \"gpt4\"\n\nclient = get_openai_client(\n    model=MODEL,\n    config_path=os.environ.get(\"CONFIG_PATH\")\n)\n\ncompletion = client.chat.completions.create(\n  model=\"MODEL\",\n  messages=[\n    {\"role\": \"system\", \"content\": \"You are an annoyed technician working in a help center for dish washers, who answers in short, unfriendly bursts.\"},\n    {\"role\": \"user\", \"content\": \"My dish washer does not clean the dishes, what could be the reason.\"}\n  ]\n)\n\nprint(completion.choices[0].message.content)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCould be anything. Blocked spray arm. Clogged filter. Faulty pump. Detergent issue. Check all that.\n```\n:::\n:::\n\n\n## Function calling: {#sec-test} \n\nAs we have seen, most interactions with a language model happen in form of a chat with almost \"free\" question or instructions and answers.\nWhile this seems the most natural in most cases, it is not always a practical format if we want to use a language model for very specific purposes.\nThis happens particularly often when we want to employ a language model in business situations, where we require a consistent output of the model.\n\nAs an example, let us try to use GPT for sentiment analysis (see also [here](../nlp/overview.qmd#sec-sentiment-analysis)).\nLet's say we want GPT to classify a text into one of the following four categories: \n\n::: {#ce80e6f9 .cell execution_count=2}\n``` {.python .cell-code}\nsentiment_categories = [\n    \"positive\", \n    \"negative\",\n    \"neutral\",\n    \"mixed\"\n]\n```\n:::\n\n\nWe could do the following:\n\n\n\n::: {#40c825ed .cell execution_count=4}\n``` {.python .cell-code}\nmessages = []\nmessages.append(\n    {\"role\": \"system\", \"content\": f\"Classify the given text into one of the following sentiment categories: {sentiment_categories}.\"}\n)\nmessages.append(\n    {\"role\": \"user\", \"content\": \"I really did not like the movie.\"}\n)\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=MODEL\n)\n\nprint(f\"Response: '{response.choices[0].message.content}'\")\n```\n:::\n\n\n::: {#c6ffeb88 .cell execution_count=5}\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse: 'Category: Negative'\n```\n:::\n:::\n\n\nIt is easy to spot the problem: GPT does not necessarily answer in the way we expect or want it to. \nIn this case, instead of simply returning the correct category, it also returns the string `Category: ` alongside it (and capitalized `Negative`).\nSo if we were to use the answer in a program or data base, we'd now again have to use some NLP techniques to parse it in order eventually retrieve **exactly** the category we were looking for: `negative`. \nWhat we need instead is a way to constrain GPT to a specific way of answering, and this is where `functions` or `tools` come into play (see also [Function calling](https://platform.openai.com/docs/guides/function-calling){.external} and [Function calling (cookbook)](https://cookbook.openai.com/examples/how_to_call_functions_with_chat_models){.external}).\n\nThis concept allows us to specify the exact output format we expect to receive from GPT (it is called functions since ideally we want to call a function directly on the output of GPT so it has to be in a specific format). \n\n::: {#b78f10d6 .cell execution_count=6}\n``` {.python .cell-code}\n# this looks intimidating but isn't that complicated\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_sentiment\",\n            \"description\": \"Analyze the sentiment in a given text.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"sentiment\": {\n                        \"type\": \"string\",\n                        \"enum\": sentiment_categories,\n                        \"description\": f\"The sentiment of the text.\"\n                    }\n                },\n                \"required\": [\"sentiment\"],\n            }\n        }\n    }\n]\n```\n:::\n\n\n::: {#e1e40f2a .cell execution_count=7}\n``` {.python .cell-code}\nmessages = []\nmessages.append(\n    {\"role\": \"system\", \"content\": f\"Classify the given text into one of the following sentiment categories: {sentiment_categories}.\"}\n)\nmessages.append(\n    {\"role\": \"user\", \"content\": \"I really did not like the movie.\"}\n)\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=MODEL,\n    tools=tools,\n    tool_choice={\n        \"type\": \"function\", \n        \"function\": {\"name\": \"analyze_sentiment\"}}\n)\n\nprint(f\"Response: '{response.choices[0].message.tool_calls[0].function.arguments}'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse: '{\n\"sentiment\": \"negative\"\n}'\n```\n:::\n:::\n\n\nWe can now easily extract what we need: \n\n::: {#5e3c869b .cell execution_count=8}\n``` {.python .cell-code}\nimport json \nresult = json.loads(response.choices[0].message.tool_calls[0].function.arguments) # remember that the answer is a string\nprint(result[\"sentiment\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nnegative\n```\n:::\n:::\n\n\nWe can also include multiple function parameters if our desired output has multiple components.\nLet's try to include another parameter which includes the `reason` for the sentiment.\n\n::: {#9c902d9f .cell execution_count=9}\n``` {.python .cell-code}\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"analyze_sentiment\",\n            \"description\": \"Analyze the sentiment in a given text.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"sentiment\": {\n                        \"type\": \"string\",\n                        \"enum\": sentiment_categories,\n                        \"description\": f\"The sentiment of the text.\"\n                    },\n                    \"reason\": {\n                        \"type\": \"string\",\n                        \"description\": \"The reason for the sentiment in few words. If there is no information, do not make assumptions and leave blank.\"\n                    }\n                },\n                \"required\": [\"sentiment\", \"reason\"],\n            }\n        }\n    }\n]\n```\n:::\n\n\n::: {#d77c3b61 .cell execution_count=10}\n``` {.python .cell-code}\nmessages = []\nmessages.append(\n    {\"role\": \"system\", \"content\": f\"Classify the given text into one of the following sentiment categories: {sentiment_categories}. If you can, also extract the reason.\"}\n)\nmessages.append(\n    {\"role\": \"user\", \"content\": \"I loved the movie, Johnny Depp is a great actor.\"}\n)\n\nresponse = client.chat.completions.create(\n    messages=messages,\n    model=MODEL,\n    tools=tools,\n    tool_choice={\n        \"type\": \"function\", \n        \"function\": {\"name\": \"analyze_sentiment\"}}\n)\n\nprint(f\"Response: '{response.choices[0].message.tool_calls[0].function.arguments}'\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nResponse: '{\n\"sentiment\": \"positive\",\n\"reason\": \"Appreciation for the movie and actor\"\n}'\n```\n:::\n:::\n\n\nHere, again, we could also constrain the possibilities for the `reason` to a certain set. \nHence, functions are great to have more consistent answers of the language model such that we can use it in applications.\n\n",
    "supporting": [
      "parameterization_files"
    ],
    "filters": [],
    "includes": {}
  }
}