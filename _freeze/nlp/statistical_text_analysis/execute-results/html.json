{
  "hash": "1540eaaf85d11e1ff722d7217e73196a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Statistical text analysis\nformat:\n  html:\n    code-fold: false\n---\n\nSo far we have mainly looked at the analysis of single words/token or n-grams. \nBut what about the analysis of a full text? \nThere are many approaches to this but a good way to get into the topic is a simple statistical analysis of a text. \nFor starters, let's simply count the number of appearances of each word in a text, also known as **term frequency**. \n\n::: {#9df561a5 .cell execution_count=1}\n``` {.python .cell-code}\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\nfrom collections import Counter\nfrom typing import List\n\nfrom nltk.corpus import stopwords\n# python -m nltk.downloader stopwords -> run this in your console once to get the stopwords\n\n\n# load a text from file\ntext = \"\"\nwith open(\"../assets/chapter1.txt\", \"r\") as file:  \n    for line in file:\n        text += line.strip()\n\n\ndef preprocess_text(text: str) -> List[str]:\n    # tokenize text\n    tokens = wordpunct_tokenize(text.lower())\n\n    # remove punctuation\n    tokens = [t for t in tokens if t not in punctuation]\n\n    # remove stopwords\n    stop_words = stopwords.words(\"english\")\n    tokens = [t for t in tokens if t not in stop_words]\n\n    return tokens\n\n# count the most frequent words\ntokens = preprocess_text(text=text)\n\nfor t in Counter(tokens).most_common(15):\n    print(f\"{t[0]}: {t[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\none: 35\nwinston: 32\nface: 28\neven: 24\n--: 24\nbig: 22\ncould: 19\nparty: 18\nwould: 18\nmoment: 18\nlike: 17\nbrother: 15\ngoldstein: 15\ntelescreen: 14\nseemed: 14\n```\n:::\n:::\n\n\nJust from the most frequent words, can you guess the text?\n\n\nIn many cases, just the simple number of appearances of a token in a text can determine its importance. \nThe concept of counting the term frequency across multiple documents in order to create a fixed vocabulary is also known as **bag of words**. \n\n\n## Bag of Words\nIf we do the same with multiple texts, we can build up a vocabulary of words and compare different texts to each other based on the appearance of terms.\n\n::: {#8c9945f4 .cell execution_count=2}\n``` {.python .cell-code}\nfrom collections import Counter\n\n\ndef create_bag_of_words(texts):\n    # Count the frequency of each word in the corpus\n    word_counts = Counter()\n    \n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Update word counts\n        word_counts.update(words)\n    \n    # Create vocabulary by sorting the words based on their frequency\n    vocabulary = [word for word, _ in sorted(word_counts.items())]\n    \n    # Create BoW vectors for each document\n    bow_vectors = []\n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Create a Counter object to count word frequencies\n        bow_vector = Counter(words)\n        \n        # Fill in missing words with zero counts\n        for word in vocabulary:\n            if word not in bow_vector:\n                bow_vector[word] = 0\n\n        # Sort the BoW vector based on the vocabulary order\n        sorted_bow_vector = [bow_vector[word] for word in vocabulary]\n        \n        # Append the BoW vector to the list\n        bow_vectors.append(sorted_bow_vector)\n    \n    return vocabulary, bow_vectors\n\n# Example texts\ntexts = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\",\n]\n\n# Create Bag of Words\nvocabulary, bow_vectors = create_bag_of_words(texts)\n\n# Print vocabulary\nprint(\"Vocabulary:\")\nprint(vocabulary)\n\n# Print BoW vectors\nprint(\"\\nBag of Words Vectors:\")\nfor i, bow_vector in enumerate(bow_vectors):\n    print(f\"Document {i + 1}: {bow_vector}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVocabulary:\n['document', 'first', 'one', 'second', 'third']\n\nBag of Words Vectors:\nDocument 1: [1, 1, 0, 0, 0]\nDocument 2: [2, 0, 0, 1, 0]\nDocument 3: [0, 0, 1, 0, 1]\nDocument 4: [1, 1, 0, 0, 0]\n```\n:::\n:::\n\n\nBag of words actually gives us some vector representation of our texts with respect to the given vocabulary.\nWe can even calculate with these vectors and try to determine a similarity between the texts.\n\n::: {#d2cb8b31 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -> float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\n\nquery = bow_vectors[3]\n\nsimilarities = []\nfor i, bv in enumerate(bow_vectors):\n\n    similarity = cosine_similarity(\n            vec1=query, \n            vec2=bv\n        )\n\n    similarities.append(\n        (texts[i], round(similarity, 2))\n    )\n\nsimilarities\n```\n\n::: {.cell-output .cell-output-display execution_count=91}\n```\n[('This is the first document.', 1.0),\n ('This document is the second document.', 0.63),\n ('And this is the third one.', 0.0),\n ('Is this the first document?', 1.0)]\n```\n:::\n:::\n\n\n## Limitations of Term Frequency and Bag of Words\nPure statistical text analysis methods like Term Frequency (also its extension Term Frequency-Inverse Document Frequency) or Bag of Words are usually a convenient starting point for text analysis.\nHowever, while they offer useful insights into textual data, they are not without their limitations. \n\nOne significant drawback is the sheer size of the vocabulary they handle. \nAs the corpus grows, so does the vocabulary, which can become overwhelmingly large, leading to computational inefficiencies and increased memory requirements.\n\nMoreover, the mentioned methods struggle with out-of-vocabulary words. \nSince the vocabulary is fixed after the training has finished (the bag of words has been created with plenty of documents), words not present in the vocabulary are often either ignored, leading to information loss, or arbitrarily handled, potentially skewing the analysis results. \nThis limitation becomes particularly pronounced in domains with specialized jargon or evolving lexicons.\n\nAnother critical limitation is the lack of context in these approaches. \nBy treating each word independently and ignoring their sequential and syntactical relationships, TF and Bag of Words fail to capture the nuanced meanings embedded in language. \nThis deficiency hampers their ability to comprehend subtleties such as sarcasm, irony, or metaphors, limiting their applicability in tasks requiring deeper semantic understanding.\n\nLast but not least, these methods lack structural awareness. \nThey disregard the hierarchical and syntactic structures inherent in language, missing out on essential cues provided by sentence and paragraph boundaries. \nThus they often struggle to differentiate between sentences with similar word distributions but differing in meaning or intent.\n\nAs a consequence, more sophisticated methods are required to really start understanding human language in a more sophisticated way.\nBut before we get to such methods, let's try one more thing.\n\n## Clustering of Bag of Word vectors \n\nAs we've seen above, the idea of BoW already gives us a rather simple possibility to compare texts to each other.\nWhenever we can compare different entities to each other (here: the texts), a pretty straight-forward extension is to try and find **clusters**, that is, groups of similar texts. \n\nClustering using bag-of-word vectors is a common technique in text analysis for grouping similar documents together based on the similarity of their word distributions. \nAs seen above, each document is represented as a high-dimensional vector, with each dimension corresponding to a unique word in the vocabulary and its value reflecting the frequency of that word in the document.\nBy treating documents as points in a high-dimensional space, clustering algorithms such as K-means or hierarchical clustering can be applied to partition the documents into coherent groups. \nThe similarity between documents is typically measured using distance metrics such as cosine similarity or Euclidean distance, which quantify the degree of overlap between their word distributions.\n\nOne advantage of using bag-of-word vectors for clustering is its simplicity and scalability. \nSince the vectors only capture the frequency of words without considering their order or context, the computational complexity remains manageable even for large datasets with extensive vocabularies.\nHowever, clustering based on bag-of-word vectors also has its limitations. \nOne major drawback is the reliance on word frequency alone, which may overlook important semantic similarities between documents. \nAdditionally, the curse of dimensionality can become a challenge as the size of the vocabulary increases, leading to decreased clustering performance and increased computational overhead.\nDespite these limitations, clustering using bag-of-word vectors serves as a foundational approach in text analysis, providing valuable insights into document similarity and aiding tasks such as document organization, topic modeling, and information retrieval.\n\nLet's finish up with a small code example. \n\n::: {#433dbb81 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\n\n# do K-means clustering\nn_clusters = 2  # specify the number of clusters\nkmeans = KMeans(n_clusters=n_clusters, n_init=\"auto\")\ncluster_labels = kmeans.fit_predict(bow_vectors)\n\nprint(\"Cluster labels:\\n\")\nfor i, label in enumerate(cluster_labels):\n    print(f\"Document {i + 1} belongs to Cluster {label + 1}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster labels:\n\nDocument 1 belongs to Cluster 1\nDocument 2 belongs to Cluster 2\nDocument 3 belongs to Cluster 1\nDocument 4 belongs to Cluster 1\n```\n:::\n:::\n\n\nLet's maybe do this with some \"texts\" that are more likely to actually create some clusters.\nAnd, of course, there are packages that can do the Bag of Words for us. \nHere we use `scikit-learn`.\n\n::: {#d1fd6c37 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# Example texts representing different topics\ntexts = [\n    \"apple orange banana\",\n    \"apple orange mango\",\n    \"banana apple kiwi\",\n    \"strawberry raspberry blueberry\",\n    \"strawberry raspberry blackberry\"\n]\n\n# create Bag of Words using CountVectorizer\nvectorizer = CountVectorizer()\nbow_matrix = vectorizer.fit_transform(texts)\n\nprint(\"Bag of Word vectors:\")\nprint(bow_matrix.toarray())\n\n# perform K-means clustering\nnum_clusters = 2\nkmeans = KMeans(n_clusters=num_clusters, n_init=\"auto\")\ncluster_labels = kmeans.fit_predict(bow_matrix)\n\nprint(\"\\nCluster labels:\")\nfor i, label in enumerate(cluster_labels):\n    print(f\"Document {i + 1} belongs to Cluster {label + 1}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBag of Word vectors:\n[[1 1 0 0 0 0 1 0 0]\n [1 0 0 0 0 1 1 0 0]\n [1 1 0 0 1 0 0 0 0]\n [0 0 0 1 0 0 0 1 1]\n [0 0 1 0 0 0 0 1 1]]\n\nCluster labels:\nDocument 1 belongs to Cluster 2\nDocument 2 belongs to Cluster 2\nDocument 3 belongs to Cluster 2\nDocument 4 belongs to Cluster 1\nDocument 5 belongs to Cluster 1\n```\n:::\n:::\n\n\n::: {#ea722de5 .cell execution_count=6}\n``` {.python .cell-code}\n# Get the vocabulary\nvocabulary = vectorizer.get_feature_names_out()\n\n# print the vocabulary\nprint(\"Vocabulary:\")\nprint(vocabulary)\n\n# print the Bag of Words matrix with corresponding words\nprint(\"\\nBag of Words matrix with corresponding words:\")\nbow_matrix_array = bow_matrix.toarray()\nfor i, document_vector in enumerate(bow_matrix_array):\n    words_in_document = [(word, frequency) for word, frequency in zip(vocabulary, document_vector) if frequency > 0]\n    print(f\"Document {i + 1}: {words_in_document}\")\n\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVocabulary:\n['apple' 'banana' 'blackberry' 'blueberry' 'kiwi' 'mango' 'orange'\n 'raspberry' 'strawberry']\n\nBag of Words matrix with corresponding words:\nDocument 1: [('apple', 1), ('banana', 1), ('orange', 1)]\nDocument 2: [('apple', 1), ('mango', 1), ('orange', 1)]\nDocument 3: [('apple', 1), ('banana', 1), ('kiwi', 1)]\nDocument 4: [('blueberry', 1), ('raspberry', 1), ('strawberry', 1)]\nDocument 5: [('blackberry', 1), ('raspberry', 1), ('strawberry', 1)]\n```\n:::\n:::\n\n\n",
    "supporting": [
      "statistical_text_analysis_files"
    ],
    "filters": [],
    "includes": {}
  }
}