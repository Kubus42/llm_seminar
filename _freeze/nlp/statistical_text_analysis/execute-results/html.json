{
  "hash": "ae062b7134dc24bf74e999970c029d67",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Statistical text analysis\nformat:\n  html:\n    code-fold: false\n---\n\n- Term frequency \n- Bag of Words -> \"Simple\" form of embeddings\n- Exercise: Create a bag of words \n- Exercise: TF-IDF\n- Clustering? -> Ref to embeddings later on\n\nTODO: Add problems with statistical analysis.\n\n\n## Term frequency\n\nSo far we have mainly looked at the analysis of single words/token or n-grams. \nBut what about the analysis of a full text? \nThere are many approaches to this but a good way to get into the topic is a simple statistical analysis of a text. \nFor starters, let's simply count the number of appearances of each word in a text. \n\n::: {#fbc3b7f6 .cell execution_count=1}\n``` {.python .cell-code}\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\nfrom collections import Counter\nfrom typing import List\n\nfrom nltk.corpus import stopwords\n# python -m nltk.downloader stopwords -> run this in your console once to get the stopwords\n\n\n# load a text from file\ntext = \"\"\nwith open(\"../assets/chapter1.txt\", \"r\") as file:  \n    for line in file:\n        text += line.strip()\n\n\ndef preprocess_text(text: str) -> List[str]:\n    # tokenize text\n    tokens = wordpunct_tokenize(text.lower())\n\n    # remove punctuation\n    tokens = [t for t in tokens if t not in punctuation]\n\n    # remove stopwords\n    stop_words = stopwords.words(\"english\")\n    tokens = [t for t in tokens if t not in stop_words]\n\n    return tokens\n\n# count the most frequent words\ntokens = preprocess_text(text=text)\n\nfor t in Counter(tokens).most_common(15):\n    print(f\"{t[0]}: {t[1]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\none: 35\nwinston: 32\nface: 28\neven: 24\n--: 24\nbig: 22\ncould: 19\nparty: 18\nwould: 18\nmoment: 18\nlike: 17\nbrother: 15\ngoldstein: 15\ntelescreen: 14\nseemed: 14\n```\n:::\n:::\n\n\nJust from the most frequent words, can you guess the text?\n\n\nIn many cases, just the simple number of appearances of a token in a text can determine its importance. \nThe concept of counting the term frequency is also known as **bag of words**. \n\n\n## Bag of Words\nIf we do the same with multiple texts, we can build up a vocabulary of words and compare different texts to each other based on the appearance of terms.\n\n::: {#2e209433 .cell execution_count=2}\n``` {.python .cell-code}\nfrom collections import Counter\n\n\ndef create_bag_of_words(texts):\n    # Count the frequency of each word in the corpus\n    word_counts = Counter()\n    \n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Update word counts\n        word_counts.update(words)\n    \n    # Create vocabulary by sorting the words based on their frequency\n    vocabulary = [word for word, _ in sorted(word_counts.items())]\n    \n    # Create BoW vectors for each document\n    bow_vectors = []\n    for text in texts:\n        # Preprocess the text\n        words = preprocess_text(text)\n        \n        # Create a Counter object to count word frequencies\n        bow_vector = Counter(words)\n        \n        # Fill in missing words with zero counts\n        for word in vocabulary:\n            if word not in bow_vector:\n                bow_vector[word] = 0\n\n        # Sort the BoW vector based on the vocabulary order\n        sorted_bow_vector = [bow_vector[word] for word in vocabulary]\n        \n        # Append the BoW vector to the list\n        bow_vectors.append(sorted_bow_vector)\n    \n    return vocabulary, bow_vectors\n\n# Example texts\ntexts = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\",\n]\n\n# Create Bag of Words\nvocabulary, bow_vectors = create_bag_of_words(texts)\n\n# Print vocabulary\nprint(\"Vocabulary:\")\nprint(vocabulary)\n\n# Print BoW vectors\nprint(\"\\nBag of Words Vectors:\")\nfor i, bow_vector in enumerate(bow_vectors):\n    print(f\"Document {i + 1}: {bow_vector}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVocabulary:\n['document', 'first', 'one', 'second', 'third']\n\nBag of Words Vectors:\nDocument 1: [1, 1, 0, 0, 0]\nDocument 2: [2, 0, 0, 1, 0]\nDocument 3: [0, 0, 1, 0, 1]\nDocument 4: [1, 1, 0, 0, 0]\n```\n:::\n:::\n\n\nBag of words actually gives us some vector representation of our texts with respect to the given vocabulary.\nWe can even calculate with these vectors and try to determine a similarity between the texts.\n\n::: {#c00d574a .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\n\ndef cosine_similarity(vec1: np.array, vec2: np.array) -> float: \n    return np.dot(vec1, vec2) / ( np.linalg.norm(vec1) * np.linalg.norm(vec2) )\n\n\nquery = bow_vectors[3]\n\nsimilarities = []\nfor i, bv in enumerate(bow_vectors):\n\n    similarity = cosine_similarity(\n            vec1=query, \n            vec2=bv\n        )\n\n    similarities.append(\n        (texts[i], round(similarity, 2))\n    )\n\nsimilarities\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n[('This is the first document.', 1.0),\n ('This document is the second document.', 0.63),\n ('And this is the third one.', 0.0),\n ('Is this the first document?', 1.0)]\n```\n:::\n:::\n\n\nIssues: \n\n- Vocabulary gets very large\n- Words not in vocabulary are an issue \n- No context\n- No structure\n\n\n## Clustering of BoW vectors \n\nTODO: Write\n\n",
    "supporting": [
      "statistical_text_analysis_files"
    ],
    "filters": [],
    "includes": {}
  }
}