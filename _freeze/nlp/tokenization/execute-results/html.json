{
  "hash": "147b87319f99cfb2fa2b4c89c16ed360",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Tokenization\"\nformat:\n  html:\n    code-fold: false\njupyter: python3\n---\n\n\n## Simple word tokenization\nA key element for a computer to understand the words we speak or type is the concept of word tokenization. \nFor a human, the sentence \n\n::: {#67092701 .cell execution_count=1}\n``` {.python .cell-code}\nsentence = \"I love reading science fiction books or books about science.\"\n```\n:::\n\n\nis easy to understand since we are able to split the sentence into its individual parts in order to figure out the meaning of the full sentence.\nFor a computer, the sentence is just a simple string of characters, like any other word or longer text.\nIn order to make a computer understand the meaning of a sentence, we need to help break it down into its relevant parts.\n\nSimply put, word tokenization is the process of breaking down a piece of text into individual words or so-called tokens. \nIt is like taking a sentence and splitting it into smaller pieces, where each piece represents a word.\nWord tokenization involves analyzing the text character by character and identifying boundaries between words. \nIt uses various rules and techniques to decide where one word ends and the next one begins. \nFor example, spaces, punctuation marks, and special characters often serve as natural boundaries between words.\n\nSo let's start breaking down the sentence into its individual parts.\n\n::: {#3bae7461 .cell execution_count=2}\n``` {.python .cell-code}\ntokenized_sentence = sentence.split(\" \")\nprint(tokenized_sentence)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science.']\n```\n:::\n:::\n\n\nOnce we have tokenized the sentence, we can start analyzing it with some simple statistical methods. \nFor example, in order to figure out what the sentence might be about, we could count the most frequent words. \n\n::: {#0fb5413c .cell execution_count=3}\n``` {.python .cell-code}\nfrom collections import Counter\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('books', 2), ('I', 1)]\n```\n:::\n:::\n\n\nUnfortunately, we already realize that we have not done the best job with our \"tokenizer\": The second occurrence of the word `science` is missing do to the punctuation. \nWhile this is great as it holds information about the ending of a sentence, it disturbs our analysis here, so let's get rid of it. \n\n::: {#19dc5f50 .cell execution_count=4}\n``` {.python .cell-code}\ntokenized_sentence = sentence.replace(\".\", \" \").split(\" \")\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('science', 2), ('books', 2)]\n```\n:::\n:::\n\n\nSo that worked.\nAs you can imagine, tokenization can get increasingly difficult when we have to deal with all sorts of situations in larger corpora of texts (see also the exercise). \nSo it is great that there are already all sorts of libraries available that can help us with this process. \n\n::: {#47966ded .cell execution_count=5}\n``` {.python .cell-code}\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\n\ntokenized_sentence = wordpunct_tokenize(sentence)\ntokenized_sentence = [t for t in tokenized_sentence if t not in punctuation]\nprint(tokenized_sentence)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science']\n```\n:::\n:::\n\n\n## Advanced word tokenization\n\nAs you can imagine, the idea of tokenization does not stop here, we have only touched the basic idea of it. \nAs a general strategy, we can now start from our word tokens and refine the tokens in a way that we need. \nA classic problem arising with the token above is that we do not get word tokens in their standardized form. \nSo if, for example, we were to take all our token together and use that as a dictionary, we would get two different token for every words that appears both in singular and plural form (with the added \"s\").\nOr we would receive different tokens for every verb in different conjugations (for example, \"speak\", \"speaks\", and \"spoken\").\nDepending on our task, a great idea is to try and find the basic form of each token, a process called **lemmatization**.\n\n\n### Lemmatization\n\nLemmatization is a natural language processing technique used to reduce words to their base or canonical form, known as the lemma. \nThe lemma represents the dictionary form of a word and is typically a valid word that exists in the language. \nLemmatization helps in standardizing words so that different forms of the same word are treated as one, simplifying text analysis and improving accuracy in tasks such as text classification, information retrieval, and sentiment analysis.\n\nUnlike stemming, which simply chops off prefixes or suffixes to derive the root form of a word (sometimes resulting in non-existent or incorrect forms), lemmatization considers the context of the word and applies linguistic rules to transform it into its lemma. \nThis ensures that the resulting lemma is a valid word in the language and retains its semantic meaning.\n\nLemmatization involves part-of-speech tagging to determine the correct lemma for each word based on its role in the sentence. \nFor example, the word \"running\" may be lemmatized to \"run\" as a verb, but to \"running\" as a noun.\nHowever, once we have lemmatized our text, we might lose some information due to the lost context.\n\nLuckily for us, there are already pre-built packages that we can use to try out lemmatization.\nHere is a quick example how to do it with `nltk`. \n\n::: {#78a565e3 .cell execution_count=6}\n``` {.python .cell-code}\nfrom nltk.stem import WordNetLemmatizer\n\nsentence = \"The three brothers went over three big bridges\"\n\nwnl = WordNetLemmatizer()\n\nlemmatized_sentence_token = [\n    wnl.lemmatize(w, pos=\"n\") for w in sentence.split(\" \")\n]\n\nprint(lemmatized_sentence_token)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['The', 'three', 'brother', 'went', 'over', 'three', 'big', 'bridge']\n```\n:::\n:::\n\n\nSince we need to include the `pos` (part-of-speech) tag of each word and only choose a noun (`n`) here, the lemmatizer only takes care of nouns in the sentence. \nLet's try it with verbs.\n\n::: {#41a1efe3 .cell execution_count=7}\n``` {.python .cell-code}\nlemmatized_sentence_token = [\n    wnl.lemmatize(w, pos=\"v\") for w in sentence.split(\" \")\n]\n\nprint(lemmatized_sentence_token)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['The', 'three', 'brothers', 'go', 'over', 'three', 'big', 'bridge']\n```\n:::\n:::\n\n\nThis works, however, we now encounter a different problem: The word `bridges` has been turned into `bridge`, since it exists also as the ver \"to bridge\".\nSo, as mentioned above, we need to involve some part of speech tagging in order to do it correctly.\nLet's try to fix it manually.\n\n::: {#e37ba8cd .cell execution_count=8}\n``` {.python .cell-code}\npos_dict = {\n  \"brothers\": \"n\", \n  \"went\": \"v\",\n  \"big\": \"a\",\n  \"bridges\": \"n\"\n}\n\nlemmatized_sentence_token = []\nfor token in sentence.split(\" \"):\n    if token in pos_dict:\n        lemma = wnl.lemmatize(token, pos=pos_dict[token])\n    else: \n        lemma = token # leave as it is\n\n    lemmatized_sentence_token.append(lemma)\n\nprint(lemmatized_sentence_token)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['The', 'three', 'brother', 'go', 'over', 'three', 'big', 'bridge']\n```\n:::\n:::\n\n\nAgain, luckily there also some packages that we can use, `spaCy` is one example. \nTheir models come with a lot of built-in functionality.\n\n::: {#1435b7aa .cell execution_count=9}\n``` {.python .cell-code}\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndoc = nlp(sentence)\n\nlemmatized_words = [(token.lemma_, token.pos_) for token in doc]\n\nprint(lemmatized_words)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('the', 'DET'), ('three', 'NUM'), ('brother', 'NOUN'), ('go', 'VERB'), ('over', 'ADP'), ('three', 'NUM'), ('big', 'ADJ'), ('bridge', 'NOUN')]\n```\n:::\n:::\n\n\nNow that we have successfully lemmatized our document or text, we can start using the lemmas to do some further analysis.\nFor example, we could build a dictionary and do some statistics on it.\nWe will see more about that [later](statistical_text_analysis.qmd). \n\n\n### Bit Pair Encoding\n\nThe above ideas illustrate well the idea of tokenization of splitting text into smaller chunks that we can feed to a language model.\nIn practice, especially in models like GPT, a critical component is the vocabulary or the set of unique words or tokens the model understands.\nTraditional approaches use fixed-size vocabularies, which means every unique word in the corpus has its own representation (index or embedding) in the model's vocabulary. \nHowever, as the vocabulary size increases (for example, by including more languages), so does the memory requirement, which can be impractical for large-scale language models. \nOne solution is the so-called bit-pair encoding.\nBit pair encoding is a data compression technique specifically designed to tackle the issue of large vocabularies in language models. \nInstead of assigning a unique index or embedding to each token, bit pair encoding identifies frequent pairs of characters (bits) within the corpus and represents them as a single token. \nThis effectively reduces the size of the vocabulary while preserving the essential information needed for language modeling tasks.\n\n\n#### How it works\n\n1. **Tokenization**: The first step in bit pair encoding is tokenization, where the text corpus is broken down into individual tokens. These tokens could be characters, subwords, or words, depending on the tokenization strategy used.\n\n2. **Pair Identification**: Next, the algorithm identifies pairs of characters (bits) that occur frequently within the corpus. These pairs are typically consecutive characters in the text.\n\n3. **Replacement with Single Token**: Once frequent pairs are identified, they are replaced with a single token. This effectively reduces the number of unique tokens in the vocabulary.\n\n4. **Iterative Process**: The process of identifying frequent pairs and replacing them with single tokens is iterative. It continues until a predefined stopping criterion is met, such as reaching a target vocabulary size or when no more frequent pairs can be found.\n\n5. **Vocabulary Construction**: After the iterative process, a vocabulary is constructed, consisting of the single tokens generated through pair replacement, along with any remaining tokens from the original tokenization process.\n\n6. **Encoding and Decoding**: During training and inference, text data is encoded using the constructed vocabulary, where each token is represented by its corresponding index in the vocabulary. During decoding, the indices are mapped back to their respective tokens.\n\n\n::: {.callout-tip}\nIt is very illustrative to use the the OpenAI [tokenizer](https://platform.openai.com/tokenizer){.external} to see how a sentence is split up into different token.\nTry mixing languages and standard as well as more rare words and observe how they are split up.\n\nAnother detailed example can be found [here](https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/){.external}.\n:::\n\n\n\n#### Advantages of Bit Pair Encoding\n\n1. **Efficient Memory Usage**: Bit pair encoding significantly reduces the size of the vocabulary, leading to more efficient memory usage, especially in large-scale language models.\n\n2. **Retains Information**: Despite reducing the vocabulary size, bit pair encoding retains important linguistic information by capturing frequent character pairs.\n\n3. **Flexible**: Bit pair encoding is flexible and can be adapted to different tokenization strategies and corpus characteristics.\n\n\n#### Limitations and Considerations\n\n1. **Computational Overhead**: The iterative nature of bit pair encoding can be computationally intensive, especially for large corpora.\n\n2. **Loss of Granularity**: While bit pair encoding reduces vocabulary size, it may lead to a loss of granularity, especially for rare or out-of-vocabulary words.\n\n3. **Tokenization Strategy**: The effectiveness of bit pair encoding depends on the tokenization strategy used and the characteristics of the corpus.\n\n\n\n::: {.callout-tip}\n__From the [OpenAI Guide](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them){.external}__:\n\nA helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words).\n:::\n\n",
    "supporting": [
      "tokenization_files"
    ],
    "filters": [],
    "includes": {}
  }
}