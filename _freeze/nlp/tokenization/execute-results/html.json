{
  "hash": "b9faa2cc27aa2ac93360727c228d3d38",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Tokenization\nformat:\n  html:\n    code-fold: false\n---\n\nTODO: Some introductory sentence.\n\n## Simple word tokenization\nA key element for a computer to understand the words we speak or type is the concept of word tokenization. \nFor a human, the sentence \n\n::: {#706b1324 .cell execution_count=1}\n``` {.python .cell-code}\nsentence = \"I love reading science fiction books or books about science.\"\n```\n:::\n\n\nis easy to understand since we are able to split the sentence into its individual parts in order to figure out the meaning of the full sentence.\nFor a computer, the sentence is just a simple string of characters, like any other word or longer text.\nIn order to make a computer understand the meaning of a sentence, we need to help break it down into its relevant parts.\n\nSimply put, word tokenization is the process of breaking down a piece of text into individual words or so-called tokens. \nIt is like taking a sentence and splitting it into smaller pieces, where each piece represents a word.\nWord tokenization involves analyzing the text character by character and identifying boundaries between words. \nIt uses various rules and techniques to decide where one word ends and the next one begins. \nFor example, spaces, punctuation marks, and special characters often serve as natural boundaries between words.\n\nSo let's start breaking down the sentence into its individual parts.\n\n::: {#650aa91e .cell execution_count=2}\n``` {.python .cell-code}\ntokenized_sentence = sentence.split(\" \")\nprint(tokenized_sentence)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science.']\n```\n:::\n:::\n\n\nOnce we have tokenized the sentence, we can start analyzing it with some simple statistical methods. \nFor example, in order to figure out what the sentence might be about, we could count the most frequent words. \n\n::: {#f5e22bc0 .cell execution_count=3}\n``` {.python .cell-code}\nfrom collections import Counter\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('books', 2), ('I', 1)]\n```\n:::\n:::\n\n\nUnfortunately, we already realize that we have not done the best job with our \"tokenizer\": The second occurrence of the word `science` is missing do to the punctuation. \nWhile this is great as it holds information about the ending of a sentence, it disturbs our analysis here, so let's get rid of it. \n\n::: {#4547c10a .cell execution_count=4}\n``` {.python .cell-code}\ntokenized_sentence = sentence.replace(\".\", \" \").split(\" \")\n\ntoken_counter = Counter(tokenized_sentence)\nprint(token_counter.most_common(2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('science', 2), ('books', 2)]\n```\n:::\n:::\n\n\nSo that worked.\nAs you can imagine, tokenization can get increasingly difficult when we have to deal with all sorts of situations in larger corpora of texts (see also the exercise). \nSo it is great that there are already all sorts of libraries available that can help us with this process. \n\n::: {#f635468e .cell execution_count=5}\n``` {.python .cell-code}\nfrom nltk.tokenize import wordpunct_tokenize\nfrom string import punctuation\n\ntokenized_sentence = wordpunct_tokenize(sentence)\ntokenized_sentence = [t for t in tokenized_sentence if t not in punctuation]\nprint(tokenized_sentence)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['I', 'love', 'reading', 'science', 'fiction', 'books', 'or', 'books', 'about', 'science']\n```\n:::\n:::\n\n\n## Advanced word tokenization\n\nThe above ideas illustrate well the idea of tokenization of splitting text into smaller chunks that we can feed to a language model.\nIn practice, especially in models like GPT, a critical component is the vocabulary or the set of unique words or tokens the model understands.\nTraditional approaches use fixed-size vocabularies, which means every unique word in the corpus has its own representation (index or embedding) in the model's vocabulary. \nHowever, as the vocabulary size increases (for example, by including more languages), so does the memory requirement, which can be impractical for large-scale language models. \nOne solution is the so-called bit-pair encoding.\nBit pair encoding is a data compression technique specifically designed to tackle the issue of large vocabularies in language models. \nInstead of assigning a unique index or embedding to each token, bit pair encoding identifies frequent pairs of characters (bits) within the corpus and represents them as a single token. \nThis effectively reduces the size of the vocabulary while preserving the essential information needed for language modeling tasks.\n\n\n### How Bit Pair Encoding Works:\n\n1. **Tokenization**: The first step in bit pair encoding is tokenization, where the text corpus is broken down into individual tokens. These tokens could be characters, subwords, or words, depending on the tokenization strategy used.\n\n2. **Pair Identification**: Next, the algorithm identifies pairs of characters (bits) that occur frequently within the corpus. These pairs are typically consecutive characters in the text.\n\n3. **Replacement with Single Token**: Once frequent pairs are identified, they are replaced with a single token. This effectively reduces the number of unique tokens in the vocabulary.\n\n4. **Iterative Process**: The process of identifying frequent pairs and replacing them with single tokens is iterative. It continues until a predefined stopping criterion is met, such as reaching a target vocabulary size or when no more frequent pairs can be found.\n\n5. **Vocabulary Construction**: After the iterative process, a vocabulary is constructed, consisting of the single tokens generated through pair replacement, along with any remaining tokens from the original tokenization process.\n\n6. **Encoding and Decoding**: During training and inference, text data is encoded using the constructed vocabulary, where each token is represented by its corresponding index in the vocabulary. During decoding, the indices are mapped back to their respective tokens.\n\n\n::: {.callout-tip}\nIt is very illustrative to use the the OpenAI [tokenizer](https://platform.openai.com/tokenizer){.external} to see how a sentence is split up into different token.\nTry mixing languages and standard as well as more rare words and observe how they are split up.\n\nAnother detailed example can be found [here](https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/){.external}.\n:::\n\n\n\n### Advantages of Bit Pair Encoding:\n\n1. **Efficient Memory Usage**: Bit pair encoding significantly reduces the size of the vocabulary, leading to more efficient memory usage, especially in large-scale language models.\n\n2. **Retains Information**: Despite reducing the vocabulary size, bit pair encoding retains important linguistic information by capturing frequent character pairs.\n\n3. **Flexible**: Bit pair encoding is flexible and can be adapted to different tokenization strategies and corpus characteristics.\n\n\n### Limitations and Considerations:\n\n1. **Computational Overhead**: The iterative nature of bit pair encoding can be computationally intensive, especially for large corpora.\n\n2. **Loss of Granularity**: While bit pair encoding reduces vocabulary size, it may lead to a loss of granularity, especially for rare or out-of-vocabulary words.\n\n3. **Tokenization Strategy**: The effectiveness of bit pair encoding depends on the tokenization strategy used and the characteristics of the corpus.\n\n\n\n::: {.callout-tip}\n__From the [OpenAI Guide](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them){.external}__:\n\nA helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly Â¾ of a word (so 100 tokens ~= 75 words).\n:::\n\n",
    "supporting": [
      "tokenization_files"
    ],
    "filters": [],
    "includes": {}
  }
}