{
  "hash": "f981e875166a9d0894daeb459c108a0c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Fuzzy matching\nformat:\n  html:\n    code-fold: false\n    code-tools: true\n---\n\nAs can be seen from the previous example, the detection of certain keywords from a text can prove more difficult than one might expect. \nThe key issues stem from the fact that natural language has many facets such as conjugation, singular and plural forms, adjectives vs. adverbs etc. \nBut even when these are handled, there remain challenges for keywords detection. \nIn the previous example, our detection still fails when: \n\n- keywords consist of multiple words (`product portfolio`),\n- keywords have different forms but mean the same (`advertisment` vs. `advertising`),\n- keywords have wrong spelling (`langscpe` vs. `landscape`),\n- keywords and target words are not exactly the same thing but closely related (`analysis` vs. `analyst`).\n\nThe former case can be handled by using so called n-grams. \nIn contrast to the single words we used for word tokens, n-grams are sequences of `n` consecutive words in a text, thus capturing some more of the context in a simple way. \nLet's see a simple example for 2-grams:\n\n::: {#a482c8c7 .cell execution_count=1}\n``` {.python .cell-code}\nfrom nltk import ngrams\n\nsentence = \"The CEO announced plans to diversify the company's product portfolio...\"\n\nfor n_gram in ngrams(sentence.split(\" \"), n=2):\n  print(n_gram)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n('The', 'CEO')\n('CEO', 'announced')\n('announced', 'plans')\n('plans', 'to')\n('to', 'diversify')\n('diversify', 'the')\n('the', \"company's\")\n(\"company's\", 'product')\n('product', 'portfolio...')\n```\n:::\n:::\n\n\nIn order to detect keywords consisting of more than a single word we can now split our text into n-grams for different `n`(e.g., 2, 3, 4) and compare these to our keywords.\n\n\nIn order to handle the other three cases, we a different approach. \nSo let us first notice that, for all three cases, the word we are trying to compare are very similar (in terms of the contained letters) but not exactly equal. \nSo what if we had a way to define a similarity between words and texts or, more generally, between any strings?\nOne solution for this is **fuzzy matching**. \nInstead of considering two strings a match if they are exactly equal, fuzzy matching assigns a score to the pair. \nIf the score is high enough, we might consider the pair a match. \n\n<details>\n<summary>Some details about fuzzy matching</summary>\nFuzzy string matching is a technique used to find strings that are approximately similar to a given pattern, even if there are differences in spelling, punctuation, or word order. It is particularly useful in situations where exact string matching is not feasible due to variations or errors in the data. Fuzzy matching algorithms compute a similarity score between pairs of strings, typically based on criteria such as character similarity, substring matching, or token-based similarity. These algorithms often employ techniques like Levenshtein distance, which measures the minimum number of single-character edits required to transform one string into another, or tokenization to compare sets or sorted versions of tokens. Overall, fuzzy string matching enables the identification of similar strings, facilitating tasks such as record linkage, spell checking, and approximate string matching in various applications, including natural language processing, data cleaning, and information retrieval.\n</details>\n\nLet's see how this works using the package [`rapidfuzz`](https://github.com/rapidfuzz/RapidFuzz?tab=readme-ov-file#description){.external}.\n\n::: {#89c83b81 .cell code-link='true' execution_count=2}\n``` {.python .cell-code}\nfrom rapidfuzz import fuzz\n\nword_pairs = [\n  (\"advertisment\", \"advertising\"),\n  (\"landscpe\", \"landscape\"),\n  (\"analysis\", \"analyst\")\n]\n\nfor word_pair in word_pairs:\n  ratio = fuzz.ratio(\n    s1=word_pair[0], \n    s2=word_pair[1]\n  )\n  print(f\"Similarity score '{word_pair[0]} - '{word_pair[1]}': {round(ratio, 2)}.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSimilarity score 'advertisment - 'advertising': 78.26.\nSimilarity score 'landscpe - 'landscape': 94.12.\nSimilarity score 'analysis - 'analyst': 80.0.\n```\n:::\n:::\n\n\nLet us use fuzzy matching on order to detect some of the missing keywords from the previous example.\n\n\n\n::: {#fe23ce8b .cell execution_count=4}\n``` {.python .cell-code}\nfrom pprint import pprint\nfrom nltk.tokenize import wordpunct_tokenize\n\ntokenized_text = wordpunct_tokenize(text=text)\n\nmin_score = 75\n\nmatches = []\nfor token in tokenized_text:\n  for keyword in keywords:\n    ratio = fuzz.ratio(\n      s1=token.lower(), \n      s2=keyword.lower()\n    )\n    if ratio >= min_score:\n      matches.append(\n        (keyword, token, round(ratio, 2))\n      )\n\npprint(matches)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[('Quarter', 'quarterly', 87.5),\n ('Earnings', 'earnings', 100.0),\n ('Report', 'reports', 92.31),\n ('Analysis', 'analysts', 87.5),\n ('Stock', 'stock', 100.0),\n ('Investor', 'investor', 100.0),\n ('Announce', 'announced', 94.12),\n ('Diversity', 'diversify', 88.89),\n ('Market', 'markets', 92.31),\n ('Market', 'marketing', 80.0),\n ('Advertisment', 'advertising', 78.26),\n ('Landscpe', 'landscape', 94.12)]\n```\n:::\n:::\n\n\nAs we can see we have now successfully found most of the keywords we were looking for. \nHowever, we can also see a new caveat: We have now detected two possible matches for `Market`: `Marketing` and `Markets`. \nIn this case, we can simply pick the one with the higher score and we are good, but there will be cases where it is more difficult to decide, whether a match, even with a higher score, is actually a match. \n\nFuzzy matching can, of course, also be used to compare n-grams or even entire texts to each other (see also the documentation of `rapidfuzz` and the next exercise); however there are certain limits to how practical it can be. \nBut the concept in general already gives us some good evidence that, in order to compare words and text to each other, we would like to be able to somehow **calculate** with text. \nIn the next sections, we will see ways how to do that more efficiently.\n\n",
    "supporting": [
      "fuzzy_matching_files"
    ],
    "filters": [],
    "includes": {}
  }
}